{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "darknet",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "KhJpd_NHrl3a"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochengJF/DeepLearning/blob/master/darknet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxR9iVyej3rh",
        "colab_type": "text"
      },
      "source": [
        "# Darknet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9zJJFZXNrRn",
        "colab_type": "text"
      },
      "source": [
        "## 解析配置文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lee0sSfNRGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#from util import * "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJVXBiSkN-HH",
        "colab_type": "text"
      },
      "source": [
        "定义一个函数 parse_cfg，用配置文件的路径作为输入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3tzMrnNeyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_cfg(cfgfile):\n",
        "    \"\"\"\n",
        "    Takes a configuration file\n",
        "    Returns a list of blocks. Each blocks describes a block in the neural\n",
        "    network to be built. Block is represented as a dictionary in the list    \n",
        "    \"\"\"\n",
        "    file = open(cfgfile, 'r')\n",
        "    lines = file.read().split('\\n')                        # store the lines in a list\n",
        "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
        "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
        "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
        "    \n",
        "    block = {}\n",
        "    blocks = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line[0] == \"[\":               # This marks the start of a new block\n",
        "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
        "                blocks.append(block)     # add it the blocks list\n",
        "                block = {}               # re-init the block\n",
        "            block[\"type\"] = line[1:-1].rstrip()     \n",
        "        else:\n",
        "            key,value = line.split(\"=\") \n",
        "            block[key.rstrip()] = value.lstrip()\n",
        "    blocks.append(block)\n",
        "\n",
        "    return blocks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HELg17PPGVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#blocks = parse_cfg(\"/content/gdrive/My Drive/YOLO/v3/cfg/yolov3.cfg\")\n",
        "#blocks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-ufryjTP1IG",
        "colab_type": "text"
      },
      "source": [
        "## 将 parse_cfg 返回的列表构建 PyTorch 模块  \n",
        "## 创建卷积层、采样层、路由层（Route Layer）和捷径层（Shortcut Layer）  \n",
        "\n",
        "<font color=green size=5>**create_modules 函数用 parse_cfg 函数返回的 blocks 列表：**</font>\n",
        "\n",
        "\n",
        "\n",
        "*   先定义变量 net_info，来存储该网络的信息\n",
        "\n",
        "*  当添加 nn.ModuleList 作为 nn.Module 对象的一个成员时（即添加模块到网络），所有 nn.ModuleList 内部的 nn.Module 对象（模块）的 parameter 也被添加   作为 nn.Module 对象（即网络添加 nn.ModuleList 作为其成员）的 parameter\n",
        "\n",
        "*   卷积核的深度是由上一层的卷积核数量（或特征图深度）决定的。这意味着我们需要持续追踪被应用卷积层的卷积核数量。用变量 prev_filter 实现追踪（RGB3通道所以初始化为3）\n",
        "*  路由层（route layer）从前面层得到特征图，如果在路由层之后有一个卷积层，那么卷积核将被应用到前面层的特征图上，精确来说是路由层得到的特征图。因此，我们不仅需要追踪前一层的卷积核数量，还需要追踪之前每个层。不断地迭代，将每个模块的输出卷积核数量添加到 output_filters 列表 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b40dPOsVPGH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_modules(blocks):\n",
        "    net_info = blocks[0]  # Captures the information about the input and pre-processing\n",
        "    module_list = nn.ModuleList()\n",
        "    index = 0  # indexing blocks helps with implementing route  layers (skip connections)\n",
        "    prev_filters = 3\n",
        "    output_filters = []\n",
        "    \n",
        "    # 迭代模块的列表，并为每个模块创建一个 PyTorch 模块\n",
        "    for index, x in enumerate(blocks[1:]): \n",
        "        module = nn.Sequential()\n",
        "        #check the type of block\n",
        "        #create a new module for the block\n",
        "        #append to module_list\n",
        "        \n",
        "       #创建卷积层\n",
        "       # If it's a convolutional layer\n",
        "        if (x[\"type\"] == \"convolutional\"):\n",
        "            # Get the info about the layer\n",
        "            activation = x[\"activation\"]\n",
        "            try:\n",
        "                batch_normalize = int(x[\"batch_normalize\"])\n",
        "                bias = False\n",
        "            except:\n",
        "                batch_normalize = 0\n",
        "                bias = True\n",
        "\n",
        "            filters = int(x[\"filters\"])\n",
        "            padding = int(x[\"pad\"])\n",
        "            kernel_size = int(x[\"size\"])\n",
        "            stride = int(x[\"stride\"])\n",
        "\n",
        "            if padding:\n",
        "                pad = (kernel_size - 1) // 2\n",
        "            else:\n",
        "                pad = 0\n",
        "\n",
        "            # Add the convolutional layer\n",
        "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)\n",
        "            module.add_module(\"conv_{0}\".format(index), conv)\n",
        "            \n",
        "            # Add the Batch Norm Layer\n",
        "            if batch_normalize:\n",
        "                bn = nn.BatchNorm2d(filters)\n",
        "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
        "\n",
        "            # Check the activation.\n",
        "            # It is either Linear or a Leaky ReLU for YOLO\n",
        "            if activation == \"leaky\":\n",
        "                activn = nn.LeakyReLU(0.1, inplace=True)\n",
        "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
        "\n",
        "        # 构建上采样层\n",
        "        # If it's an upsampling layer\n",
        "        # We use Bilinear2dUpsampling\n",
        "        elif (x[\"type\"] == \"upsample\"):\n",
        "            stride = int(x[\"stride\"])\n",
        "            #            upsample = Upsample(stride)\n",
        "            upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
        "            \n",
        "\n",
        "        # 路由层\n",
        "        # If it is a route layer\n",
        "        elif (x[\"type\"] == \"route\"):\n",
        "            x[\"layers\"] = x[\"layers\"].split(',')\n",
        "\n",
        "            # Start  of a route\n",
        "            start = int(x[\"layers\"][0])\n",
        "\n",
        "            # end, if there exists one.\n",
        "            try:\n",
        "                end = int(x[\"layers\"][1])\n",
        "            except:\n",
        "                end = 0\n",
        "\n",
        "            # Positive anotation\n",
        "            if start > 0:\n",
        "                start = start - index\n",
        "\n",
        "            if end > 0:\n",
        "                end = end - index\n",
        "\n",
        "            route = EmptyLayer()\n",
        "            module.add_module(\"route_{0}\".format(index), route)\n",
        "\n",
        "            if end < 0:\n",
        "                filters = output_filters[index + start] + output_filters[index + end]\n",
        "            else:\n",
        "                filters = output_filters[index + start]\n",
        "                \n",
        "        # 捷径层       \n",
        "        # shortcut corresponds to skip connection\n",
        "        if x[\"type\"] == \"shortcut\":\n",
        "            #from_ = int(x[\"from\"]) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "            shortcut = EmptyLayer()\n",
        "            module.add_module(\"shortcut_{}\".format(index), shortcut)    \n",
        "            \n",
        "            \n",
        "            \n",
        "        # YOLO层\n",
        "        # Yolo is the detection layer\n",
        "        if x[\"type\"] == \"yolo\":\n",
        "            mask = x[\"mask\"].split(\",\")\n",
        "            mask = [int(x) for x in mask]\n",
        "\n",
        "            anchors = x[\"anchors\"].split(\",\")\n",
        "            anchors = [int(a) for a in anchors]\n",
        "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
        "            anchors = [anchors[i] for i in mask]\n",
        "\n",
        "            detection = DetectionLayer(anchors)\n",
        "            module.add_module(\"Detection_{}\".format(index), detection)  \n",
        "            \n",
        "        # 回路结束时，做一些统计（bookkeeping.）\n",
        "        module_list.append(module)\n",
        "        prev_filters = filters\n",
        "        output_filters.append(filters)\n",
        "        \n",
        "    return (net_info, module_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-X6Zb85urR3",
        "colab_type": "text"
      },
      "source": [
        "## 定义网络\n",
        "用 nn.Module 在 PyTorch 中构建自定义架构  ：\n",
        "\n",
        "*   定义nn.Module子类Darknet，用 members、blocks、net_info 和 module_list 进行初始化  \n",
        "\n",
        "*   重写forward方法来实现网络的前向传递nn.Module ：1）计算输出；2）以一种可以更容易处理的方式转换输出检测特征图，例如：将它们转换为可以连接多个尺度的检测图（ self.blocks 的第一个元素是 net 块，不属于前向传播，所以迭代的对象是 self.block[1:]）\n",
        "\n",
        "*   与create_modules函数一样，module_list中包含网络的模块。模块的附加顺序与配置文件中的顺序相同。这意味着，我们可以通过每个模块简单地运行输入以获得输出。\n",
        "*  路由层需要连接两个特征映射， 用torch.cat带有第二个参数的函数为1，因为要沿着深度连接特征映射\n",
        "\n",
        "\n",
        "*   列表项\n",
        "\n",
        "\n",
        "*   列表项  \n",
        "\n",
        "## 加载权重\n",
        "<font color=green size=4>**权重是如何存储：**</font>  \n",
        "\n",
        "权重只属于<font color=green>批量归一化层（batch norm layer）和卷积层</font>  两种类型的层，储存顺序和配置文件中定义层级的顺序完全相同  \n",
        "下图展示了权重如何储存：\n",
        "![替代文字](https://img-blog.csdn.net/20180507102704610?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1MjA1MTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzhDjzWcZrzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Darknet(nn.Module):\n",
        "    def __init__(self, cfgfile):\n",
        "        super(Darknet, self).__init__()\n",
        "        self.blocks = parse_cfg(cfgfile)\n",
        "        self.net_info, self.module_list = create_modules(self.blocks)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, x, CUDA):\n",
        "        detections = []\n",
        "        modules = self.blocks[1:]\n",
        "        outputs = {}  # We cache the outputs for the route layer    \n",
        "        \n",
        "        write = 0\n",
        "        for i in range(len(modules)):\n",
        "            module_type = (modules[i][\"type\"])    \n",
        "            \n",
        "            # 卷积和上采样层\n",
        "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
        "                x = self.module_list[i](x)\n",
        "                outputs[i] = x     \n",
        "                \n",
        "\n",
        "            # 路由层\n",
        "            elif module_type == \"route\":\n",
        "                layers = modules[i][\"layers\"]\n",
        "                layers = [int(a) for a in layers]\n",
        "\n",
        "                if (layers[0]) > 0:\n",
        "                    layers[0] = layers[0] - i\n",
        "\n",
        "                if len(layers) == 1:\n",
        "                    x = outputs[i + (layers[0])]\n",
        "\n",
        "                else:\n",
        "                    if (layers[1]) > 0:\n",
        "                        layers[1] = layers[1] - i\n",
        "\n",
        "                    map1 = outputs[i + layers[0]]\n",
        "                    map2 = outputs[i + layers[1]]\n",
        "\n",
        "                    x = torch.cat((map1, map2), 1)\n",
        "                \n",
        "            elif module_type == \"shortcut\":\n",
        "                from_ = int(modules[i][\"from\"])\n",
        "                x = outputs[i - 1] + outputs[i + from_]\n",
        "                outputs[i] = x\n",
        "                \n",
        "    def load_weights(self, weightfile):\n",
        "        # Open the weights file\n",
        "        fp = open(weightfile, \"rb\")\n",
        "\n",
        "        # 第一个 160 比特的权重文件保存了 5 个 int32 值，它们构成了文件的标头\n",
        "        # The first 4 values are header information\n",
        "        # 1. Major version number\n",
        "        # 2. Minor Version Number\n",
        "        # 3. Subversion number\n",
        "        # 4. IMages seen\n",
        "        header = np.fromfile(fp, dtype=np.int32, count=5)\n",
        "        self.header = torch.from_numpy(header)\n",
        "        self.seen = self.header[3]        \n",
        "        \n",
        "        # The rest of the values are the weights\n",
        "        # Let's load them up\n",
        "        weights = np.fromfile(fp, dtype=np.float32)\n",
        "        \n",
        "        # 循环地加载权重文件到网络的模块上\n",
        "        ptr = 0\n",
        "        for i in range(len(self.module_list)):\n",
        "            module_type = self.blocks[i + 1][\"type\"]\n",
        "\n",
        "            if module_type == \"convolutional\":        \n",
        "                \n",
        "                model = self.module_list[i]\n",
        "                try:\n",
        "                    batch_normalize = int(self.blocks[i + 1][\"batch_normalize\"])\n",
        "                except:\n",
        "                    batch_normalize = 0\n",
        "\n",
        "                conv = model[0]\n",
        "                \n",
        "                #如果 batch_normalize 检查结果是 True，则我们按以下方式加载权重\n",
        "                if (batch_normalize):\n",
        "                    bn = model[1]\n",
        "\n",
        "                    # Get the number of weights of Batch Norm Layer\n",
        "                    num_bn_biases = bn.bias.numel()\n",
        "\n",
        "                    # Load the weights\n",
        "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
        "                    ptr += num_bn_biases\n",
        "\n",
        "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr += num_bn_biases\n",
        "\n",
        "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr += num_bn_biases\n",
        "\n",
        "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
        "                    ptr += num_bn_biases\n",
        "\n",
        "                    # Cast the loaded weights into dims of model weights.\n",
        "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
        "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
        "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
        "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
        "\n",
        "                    # Copy the data to model\n",
        "                    bn.bias.data.copy_(bn_biases)\n",
        "                    bn.weight.data.copy_(bn_weights)\n",
        "                    bn.running_mean.copy_(bn_running_mean)\n",
        "                    bn.running_var.copy_(bn_running_var)     \n",
        "                    \n",
        "                #如果 batch_normalize 的检查结果不是 True，只需要加载卷积层的偏置项。\n",
        "                else:\n",
        "                    # Number of biases\n",
        "                    num_biases = conv.bias.numel()\n",
        "\n",
        "                    # Load the weights\n",
        "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
        "                    ptr = ptr + num_biases\n",
        "\n",
        "                    # reshape the loaded weights according to the dims of the model weights\n",
        "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
        "\n",
        "                    # Finally copy the data\n",
        "                    conv.bias.data.copy_(conv_biases)\n",
        "                    \n",
        "                # Let us load the weights for the Convolutional layers\n",
        "                num_weights = conv.weight.numel()\n",
        "\n",
        "                # Do the same as above for weights\n",
        "                conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights])\n",
        "                ptr = ptr + num_weights\n",
        "\n",
        "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
        "                conv.weight.data.copy_(conv_weights)                    \n",
        "\n",
        "                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOYbAE6MWOy9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "创建路由层：首先，我们提取关于层属性的值，将其表示为一个整数，并保存在一个列表中。\n",
        "\n",
        " 然后我们得到一个新的称为 EmptyLayer 的层，即空的层。  \n",
        " \n",
        " <font color=red size=4>**为什么要一个空的层？？**</font>  \n",
        " \n",
        " 在 Route 模块中设计一个层，我们必须建立一个 nn.Module 对象，其作为 layers 的成员被初始化。然后，我们可以写下代码，将 forward 函数中的特征图拼接起来并向前馈送。最后，我们执行网络的某个 forward 函数的这个层。 \n",
        " \n",
        " 但拼接操作的代码相当地短和简单（在特征图上调用 torch.cat），像上述过程那样设计一个层将导致不必要的抽象，增加样板代码。取而代之，我们可以将一个假的层置于之前提出的路由层的位置上，然后直接在代表 darknet 的 nn.Module 对象的 forward 函数中执行拼接运算。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG8x1JkOOUBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmptyLayer(nn.Module):\n",
        "     def __init__(self):\n",
        "            super(EmptyLayer, self).__init__()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTpeonDvOal2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#route = EmptyLayer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpgGOgn2hspz",
        "colab_type": "text"
      },
      "source": [
        "### 定义DetectionLayer 保存用于检测边界框的锚点"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArUR8AMhaiZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DetectionLayer(nn.Module):\n",
        "    def __init__(self, anchors):\n",
        "        super(DetectionLayer, self).__init__()\n",
        "        self.anchors = anchors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lks30oz6jLdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_input():\n",
        "    img = cv2.imread(\"dog-cycle-car.png\")\n",
        "    img = cv2.resize(img, (416, 416))\n",
        "    img_ = img[:, :, ::-1].transpose((2, 0, 1))\n",
        "    img_ = img_[np.newaxis, :, :, :] / 255.0\n",
        "    img_ = torch.from_numpy(img_).float()\n",
        "    img_ = Variable(img_)\n",
        "    return img_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFD0ro05iUiS",
        "colab_type": "code",
        "outputId": "3520cbb5-f20d-4915-a21a-dbb49679af7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "blocks = parse_cfg(\"/content/gdrive/My Drive/YOLO/v3/cfg/yolov3.cfg\")\n",
        "print(create_modules(blocks))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5946cb7a9262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/YOLO/v3/cfg/yolov3.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-cc0ed0381a9f>\u001b[0m in \u001b[0;36mparse_cfg\u001b[0;34m(cfgfile)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbuilt\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mBlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrepresented\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# store the lines in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m               \u001b[0;31m# get read of the empty lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/YOLO/v3/cfg/yolov3.cfg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywbs55hJkfqm",
        "colab_type": "text"
      },
      "source": [
        "# util.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv5gWoIhygEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITl_JgY6ktoL",
        "colab_type": "text"
      },
      "source": [
        "YOLO 的输出是一个卷积特征图，包含沿特征图深度的边界框属性。边界框属性由彼此堆叠的单元格预测得出。因此，如果你需要在 (5,6) 处访问单元格的第二个边框，那么你需要通过 map[5,6, (5+C): 2*(5+C)] 将其编入索引。这种格式对于输出处理过程（例如通过目标置信度进行阈值处理、添加对中心的网格偏移、应用锚点等）很不方便。\n",
        "\n",
        "另一个问题是由于检测是在三个尺度上进行的，预测图的维度将是不同的。虽然三个特征图的维度不同，但对它们执行的输出处理过程是相似的。最好能在单个张量上执行这些运算而不是三个单独张量\n",
        "\n",
        "为了解决这些问题，引入函数 predict_transform：\n",
        "![在这里插入图片描述](https://img-blog.csdnimg.cn/20190804195418781.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrk0_w3Tklp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA=True):\n",
        "    \n",
        "    # 将上图代码化\n",
        "    batch_size = prediction.size(0)\n",
        "    stride = inp_dim // prediction.size(2)\n",
        "    grid_size = inp_dim // stride\n",
        "    bbox_attrs = 5 + num_classes\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    prediction = prediction.view(batch_size, bbox_attrs * num_anchors, grid_size * grid_size)\n",
        "    prediction = prediction.transpose(1, 2).contiguous()\n",
        "    prediction = prediction.view(batch_size, grid_size * grid_size * num_anchors, bbox_attrs)\n",
        "    \n",
        "    anchors = [(a[0] / stride, a[1] / stride) for a in anchors]\n",
        "    \n",
        "    # Sigmoid the  centre_X, centre_Y. and object confidencce\n",
        "    prediction[:, :, 0] = torch.sigmoid(prediction[:, :, 0])\n",
        "    prediction[:, :, 1] = torch.sigmoid(prediction[:, :, 1])\n",
        "    prediction[:, :, 4] = torch.sigmoid(prediction[:, :, 4])\n",
        "    \n",
        "    # Add the center offsets\n",
        "    grid_len = np.arange(grid_size)\n",
        "    a, b = np.meshgrid(grid_len, grid_len)\n",
        "\n",
        "    x_offset = torch.FloatTensor(a).view(-1, 1)\n",
        "    y_offset = torch.FloatTensor(b).view(-1, 1)\n",
        "\n",
        "    if CUDA:\n",
        "        x_offset = x_offset.cuda()\n",
        "        y_offset = y_offset.cuda()\n",
        "\n",
        "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1, num_anchors).view(-1, 2).unsqueeze(0)\n",
        "\n",
        "    prediction[:, :, :2] += x_y_offset    \n",
        "    \n",
        "    # log space transform height and the width\n",
        "    anchors = torch.FloatTensor(anchors)\n",
        "\n",
        "    if CUDA:\n",
        "        anchors = anchors.cuda()\n",
        "\n",
        "    anchors = anchors.repeat(grid_size * grid_size, 1).unsqueeze(0)\n",
        "    prediction[:, :, 2:4] = torch.exp(prediction[:, :, 2:4]) * anchors   \n",
        "    \n",
        "    # Softmax the class scores\n",
        "    prediction[:, :, 5: 5 + num_classes] = torch.sigmoid((prediction[:, :, 5: 5 + num_classes])) \n",
        "    \n",
        "    prediction[:, :, :4] *= stride\n",
        "    \n",
        "    return prediction    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u5_qUscsN-5",
        "colab_type": "text"
      },
      "source": [
        "输出满足 objectness 分数阈值和非极大值抑制（NMS），以得到后文所提到的「真实（true）」检测结果  \n",
        "创建一个名为 write_results 的函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0iB2MnDr5-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_results(prediction, confidence, num_classes, nms=True, nms_conf=0.4):\n",
        "\n",
        "    # 预测张量包含有关 B x 10647 边界框的信息\n",
        "    # 对于有低于一个阈值的 objectness 分数的每个边界框，将其每个属性的值（表示该边界框的一整行）都设为零\n",
        "    conf_mask = (prediction[:, :, 4] > confidence).float().unsqueeze(2)\n",
        "    prediction = prediction * conf_mask   \n",
        "    \n",
        "    # 现在的边界框属性是由中心坐标以及边界框的高度和宽度决定\n",
        "    # 使用每个框的两个对角坐标能更轻松地计算两个框的 IoU\n",
        "    # 将框的 (中心 x, 中心 y, 高度, 宽度) 属性转换成 (左上角 x, 左上角 y, 右下角 x, 右下角 y)\n",
        "    box_a = prediction.new(prediction.shape)\n",
        "    box_a[:, :, 0] = (prediction[:, :, 0] - prediction[:, :, 2] / 2)\n",
        "    box_a[:, :, 1] = (prediction[:, :, 1] - prediction[:, :, 3] / 2)\n",
        "    box_a[:, :, 2] = (prediction[:, :, 0] + prediction[:, :, 2] / 2)\n",
        "    box_a[:, :, 3] = (prediction[:, :, 1] + prediction[:, :, 3] / 2)\n",
        "    prediction[:, :, :4] = box_a[:, :, :4]\n",
        "    \n",
        "    # 每张图像中的「真实」检测结果的数量可能存在差异\n",
        "    # 比如，一个大小为 3 的 batch 中有 1、2、3 这 3 张图像，它们各自有 5、2、4 个「真实」检测结果\n",
        "    # 因此，一次只能完成一张图像的置信度阈值设置和 NMS，不能将所涉及的操作向量化，且必须在预测的第一个维度（包含一个 batch 中图像的索引）上循环\n",
        "    batch_size = prediction.size(0)\n",
        "    write = False\n",
        "    for ind in range(batch_size):\n",
        "        # select the image from the batch\n",
        "        image_pred = prediction[ind]\n",
        "        # confidence threshholding\n",
        "        # NMS    \n",
        "\n",
        "        # Get the class having maximum score, and the index of that class\n",
        "        # Get rid of num_classes softmax scores\n",
        "        # Add the class index and the class score of class having maximum score\n",
        "        max_conf, max_conf_score = torch.max(image_pred[:, 5:5 + num_classes], 1)\n",
        "        max_conf = max_conf.float().unsqueeze(1)\n",
        "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
        "        seq = (image_pred[:, :5], max_conf, max_conf_score)\n",
        "        image_pred = torch.cat(seq, 1)   \n",
        "        \n",
        "        # Get rid of the zero entries\n",
        "        non_zero_ind = (torch.nonzero(image_pred[:, 4]))\n",
        "\n",
        "        try:\n",
        "            image_pred_ = image_pred[non_zero_ind.squeeze(), :].view(-1, 7)\n",
        "        except:\n",
        "            continue     \n",
        "            \n",
        "        # Get the various classes detected in the image\n",
        "        img_classes = unique(image_pred_[:, -1])     \n",
        "        \n",
        "        # WE will do NMS classwise\n",
        "        for cls in img_classes:\n",
        "        \n",
        "            # get the detections with one particular class\n",
        "            cls_mask = image_pred_ * (image_pred_[:, -1] == cls).float().unsqueeze(1)\n",
        "            class_mask_ind = torch.nonzero(cls_mask[:, -2]).squeeze()\n",
        "\n",
        "            image_pred_class = image_pred_[class_mask_ind].view(-1, 7)\n",
        "\n",
        "            # sort the detections such that the entry with the maximum objectness\n",
        "            # confidence is at the top\n",
        "            conf_sort_index = torch.sort(image_pred_class[:, 4], descending=True)[1]\n",
        "            image_pred_class = image_pred_class[conf_sort_index]\n",
        "            idx = image_pred_class.size(0)  \n",
        "            \n",
        "                # 执行 NMS\n",
        "                # For each detection\n",
        "                for i in range(idx):\n",
        "                    # Get the IOUs of all boxes that come after the one we are looking at\n",
        "                    # in the loop\n",
        "                    try:\n",
        "                        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i + 1:])\n",
        "                    except ValueError:\n",
        "                        break\n",
        "\n",
        "                    except IndexError:\n",
        "                        break\n",
        "\n",
        "                    # Zero out all the detections that have IoU > treshhold\n",
        "                    iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
        "                    image_pred_class[i + 1:] *= iou_mask\n",
        "\n",
        "                    # Remove the non-zero entries\n",
        "                    non_zero_ind = torch.nonzero(image_pred_class[:, 4]).squeeze()\n",
        "                    image_pred_class = image_pred_class[non_zero_ind].view(-1, 7)\n",
        "            \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJOaAfrtujlm",
        "colab_type": "text"
      },
      "source": [
        "同一类别可能会有多个「真实」检测结果，所以我们使用 unique 函数来获取任意给定图像中存在的类别"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi5iCRlduhDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unique(tensor):\n",
        "    tensor_np = tensor.cpu().numpy()\n",
        "    unique_np = np.unique(tensor_np)\n",
        "    unique_tensor = torch.from_numpy(unique_np)\n",
        "\n",
        "    tensor_res = tensor.new(unique_tensor.shape)\n",
        "    tensor_res.copy_(unique_tensor)\n",
        "    return tensor_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhJpd_NHrl3a",
        "colab_type": "text"
      },
      "source": [
        "# 测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9G6ihYWlug7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4edfbcaf-0198-43f7-86eb-0575d7270439"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-05 02:48:23--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg.1’\n",
            "\n",
            "\ryolov3.cfg.1          0%[                    ]       0  --.-KB/s               \ryolov3.cfg.1        100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-08-05 02:48:23 (154 MB/s) - ‘yolov3.cfg.1’ saved [8342/8342]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTiq-J8pluS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "179ff56b-069e-4eff-c982-0749a3c38079"
      },
      "source": [
        "!wget wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-05 02:48:25--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2019-08-05 02:48:25--  https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png [following]\n",
            "--2019-08-05 02:48:26--  https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347445 (339K) [image/png]\n",
            "Saving to: ‘dog-cycle-car.png.1’\n",
            "\n",
            "\rdog-cycle-car.png.1   0%[                    ]       0  --.-KB/s               \rdog-cycle-car.png.1 100%[===================>] 339.30K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-08-05 02:48:26 (7.95 MB/s) - ‘dog-cycle-car.png.1’ saved [347445/347445]\n",
            "\n",
            "FINISHED --2019-08-05 02:48:26--\n",
            "Total wall clock time: 0.5s\n",
            "Downloaded: 1 files, 339K in 0.04s (7.95 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-5W-ugVnNJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "4f2bad2f-caad-4a13-d2cc-87cbfc636525"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-05 02:48:28--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights.1’\n",
            "\n",
            "yolov3.weights.1    100%[===================>] 236.52M  61.3MB/s    in 4.1s    \n",
            "\n",
            "2019-08-05 02:48:33 (57.4 MB/s) - ‘yolov3.weights.1’ saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ajaoj3olnVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0389ab30-febc-46b0-a7f4-eedd0340408b"
      },
      "source": [
        "model = Darknet(\"yolov3.cfg\")\n",
        "inp = get_test_input()\n",
        "pred = model(inp, torch.cuda.is_available())\n",
        "print (pred)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S1yXdMimrG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Darknet(\"yolov3.cfg\")\n",
        "model.load_weights(\"yolov3.weights\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9Sst-uirOVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}