{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FCOS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1TUHLBNG-SeL_7UPvxVPrGgXKY17Oa8xL",
      "authorship_tag": "ABX9TyNn2CGt2yZGsZtHfSirCn5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochengJF/DeepLearning/blob/master/FCOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldtOIgzeBD1v",
        "outputId": "6482eb07-d4b0-4934-eb07-36d643e41d48"
      },
      "source": [
        "cd /content/drive/MyDrive/My/FCOS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/My/FCOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wAjP-35C7Pt"
      },
      "source": [
        "## 相关库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q8r9Sqy5aaV"
      },
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import cv2\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchvision import transforms\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.utils.model_zoo as model_zoo\r\n",
        "\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n",
        "import xml.etree.ElementTree as ET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNRnMjh_dZAB"
      },
      "source": [
        "## Backbone\r\n",
        "\r\n",
        "ResNet系列"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCRFXav1IYim"
      },
      "source": [
        "__all__=['resnet18', 'resnet34', 'resnet50', 'resnet101','resnet152']\r\n",
        "\r\n",
        "model_urls = {\r\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\r\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\r\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\r\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\r\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\r\n",
        "}\r\n",
        "\r\n",
        "def conv3x3(in_planes, out_planes, stride=1):\r\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
        "                     padding=1, bias=False)\r\n",
        "\r\n",
        "class BasicBlock(nn.Module):\r\n",
        "    expansion = 1\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n",
        "        super(BasicBlock, self).__init__()\r\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            residual = self.downsample(x)\r\n",
        "\r\n",
        "        out += residual\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    # ResNet-B\r\n",
        "    expansion = 4\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\r\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n",
        "                               padding=1, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\r\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv3(out)\r\n",
        "        out = self.bn3(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            residual = self.downsample(x)\r\n",
        "\r\n",
        "        out += residual\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "class ResNet(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, block, layers, num_classes=1000,if_include_top=False):\r\n",
        "        self.inplanes = 64\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\r\n",
        "                               bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(64)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\r\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\r\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\r\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\r\n",
        "        if if_include_top:\r\n",
        "            self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n",
        "        self.if_include_top=if_include_top\r\n",
        "        \r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\r\n",
        "        downsample = None\r\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n",
        "                          kernel_size=1, stride=stride, bias=False),\r\n",
        "                nn.BatchNorm2d(planes * block.expansion),\r\n",
        "            )\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\r\n",
        "        self.inplanes = planes * block.expansion\r\n",
        "        for i in range(1, blocks):\r\n",
        "            layers.append(block(self.inplanes, planes))\r\n",
        "\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.maxpool(x)\r\n",
        "\r\n",
        "        x = self.layer1(x)\r\n",
        "        out3 = self.layer2(x)\r\n",
        "        out4 = self.layer3(out3)\r\n",
        "        out5 = self.layer4(out4)\r\n",
        "\r\n",
        "        if self.if_include_top:\r\n",
        "            x = self.avgpool(out5)\r\n",
        "            x = x.view(x.size(0), -1)\r\n",
        "            x = self.fc(x)\r\n",
        "            return x\r\n",
        "        else:\r\n",
        "            return (out3, out4, out5)\r\n",
        "    \r\n",
        "    def freeze_bn(self):\r\n",
        "        for layer in self.modules():\r\n",
        "            if isinstance(layer, nn.BatchNorm2d):\r\n",
        "                layer.eval()\r\n",
        "    \r\n",
        "    def freeze_stages(self, stage):\r\n",
        "        if stage >= 0:\r\n",
        "            self.bn1.eval()\r\n",
        "            for m in [self.conv1, self.bn1]:\r\n",
        "                for param in m.parameters():\r\n",
        "                    param.requires_grad = False\r\n",
        "        for i in range(1, stage + 1):\r\n",
        "            layer = getattr(self, 'layer{}'.format(i))\r\n",
        "            layer.eval()\r\n",
        "            for param in layer.parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "                \r\n",
        "def resnet18(pretrained=False, **kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-18 model.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "    \"\"\"\r\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet34(pretrained=False, **kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-34 model.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "    \"\"\"\r\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet50(pretrained=False, **kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-50 model.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "    \"\"\"\r\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        #model.load_state_dict(torch.load('./resnet50.pth'),strict=False)\r\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']), strict=False)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet101(pretrained=False, **kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-101 model.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "    \"\"\"\r\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet152(pretrained=False, **kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-152 model.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "    \"\"\"\r\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1xajpHQdiUT"
      },
      "source": [
        "## FPNNeck\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTxwCTC7_E4f"
      },
      "source": [
        "\r\n",
        "<img src=\"https://img-blog.csdnimg.cn/20201104222305325.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxMTU1NA==,size_16,color_FFFFFF,t_70#pic_center\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfYPoFf6IgRJ"
      },
      "source": [
        "class FPN(nn.Module):\r\n",
        "    '''for resnet'''\r\n",
        "    def __init__(self,features=256,use_p5=True):\r\n",
        "        super(FPN,self).__init__()\r\n",
        "        self.prj_5 = nn.Conv2d(2048, features, kernel_size=1)\r\n",
        "        self.prj_4 = nn.Conv2d(1024, features, kernel_size=1)\r\n",
        "        self.prj_3 = nn.Conv2d(512, features, kernel_size=1)\r\n",
        "        self.conv_5 = nn.Conv2d(features, features, kernel_size=3, padding=1)\r\n",
        "        self.conv_4 = nn.Conv2d(features, features, kernel_size=3, padding=1)\r\n",
        "        self.conv_3 = nn.Conv2d(features, features, kernel_size=3, padding=1)\r\n",
        "        if use_p5:\r\n",
        "            self.conv_out6 = nn.Conv2d(features, features, kernel_size=3, padding=1, stride=2)\r\n",
        "        else:\r\n",
        "            self.conv_out6 = nn.Conv2d(2048, features, kernel_size=3, padding=1, stride=2)\r\n",
        "        self.conv_out7 = nn.Conv2d(features, features, kernel_size=3, padding=1, stride=2)\r\n",
        "        self.use_p5 = use_p5\r\n",
        "        self.apply(self.init_conv_kaiming)\r\n",
        "    \r\n",
        "    def upsamplelike(self,inputs):\r\n",
        "        src,target = inputs\r\n",
        "        return F.interpolate(src, size=(target.shape[2], target.shape[3]),\r\n",
        "                    mode='nearest')\r\n",
        "    \r\n",
        "    def init_conv_kaiming(self,module):\r\n",
        "        if isinstance(module, nn.Conv2d):\r\n",
        "            nn.init.kaiming_uniform_(module.weight, a=1)\r\n",
        "\r\n",
        "            if module.bias is not None:\r\n",
        "                nn.init.constant_(module.bias, 0)\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        C3, C4, C5 = x\r\n",
        "        P5 = self.prj_5(C5)\r\n",
        "        P4 = self.prj_4(C4)\r\n",
        "        P3 = self.prj_3(C3)\r\n",
        "        \r\n",
        "        P4 = P4 + self.upsamplelike([P5,C4])\r\n",
        "        P3 = P3 + self.upsamplelike([P4,C3])\r\n",
        "\r\n",
        "        P3 = self.conv_3(P3)\r\n",
        "        P4 = self.conv_4(P4)\r\n",
        "        \r\n",
        "        P5 = self.conv_5(P5) if self.use_p5 else C5\r\n",
        "        P6 = self.conv_out6(P5)\r\n",
        "        P7 = self.conv_out7(F.relu(P6))\r\n",
        "        return [P3, P4, P5, P6, P7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7D6wZ_JhV1K"
      },
      "source": [
        "## Head\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t_ce-C-_JJH"
      },
      "source": [
        "<img src=\"https://img-blog.csdnimg.cn/2020121716082239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxMTU1NA==,size_16,color_FFFFFF,t_7\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LHpqM1HhLz1"
      },
      "source": [
        "class ScaleExp(nn.Module):\r\n",
        "    def __init__(self,init_value=1.0):\r\n",
        "        super(ScaleExp,self).__init__()\r\n",
        "        self.scale=nn.Parameter(torch.tensor([init_value],dtype=torch.float32))\r\n",
        "    def forward(self,x):\r\n",
        "        return torch.exp(x*self.scale)\r\n",
        "\r\n",
        "class ClsCntRegHead(nn.Module):\r\n",
        "    def __init__(self,in_channel,class_num,GN=True,cnt_on_reg=True,prior=0.01):\r\n",
        "        '''\r\n",
        "        Args  \r\n",
        "        in_channel  \r\n",
        "        class_num  \r\n",
        "        GN  \r\n",
        "        prior  \r\n",
        "        '''\r\n",
        "        super(ClsCntRegHead,self).__init__()\r\n",
        "        self.prior=prior\r\n",
        "        self.class_num=class_num\r\n",
        "        self.cnt_on_reg=cnt_on_reg\r\n",
        "        \r\n",
        "        cls_branch=[]\r\n",
        "        reg_branch=[]\r\n",
        "\r\n",
        "        for i in range(4):\r\n",
        "            cls_branch.append(nn.Conv2d(in_channel,in_channel,kernel_size=3,padding=1,bias=True))\r\n",
        "            if GN:\r\n",
        "                cls_branch.append(nn.GroupNorm(32,in_channel))\r\n",
        "            cls_branch.append(nn.ReLU(True))\r\n",
        "\r\n",
        "            reg_branch.append(nn.Conv2d(in_channel,in_channel,kernel_size=3,padding=1,bias=True))\r\n",
        "            if GN:\r\n",
        "                reg_branch.append(nn.GroupNorm(32,in_channel))\r\n",
        "            reg_branch.append(nn.ReLU(True))\r\n",
        "\r\n",
        "        self.cls_conv = nn.Sequential(*cls_branch)\r\n",
        "        self.reg_conv = nn.Sequential(*reg_branch)\r\n",
        "\r\n",
        "        self.cls_logits = nn.Conv2d(in_channel,class_num,kernel_size=3,padding=1)\r\n",
        "        self.cnt_logits = nn.Conv2d(in_channel,1,kernel_size=3,padding=1)\r\n",
        "        self.reg_pred = nn.Conv2d(in_channel,4,kernel_size=3,padding=1)\r\n",
        "        \r\n",
        "        self.apply(self.init_conv_RandomNormal)\r\n",
        "        \r\n",
        "        nn.init.constant_(self.cls_logits.bias,-math.log((1 - prior) / prior))\r\n",
        "        self.scale_exp = nn.ModuleList([ScaleExp(1.0) for _ in range(5)])\r\n",
        "    \r\n",
        "    def init_conv_RandomNormal(self,module,std=0.01):\r\n",
        "        if isinstance(module, nn.Conv2d):\r\n",
        "            nn.init.normal_(module.weight, std=std)\r\n",
        "\r\n",
        "            if module.bias is not None:\r\n",
        "                nn.init.constant_(module.bias, 0)\r\n",
        "    \r\n",
        "    def forward(self,inputs):\r\n",
        "        '''inputs:[P3~P7]'''\r\n",
        "        cls_logits = []\r\n",
        "        cnt_logits = []\r\n",
        "        reg_preds = []\r\n",
        "        for index, P in enumerate(inputs):\r\n",
        "            cls_conv_out = self.cls_conv(P)\r\n",
        "            reg_conv_out = self.reg_conv(P)\r\n",
        "\r\n",
        "            cls_logits.append(self.cls_logits(cls_conv_out))\r\n",
        "            if not self.cnt_on_reg:\r\n",
        "                cnt_logits.append(self.cnt_logits(cls_conv_out))\r\n",
        "            else:\r\n",
        "                cnt_logits.append(self.cnt_logits(reg_conv_out))\r\n",
        "            reg_preds.append(self.scale_exp[index](self.reg_pred(reg_conv_out)))\r\n",
        "        return cls_logits, cnt_logits, reg_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_FOioX8hZzc"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYJdhQ75hR4t"
      },
      "source": [
        "def coords_fmap2orig(feature,stride):\r\n",
        "    '''\r\n",
        "    transfor one fmap coords to orig coords\r\n",
        "    Args\r\n",
        "    featurn [batch_size,h,w,c]\r\n",
        "    stride int\r\n",
        "    Returns \r\n",
        "    coords [n,2]\r\n",
        "    '''\r\n",
        "    h, w = feature.shape[1:3]\r\n",
        "    shifts_x = torch.arange(0, w * stride, stride, dtype=torch.float32)\r\n",
        "    shifts_y = torch.arange(0, h * stride, stride, dtype=torch.float32)\r\n",
        "\r\n",
        "    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\r\n",
        "    shift_x = torch.reshape(shift_x, [-1])\r\n",
        "    shift_y = torch.reshape(shift_y, [-1])\r\n",
        "    coords = torch.stack([shift_x, shift_y], -1) + stride // 2\r\n",
        "    return coords\r\n",
        "\r\n",
        "class GenTargets(nn.Module):\r\n",
        "    def __init__(self,strides,limit_range):\r\n",
        "        super().__init__()\r\n",
        "        self.strides = strides\r\n",
        "        self.limit_range = limit_range\r\n",
        "        assert len(strides) == len(limit_range)\r\n",
        "\r\n",
        "    def forward(self,inputs):\r\n",
        "        '''\r\n",
        "        inputs  \r\n",
        "        [0]list [cls_logits,cnt_logits,reg_preds]  \r\n",
        "        cls_logits  list contains five [batch_size,class_num,h,w]  \r\n",
        "        cnt_logits  list contains five [batch_size,1,h,w]  \r\n",
        "        reg_preds   list contains five [batch_size,4,h,w]  \r\n",
        "        [1]gt_boxes [batch_size,m,4]  FloatTensor  \r\n",
        "        [2]classes [batch_size,m]  LongTensor\r\n",
        "        Returns\r\n",
        "        cls_targets:[batch_size,sum(_h*_w),1]\r\n",
        "        cnt_targets:[batch_size,sum(_h*_w),1]\r\n",
        "        reg_targets:[batch_size,sum(_h*_w),4]\r\n",
        "        '''\r\n",
        "        cls_logits, cnt_logits, reg_preds = inputs[0]\r\n",
        "        gt_boxes = inputs[1]\r\n",
        "        classes = inputs[2]\r\n",
        "        cls_targets_all_level = []\r\n",
        "        cnt_targets_all_level = []\r\n",
        "        reg_targets_all_level = []\r\n",
        "        assert len(self.strides) == len(cls_logits)\r\n",
        "        for level in range(len(cls_logits)):\r\n",
        "            level_out = [cls_logits[level],cnt_logits[level],reg_preds[level]]\r\n",
        "            level_targets = self._gen_level_targets(level_out,gt_boxes,classes,self.strides[level],\r\n",
        "                                                    self.limit_range[level])\r\n",
        "            cls_targets_all_level.append(level_targets[0])\r\n",
        "            cnt_targets_all_level.append(level_targets[1])\r\n",
        "            reg_targets_all_level.append(level_targets[2])\r\n",
        "            \r\n",
        "        return torch.cat(cls_targets_all_level,dim=1),torch.cat(cnt_targets_all_level,dim=1),torch.cat(reg_targets_all_level,dim=1)\r\n",
        "\r\n",
        "    def _gen_level_targets(self,out,gt_boxes,classes,stride,limit_range,sample_radiu_ratio=1.5):\r\n",
        "        '''\r\n",
        "        Args  \r\n",
        "        out list contains [[batch_size,class_num,h,w],[batch_size,1,h,w],[batch_size,4,h,w]]  \r\n",
        "        gt_boxes [batch_size,m,4]  \r\n",
        "        classes [batch_size,m]  \r\n",
        "        stride int  \r\n",
        "        limit_range list [min,max]  \r\n",
        "        Returns  \r\n",
        "        cls_targets,cnt_targets,reg_targets\r\n",
        "        '''\r\n",
        "        cls_logits, cnt_logits, reg_preds = out\r\n",
        "        batch_size = cls_logits.shape[0]\r\n",
        "        class_num = cls_logits.shape[1]\r\n",
        "        m = gt_boxes.shape[1]\r\n",
        "\r\n",
        "        cls_logits = cls_logits.permute(0,2,3,1)  # [batch_size, h, w, class_num]  \r\n",
        "        coords = coords_fmap2orig(cls_logits,stride).to(device=gt_boxes.device)  # [h*w, 2]\r\n",
        "\r\n",
        "        cls_logits = cls_logits.reshape((batch_size,-1,class_num))  # [batch_size, h*w, class_num]  \r\n",
        "        cnt_logits = cnt_logits.permute(0,2,3,1)\r\n",
        "        cnt_logits = cnt_logits.reshape((batch_size,-1,1))\r\n",
        "        reg_preds = reg_preds.permute(0,2,3,1)\r\n",
        "        reg_preds = reg_preds.reshape((batch_size,-1,4))\r\n",
        "\r\n",
        "        h_mul_w = cls_logits.shape[1]\r\n",
        "\r\n",
        "        x = coords[:,0]\r\n",
        "        y = coords[:,1]\r\n",
        "        l_off = x[None,:,None]-gt_boxes[...,0][:,None,:]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\r\n",
        "        t_off = y[None,:,None]-gt_boxes[...,1][:,None,:]\r\n",
        "        r_off = gt_boxes[...,2][:,None,:]-x[None,:,None]\r\n",
        "        b_off = gt_boxes[...,3][:,None,:]-y[None,:,None]\r\n",
        "        ltrb_off = torch.stack([l_off,t_off,r_off,b_off],dim=-1)  # [batch_size, h*w, m, 4]\r\n",
        "\r\n",
        "        areas = (ltrb_off[...,0]+ltrb_off[...,2]) * (ltrb_off[...,1]+ltrb_off[...,3])  # [batch_size, h*w, m]\r\n",
        "\r\n",
        "        off_min = torch.min(ltrb_off,dim=-1)[0]  # [batch_size,h*w,m]\r\n",
        "        off_max = torch.max(ltrb_off,dim=-1)[0]  # [batch_size,h*w,m]\r\n",
        "\r\n",
        "        mask_in_gtboxes = off_min>0\r\n",
        "        mask_in_level = (off_max>limit_range[0])&(off_max<=limit_range[1])\r\n",
        "\r\n",
        "        radiu = stride * sample_radiu_ratio\r\n",
        "        gt_center_x = (gt_boxes[...,0] + gt_boxes[...,2]) / 2\r\n",
        "        gt_center_y = (gt_boxes[...,1] + gt_boxes[...,3]) / 2\r\n",
        "        c_l_off = x[None,:,None] - gt_center_x[:,None,:]  # [1,h*w,1]-[batch_size,1,m]-->[batch_size,h*w,m]\r\n",
        "        c_t_off = y[None,:,None] - gt_center_y[:,None,:]\r\n",
        "        c_r_off = gt_center_x[:,None,:] - x[None,:,None]\r\n",
        "        c_b_off = gt_center_y[:,None,:] - y[None,:,None]\r\n",
        "        c_ltrb_off = torch.stack([c_l_off,c_t_off,c_r_off,c_b_off],dim=-1)  # [batch_size,h*w,m,4]\r\n",
        "        c_off_max = torch.max(c_ltrb_off,dim=-1)[0]\r\n",
        "        mask_center = c_off_max<radiu\r\n",
        "\r\n",
        "        mask_pos = mask_in_gtboxes & mask_in_level & mask_center  # [batch_size,h*w,m]\r\n",
        "\r\n",
        "        areas[~mask_pos] = 99999999\r\n",
        "        areas_min_ind = torch.min(areas,dim=-1)[1]  # [batch_size,h*w]\r\n",
        "        reg_targets = ltrb_off[torch.zeros_like(areas,dtype=torch.bool).scatter_(-1,areas_min_ind.unsqueeze(dim=-1),1)]  # [batch_size*h*w, 4]\r\n",
        "        reg_targets = torch.reshape(reg_targets,(batch_size,-1,4))  # [batch_size, h*w, 4]\r\n",
        "\r\n",
        "        classes = torch.broadcast_tensors(classes[:,None,:],areas.long())[0]  # [batch_size, h*w, m]\r\n",
        "        cls_targets = classes[torch.zeros_like(areas,dtype=torch.bool).scatter_(-1,areas_min_ind.unsqueeze(dim=-1),1)]\r\n",
        "        cls_targets = torch.reshape(cls_targets,(batch_size,-1,1))  # [batch_size, h*w, 1]\r\n",
        "\r\n",
        "        left_right_min = torch.min(reg_targets[..., 0], reg_targets[..., 2])  # [batch_size, h*w]\r\n",
        "        left_right_max = torch.max(reg_targets[..., 0], reg_targets[..., 2])\r\n",
        "        top_bottom_min = torch.min(reg_targets[..., 1], reg_targets[..., 3])\r\n",
        "        top_bottom_max = torch.max(reg_targets[..., 1], reg_targets[..., 3])\r\n",
        "        cnt_targets = ((left_right_min*top_bottom_min)/(left_right_max*top_bottom_max+1e-10)).sqrt().unsqueeze(dim=-1)  # [batch_size,h*w,1]\r\n",
        "\r\n",
        "        assert reg_targets.shape == (batch_size,h_mul_w,4)\r\n",
        "        assert cls_targets.shape == (batch_size,h_mul_w,1)\r\n",
        "        assert cnt_targets.shape == (batch_size,h_mul_w,1)\r\n",
        "\r\n",
        "        # process neg coords\r\n",
        "        mask_pos_2 = mask_pos.long().sum(dim=-1)#[batch_size,h*w]\r\n",
        "        # num_pos=mask_pos_2.sum(dim=-1)\r\n",
        "        # assert num_pos.shape==(batch_size,)\r\n",
        "        mask_pos_2 = mask_pos_2>=1\r\n",
        "        assert mask_pos_2.shape == (batch_size,h_mul_w)\r\n",
        "        cls_targets[~mask_pos_2]= 0  #[batch_size,h*w,1]\r\n",
        "        cnt_targets[~mask_pos_2]=-1\r\n",
        "        reg_targets[~mask_pos_2]=-1\r\n",
        "        \r\n",
        "        return cls_targets, cnt_targets, reg_targets\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "def compute_cls_loss(preds,targets,mask):\r\n",
        "    '''\r\n",
        "    Args  \r\n",
        "    preds: list contains five level pred [batch_size,class_num,_h,_w]\r\n",
        "    targets: [batch_size,sum(_h*_w),1]\r\n",
        "    mask: [batch_size,sum(_h*_w)]\r\n",
        "    '''\r\n",
        "    batch_size = targets.shape[0]\r\n",
        "    preds_reshape = []\r\n",
        "    class_num = preds[0].shape[1]\r\n",
        "    mask = mask.unsqueeze(dim=-1)\r\n",
        "    # mask = targets > -1 [batch_size,sum(_h*_w),1]\r\n",
        "    num_pos = torch.sum(mask,dim=[1,2]).clamp_(min=1).float()  # [batch_size,]\r\n",
        "    for pred in preds:\r\n",
        "        pred = pred.permute(0,2,3,1)\r\n",
        "        pred = torch.reshape(pred,[batch_size,-1,class_num])\r\n",
        "        preds_reshape.append(pred)\r\n",
        "    preds = torch.cat(preds_reshape,dim=1)  # [batch_size, sum(_h*_w), class_num]\r\n",
        "    assert preds.shape[:2] == targets.shape[:2]\r\n",
        "    loss = []\r\n",
        "    for batch_index in range(batch_size):\r\n",
        "        pred_pos = preds[batch_index]  # [sum(_h*_w), class_num]\r\n",
        "        target_pos = targets[batch_index]  #[sum(_h*_w), 1]\r\n",
        "        target_pos = (torch.arange(1,class_num+1,device=target_pos.device)[None,:]==target_pos).float()  #sparse --> onehot\r\n",
        "        loss.append(focal_loss_from_logits(pred_pos,target_pos).view(1))\r\n",
        "    return torch.cat(loss,dim=0) / num_pos  # [batch_size,]\r\n",
        "\r\n",
        "def compute_cnt_loss(preds,targets,mask):\r\n",
        "    '''\r\n",
        "    Args  \r\n",
        "    preds: list contains five level pred [batch_size,1,_h,_w]\r\n",
        "    targets: [batch_size,sum(_h*_w),1]\r\n",
        "    mask: [batch_size,sum(_h*_w)]\r\n",
        "    '''\r\n",
        "    batch_size = targets.shape[0]\r\n",
        "    c = targets.shape[-1]\r\n",
        "    preds_reshape = []\r\n",
        "    mask = mask.unsqueeze(dim=-1)\r\n",
        "    # mask = targets > -1  [batch_size, sum(_h*_w), 1]\r\n",
        "    num_pos = torch.sum(mask,dim=[1,2]).clamp_(min=1).float()  #[batch_size,]\r\n",
        "    for pred in preds:\r\n",
        "        pred = pred.permute(0,2,3,1)\r\n",
        "        pred = torch.reshape(pred,[batch_size,-1,c])\r\n",
        "        preds_reshape.append(pred)\r\n",
        "    preds = torch.cat(preds_reshape,dim=1)\r\n",
        "    assert preds.shape == targets.shape  #[batch_size, sum(_h*_w), 1]\r\n",
        "    loss = []\r\n",
        "    for batch_index in range(batch_size):\r\n",
        "        pred_pos=preds[batch_index][mask[batch_index]]  # [num_pos_b,]\r\n",
        "        target_pos=targets[batch_index][mask[batch_index]]  # [num_pos_b,]\r\n",
        "        assert len(pred_pos.shape) == 1\r\n",
        "        loss.append(nn.functional.binary_cross_entropy_with_logits(input=pred_pos,target=target_pos,reduction='sum').view(1))\r\n",
        "    return torch.cat(loss,dim=0)/num_pos   # [batch_size,]\r\n",
        "\r\n",
        "def compute_reg_loss(preds, targets, mask, mode='giou'):\r\n",
        "    '''\r\n",
        "    Args  \r\n",
        "    preds: list contains five level pred [batch_size,4,_h,_w]\r\n",
        "    targets: [batch_size,sum(_h*_w),4]\r\n",
        "    mask: [batch_size,sum(_h*_w)]\r\n",
        "    '''\r\n",
        "    batch_size = targets.shape[0]\r\n",
        "    c = targets.shape[-1]\r\n",
        "    preds_reshape = []\r\n",
        "    # mask = targets > -1  [batch_size, sum(_h*_w), 4]\r\n",
        "    num_pos = torch.sum(mask,dim=1).clamp_(min=1).float()  # [batch_size,]\r\n",
        "    for pred in preds:\r\n",
        "        pred = pred.permute(0, 2, 3, 1)\r\n",
        "        pred = torch.reshape(pred, [batch_size,-1,c])\r\n",
        "        preds_reshape.append(pred)\r\n",
        "    preds = torch.cat(preds_reshape,dim=1)\r\n",
        "    assert preds.shape == targets.shape  # [batch_size,sum(_h*_w), 4]\r\n",
        "    loss = []\r\n",
        "    for batch_index in range(batch_size):\r\n",
        "        pred_pos = preds[batch_index][mask[batch_index]]  # [num_pos_b, 4]\r\n",
        "        target_pos = targets[batch_index][mask[batch_index]]  # [num_pos_b, 4]\r\n",
        "        assert len(pred_pos.shape)==2\r\n",
        "        if mode == 'iou':\r\n",
        "            loss.append(iou_loss(pred_pos, target_pos).view(1))\r\n",
        "        elif mode == 'giou':\r\n",
        "            loss.append(giou_loss(pred_pos, target_pos).view(1))\r\n",
        "        else:\r\n",
        "            raise NotImplementedError(\"reg loss only implemented ['iou','giou']\")\r\n",
        "    return torch.cat(loss, dim=0) / num_pos  # [batch_size,]\r\n",
        "\r\n",
        "def iou_loss(preds,targets):\r\n",
        "    '''\r\n",
        "    Args:\r\n",
        "    preds: [n,4] ltrb\r\n",
        "    targets: [n,4]\r\n",
        "    '''\r\n",
        "    lt = torch.min(preds[:, :2],targets[:, :2])\r\n",
        "    rb = torch.min(preds[:, 2:],targets[:, 2:])\r\n",
        "    wh = (rb + lt).clamp(min=0)\r\n",
        "    overlap = wh[:, 0] * wh[:, 1]  #[n]\r\n",
        "    area1 = (preds[:, 2] + preds[:, 0]) * (preds[:, 3] + preds[:, 1])\r\n",
        "    area2 = (targets[:, 2]+targets[:, 0]) * (targets[:, 3] + targets[:, 1])\r\n",
        "    iou = overlap / (area1 + area2 - overlap)\r\n",
        "    loss = -iou.clamp(min=1e-6).log()\r\n",
        "    return loss.sum()\r\n",
        "\r\n",
        "def giou_loss(preds,targets):\r\n",
        "    '''\r\n",
        "    Args:\r\n",
        "    preds: [n,4] ltrb\r\n",
        "    targets: [n,4]\r\n",
        "    '''\r\n",
        "    lt_min = torch.min(preds[:, :2], targets[:, :2])\r\n",
        "    rb_min = torch.min(preds[:, 2:], targets[:, 2:])\r\n",
        "    wh_min = (rb_min + lt_min).clamp(min=0)\r\n",
        "    overlap = wh_min[:, 0] * wh_min[:, 1]  # [n]\r\n",
        "    area1 = (preds[:, 2] + preds[:, 0]) * (preds[:, 3] + preds[:, 1])\r\n",
        "    area2 = (targets[:, 2] + targets[:, 0]) * (targets[:, 3] + targets[:, 1])\r\n",
        "    union = (area1 + area2 - overlap)\r\n",
        "    iou = overlap / union\r\n",
        "\r\n",
        "    lt_max = torch.max(preds[:, :2], targets[:, :2])\r\n",
        "    rb_max = torch.max(preds[:, 2:], targets[:, 2:])\r\n",
        "    wh_max = (rb_max + lt_max).clamp(0)\r\n",
        "    G_area = wh_max[:, 0] * wh_max[:, 1]  #[n]\r\n",
        "\r\n",
        "    giou=iou-(G_area-union) / G_area.clamp(1e-10)\r\n",
        "    loss= 1. - giou\r\n",
        "    return loss.sum()\r\n",
        "\r\n",
        "def focal_loss_from_logits(preds, targets, gamma=2.0, alpha=0.25):\r\n",
        "    '''\r\n",
        "    Args:\r\n",
        "    preds: [n,class_num] \r\n",
        "    targets: [n,class_num]\r\n",
        "    '''\r\n",
        "    preds = preds.sigmoid()\r\n",
        "    pt = preds * targets + (1.0 - preds) * (1.0 - targets)\r\n",
        "    w = alpha * targets + (1.0 - alpha) * (1.0 - targets)\r\n",
        "    loss = - w * torch.pow((1.0 - pt), gamma) * pt.log()\r\n",
        "    return loss.sum()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class LOSS(nn.Module):\r\n",
        "    def __init__(self,config=None):\r\n",
        "        super().__init__()\r\n",
        "        if config is None:\r\n",
        "            self.config = DefaultConfig\r\n",
        "        else:\r\n",
        "            self.config = config\r\n",
        "    def forward(self,inputs):\r\n",
        "        '''\r\n",
        "        inputs list\r\n",
        "        [0]preds:  ....\r\n",
        "        [1]targets : list contains three elements [[batch_size,sum(_h*_w),1],[batch_size,sum(_h*_w),1],[batch_size,sum(_h*_w),4]]\r\n",
        "        '''\r\n",
        "        preds, targets = inputs\r\n",
        "        cls_logits, cnt_logits, reg_preds = preds\r\n",
        "        cls_targets, cnt_targets , reg_targets = targets\r\n",
        "        mask_pos = (cnt_targets>-1).squeeze(dim=-1)  # [batch_size, sum(_h*_w)]\r\n",
        "        cls_loss = compute_cls_loss(cls_logits,cls_targets,mask_pos).mean() \r\n",
        "        cnt_loss = compute_cnt_loss(cnt_logits,cnt_targets,mask_pos).mean()\r\n",
        "        reg_loss = compute_reg_loss(reg_preds,reg_targets,mask_pos).mean()\r\n",
        "        if self.config.add_centerness:\r\n",
        "            total_loss = cls_loss + cnt_loss + reg_loss\r\n",
        "            return cls_loss, cnt_loss, reg_loss, total_loss\r\n",
        "        else:\r\n",
        "            total_loss = cls_loss + reg_loss + cnt_loss * 0.0\r\n",
        "            return cls_loss, cnt_loss, reg_loss, total_loss\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4U_-UnDhg8V"
      },
      "source": [
        "## FCOS整体模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H46CzuWag7r-"
      },
      "source": [
        "class FCOS(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self,config=None):\r\n",
        "        super().__init__()\r\n",
        "        if config is None:\r\n",
        "            config = DefaultConfig\r\n",
        "        self.backbone = resnet50(pretrained=config.pretrained, if_include_top=False)\r\n",
        "        self.fpn = FPN(config.fpn_out_channels, use_p5=config.use_p5)\r\n",
        "        self.head = ClsCntRegHead(config.fpn_out_channels, config.class_num,\r\n",
        "                                config.use_GN_head, config.cnt_on_reg, config.prior)\r\n",
        "        self.config = config\r\n",
        "    def train(self, mode=True):\r\n",
        "        '''set module training mode, and frozen bn'''\r\n",
        "        super().train(mode=True)\r\n",
        "        def freeze_bn(module):\r\n",
        "            if isinstance(module,nn.BatchNorm2d):\r\n",
        "                module.eval()\r\n",
        "            classname = module.__class__.__name__\r\n",
        "            if classname.find('BatchNorm') != -1:\r\n",
        "                for p in module.parameters(): p.requires_grad=False\r\n",
        "        if self.config.freeze_bn:\r\n",
        "            self.apply(freeze_bn)\r\n",
        "            print(\"INFO===>success frozen BN\")\r\n",
        "        if self.config.freeze_stage_1:\r\n",
        "            self.backbone.freeze_stages(1)\r\n",
        "            print(\"INFO===>success frozen backbone stage1\")\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        '''\r\n",
        "        Returns\r\n",
        "        list [cls_logits,cnt_logits,reg_preds]  \r\n",
        "        cls_logits  list contains five [batch_size,class_num,h,w]\r\n",
        "        cnt_logits  list contains five [batch_size,1,h,w]\r\n",
        "        reg_preds   list contains five [batch_size,4,h,w]\r\n",
        "        '''\r\n",
        "        C3, C4, C5 = self.backbone(x)\r\n",
        "        all_P = self.fpn([C3,C4,C5])\r\n",
        "        cls_logits, cnt_logits, reg_preds = self.head(all_P)\r\n",
        "        return [cls_logits, cnt_logits, reg_preds]\r\n",
        "\r\n",
        "class DetectHead(nn.Module):\r\n",
        "    def __init__(self, score_threshold, nms_iou_threshold, max_detection_boxes_num, strides, config=None):\r\n",
        "        super().__init__()\r\n",
        "        self.score_threshold = score_threshold\r\n",
        "        self.nms_iou_threshold = nms_iou_threshold\r\n",
        "        self.max_detection_boxes_num = max_detection_boxes_num\r\n",
        "        self.strides = strides\r\n",
        "        if config is None:\r\n",
        "            self.config = DefaultConfig\r\n",
        "        else:\r\n",
        "            self.config = config\r\n",
        "    def forward(self, inputs):\r\n",
        "        '''\r\n",
        "        inputs  list [cls_logits,cnt_logits,reg_preds]  \r\n",
        "        cls_logits  list contains five [batch_size,class_num,h,w]  \r\n",
        "        cnt_logits  list contains five [batch_size,1,h,w]  \r\n",
        "        reg_preds   list contains five [batch_size,4,h,w] \r\n",
        "        '''\r\n",
        "        cls_logits,coords = self._reshape_cat_out(inputs[0],self.strides)  # [batch_size,sum(_h*_w), class_num]\r\n",
        "        cnt_logits, _ = self._reshape_cat_out(inputs[1],self.strides)   # [batch_size,sum(_h*_w), 1]\r\n",
        "        reg_preds, _ = self._reshape_cat_out(inputs[2],self.strides)   # [batch_size,sum(_h*_w), 4]\r\n",
        "\r\n",
        "        cls_preds = cls_logits.sigmoid_()\r\n",
        "        cnt_preds = cnt_logits.sigmoid_()\r\n",
        "\r\n",
        "        coords = coords.cuda() if torch.cuda.is_available() else coords\r\n",
        "\r\n",
        "        cls_scores,cls_classes = torch.max(cls_preds,dim=-1)  # [batch_size,sum(_h*_w)]\r\n",
        "        if self.config.add_centerness:\r\n",
        "            cls_scores = torch.sqrt(cls_scores*(cnt_preds.squeeze(dim=-1)))  # [batch_size, sum(_h*_w)]\r\n",
        "        cls_classes = cls_classes + 1  # [batch_size,sum(_h*_w)]\r\n",
        "\r\n",
        "        boxes = self._coords2boxes(coords,reg_preds)  # [batch_size, sum(_h*_w), 4]\r\n",
        "\r\n",
        "        # select topk\r\n",
        "        max_num = min(self.max_detection_boxes_num, cls_scores.shape[-1])\r\n",
        "        topk_ind=torch.topk(cls_scores, max_num, dim=-1, largest=True, sorted=True)[1]  # [batch_size, max_num]\r\n",
        "        _cls_scores = []\r\n",
        "        _cls_classes = []\r\n",
        "        _boxes = []\r\n",
        "        for batch in range(cls_scores.shape[0]):\r\n",
        "            _cls_scores.append(cls_scores[batch][topk_ind[batch]])  # [max_num]\r\n",
        "            _cls_classes.append(cls_classes[batch][topk_ind[batch]])  # [max_num]\r\n",
        "            _boxes.append(boxes[batch][topk_ind[batch]])  # [max_num,4]\r\n",
        "        cls_scores_topk = torch.stack(_cls_scores,dim=0)  # [batch_size,max_num]\r\n",
        "        cls_classes_topk = torch.stack(_cls_classes,dim=0)  # [batch_size,max_num]\r\n",
        "        boxes_topk = torch.stack(_boxes, dim=0)  # [batch_size,max_num,4]\r\n",
        "        assert boxes_topk.shape[-1] == 4\r\n",
        "        return self._post_process([cls_scores_topk, cls_classes_topk, boxes_topk])\r\n",
        "\r\n",
        "    def _post_process(self,preds_topk):\r\n",
        "        '''\r\n",
        "        cls_scores_topk [batch_size,max_num]\r\n",
        "        cls_classes_topk [batch_size,max_num]\r\n",
        "        boxes_topk [batch_size,max_num,4]\r\n",
        "        '''\r\n",
        "        _cls_scores_post = []\r\n",
        "        _cls_classes_post = []\r\n",
        "        _boxes_post = []\r\n",
        "        cls_scores_topk, cls_classes_topk, boxes_topk = preds_topk\r\n",
        "        for batch in range(cls_classes_topk.shape[0]):\r\n",
        "            mask = cls_scores_topk[batch]>=self.score_threshold\r\n",
        "            _cls_scores_b = cls_scores_topk[batch][mask]  # [?]\r\n",
        "            _cls_classes_b = cls_classes_topk[batch][mask]  # [?]\r\n",
        "            _boxes_b = boxes_topk[batch][mask]  #[?, 4]\r\n",
        "            nms_ind = self.batched_nms(_boxes_b,_cls_scores_b,_cls_classes_b,self.nms_iou_threshold)\r\n",
        "            _cls_scores_post.append(_cls_scores_b[nms_ind])\r\n",
        "            _cls_classes_post.append(_cls_classes_b[nms_ind])\r\n",
        "            _boxes_post.append(_boxes_b[nms_ind])\r\n",
        "        scores,classes,boxes = torch.stack(_cls_scores_post, dim=0), torch.stack(_cls_classes_post, dim=0), torch.stack(_boxes_post, dim=0)\r\n",
        "        \r\n",
        "        return scores, classes, boxes\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def box_nms(boxes, scores, thr):\r\n",
        "        '''\r\n",
        "        boxes: [?, 4]\r\n",
        "        scores: [?]\r\n",
        "        '''\r\n",
        "        if boxes.shape[0] == 0:\r\n",
        "            return torch.zeros(0, device=boxes.device).long()\r\n",
        "        assert boxes.shape[-1] == 4\r\n",
        "        x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\r\n",
        "        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n",
        "        order = scores.sort(0, descending=True)[1]\r\n",
        "        keep = []\r\n",
        "        while order.numel() > 0:\r\n",
        "            if order.numel() == 1:\r\n",
        "                i = order.item()\r\n",
        "                keep.append(i)\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                i = order[0].item()\r\n",
        "                keep.append(i)\r\n",
        "            \r\n",
        "            xmin = x1[order[1:]].clamp(min=float(x1[i]))\r\n",
        "            ymin = y1[order[1:]].clamp(min=float(y1[i]))\r\n",
        "            xmax = x2[order[1:]].clamp(max=float(x2[i]))\r\n",
        "            ymax = y2[order[1:]].clamp(max=float(y2[i]))\r\n",
        "            inter = (xmax - xmin).clamp(min=0) * (ymax - ymin).clamp(min=0)\r\n",
        "            iou = inter/(areas[i] + areas[order[1:]] - inter)\r\n",
        "            idx = (iou <= thr).nonzero().squeeze()\r\n",
        "            if idx.numel() == 0:\r\n",
        "                break\r\n",
        "            order = order[idx+1]\r\n",
        "        return torch.LongTensor(keep)\r\n",
        "\r\n",
        "    def batched_nms(self, boxes, scores, idxs, iou_threshold):\r\n",
        "        \r\n",
        "        if boxes.numel() == 0:\r\n",
        "            return torch.empty((0,), dtype=torch.int64, device=boxes.device)\r\n",
        "        # strategy: in order to perform NMS independently per class.\r\n",
        "        # we add an offset to all the boxes. The offset is dependent\r\n",
        "        # only on the class idx, and is large enough so that boxes\r\n",
        "        # from different classes do not overlap\r\n",
        "        max_coordinate = boxes.max()\r\n",
        "        offsets = idxs.to(boxes) * (max_coordinate + 1)\r\n",
        "        boxes_for_nms = boxes + offsets[:, None]\r\n",
        "        keep = self.box_nms(boxes_for_nms, scores, iou_threshold)\r\n",
        "        return keep\r\n",
        "\r\n",
        "    def _coords2boxes(self, coords, offsets):\r\n",
        "        '''\r\n",
        "        Args\r\n",
        "        coords [sum(_h*_w), 2]\r\n",
        "        offsets [batch_size, sum(_h*_w), 4] ltrb\r\n",
        "        '''\r\n",
        "        x1y1 = coords[None, :, :] - offsets[...,:2]\r\n",
        "        x2y2 = coords[None, :, :] + offsets[...,2:]  # [batch_size, sum(_h*_w),2]\r\n",
        "        boxes = torch.cat([x1y1, x2y2], dim=-1)   # [batch_size, sum(_h*_w), 4]\r\n",
        "        return boxes\r\n",
        "\r\n",
        "\r\n",
        "    def _reshape_cat_out(self, inputs, strides):\r\n",
        "        '''\r\n",
        "        Args\r\n",
        "        inputs: list contains five [batch_size,c,_h,_w]\r\n",
        "        Returns\r\n",
        "        out [batch_size,sum(_h*_w),c]\r\n",
        "        coords [sum(_h*_w),2]\r\n",
        "        '''\r\n",
        "        batch_size = inputs[0].shape[0]\r\n",
        "        c = inputs[0].shape[1]\r\n",
        "        out = []\r\n",
        "        coords = []\r\n",
        "        for pred,stride in zip(inputs, strides):\r\n",
        "            pred = pred.permute(0, 2, 3, 1)\r\n",
        "            coord = coords_fmap2orig(pred, stride).to(device = pred.device)\r\n",
        "            pred = torch.reshape(pred, [batch_size, -1, c])\r\n",
        "            out.append(pred)\r\n",
        "            coords.append(coord)\r\n",
        "        return torch.cat(out, dim=1), torch.cat(coords, dim=0)\r\n",
        "\r\n",
        "class ClipBoxes(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "    def forward(self, batch_imgs, batch_boxes):\r\n",
        "        batch_boxes = batch_boxes.clamp_(min=0)\r\n",
        "        h, w = batch_imgs.shape[2: ]\r\n",
        "        batch_boxes[..., [0, 2]] = batch_boxes[...,[0, 2]].clamp_(max=w-1)\r\n",
        "        batch_boxes[..., [1, 3]] = batch_boxes[..., [1,3]].clamp_(max=h-1)\r\n",
        "        return batch_boxes\r\n",
        "\r\n",
        "        \r\n",
        "class FCOSDetector(nn.Module):\r\n",
        "    def __init__(self, mode=\"training\", config=None):\r\n",
        "        super().__init__()\r\n",
        "        if config is None:\r\n",
        "            config = DefaultConfig\r\n",
        "        self.mode = mode\r\n",
        "        self.fcos_body = FCOS(config=config)\r\n",
        "        if mode == \"training\":\r\n",
        "            self.target_layer = GenTargets(strides=config.strides, limit_range = config.limit_range)\r\n",
        "            self.loss_layer = LOSS()\r\n",
        "        elif mode == \"inference\":\r\n",
        "            self.detection_head = DetectHead(config.score_threshold, config.nms_iou_threshold,\r\n",
        "                                            config.max_detection_boxes_num, config.strides,config)\r\n",
        "            self.clip_boxes = ClipBoxes()\r\n",
        "        \r\n",
        "    \r\n",
        "    def forward(self,inputs):\r\n",
        "        '''\r\n",
        "        inputs \r\n",
        "        [training] list  batch_imgs,batch_boxes,batch_classes\r\n",
        "        [inference] img\r\n",
        "        '''\r\n",
        "\r\n",
        "        if self.mode == \"training\":\r\n",
        "            batch_imgs, batch_boxes, batch_classes = inputs\r\n",
        "            out = self.fcos_body(batch_imgs)\r\n",
        "            targets = self.target_layer([out, batch_boxes, batch_classes])\r\n",
        "            losses = self.loss_layer([out, targets])\r\n",
        "            return losses\r\n",
        "        elif self.mode == \"inference\":\r\n",
        "            # raise NotImplementedError(\"no implement inference model\")\r\n",
        "            '''\r\n",
        "            for inference mode, img should preprocessed before feeding in net \r\n",
        "            '''\r\n",
        "            batch_imgs = inputs\r\n",
        "            out = self.fcos_body(batch_imgs)\r\n",
        "            scores, classes, boxes = self.detection_head(out)\r\n",
        "            boxes = self.clip_boxes(batch_imgs, boxes)\r\n",
        "            return scores, classes, boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-8uqSIyh6_M"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHYLkCkog7on"
      },
      "source": [
        "def flip(img, boxes):\r\n",
        "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\r\n",
        "    w = img.width\r\n",
        "    if boxes.shape[0] != 0:\r\n",
        "        xmin = w - boxes[:,2]\r\n",
        "        xmax = w - boxes[:,0]\r\n",
        "        boxes[:, 2] = xmax\r\n",
        "        boxes[:, 0] = xmin\r\n",
        "    return img, boxes\r\n",
        "\r\n",
        "class VOCDataset(torch.utils.data.Dataset):\r\n",
        "    CLASSES_NAME = (\r\n",
        "        \"__background__ \",\r\n",
        "        \"aeroplane\",\r\n",
        "        \"bicycle\",\r\n",
        "        \"bird\",\r\n",
        "        \"boat\",\r\n",
        "        \"bottle\",\r\n",
        "        \"bus\",\r\n",
        "        \"car\",\r\n",
        "        \"cat\",\r\n",
        "        \"chair\",\r\n",
        "        \"cow\",\r\n",
        "        \"diningtable\",\r\n",
        "        \"dog\",\r\n",
        "        \"horse\",\r\n",
        "        \"motorbike\",\r\n",
        "        \"person\",\r\n",
        "        \"pottedplant\",\r\n",
        "        \"sheep\",\r\n",
        "        \"sofa\",\r\n",
        "        \"train\",\r\n",
        "        \"tvmonitor\",\r\n",
        "    )\r\n",
        "    def __init__(self, root_dir, resize_size=[800,1333], split='trainval', use_difficult=False, is_train=True, augment=None):\r\n",
        "        self.root = root_dir\r\n",
        "        self.use_difficult = use_difficult\r\n",
        "        self.imgset = split\r\n",
        "\r\n",
        "        self._annopath = os.path.join(self.root, \"Annotations\", \"%s.xml\")\r\n",
        "        self._imgpath = os.path.join(self.root, \"JPEGImages\", \"%s.jpg\")\r\n",
        "        self._imgsetpath = os.path.join(self.root, \"ImageSets\", \"Main\", \"%s.txt\")\r\n",
        "\r\n",
        "        with open(self._imgsetpath%self.imgset) as f:\r\n",
        "            self.img_ids = f.readlines()\r\n",
        "        self.img_ids = [x.strip() for x in self.img_ids]\r\n",
        "        self.name2id = dict(zip(VOCDataset.CLASSES_NAME, range(len(VOCDataset.CLASSES_NAME))))\r\n",
        "        self.id2name = {v:k for k,v in self.name2id.items()}\r\n",
        "        self.resize_size = resize_size\r\n",
        "        self.mean = [0.485, 0.456, 0.406]\r\n",
        "        self.std = [0.229, 0.224, 0.225]\r\n",
        "        self.train = is_train\r\n",
        "        self.augment = augment\r\n",
        "        print(\"INFO=====>voc dataset init finished  ! !\")\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.img_ids)\r\n",
        "\r\n",
        "    def __getitem__(self,index):\r\n",
        "\r\n",
        "        img_id = self.img_ids[index]\r\n",
        "        img = Image.open(self._imgpath%img_id)\r\n",
        "\r\n",
        "        anno = ET.parse(self._annopath%img_id).getroot()\r\n",
        "        boxes = []\r\n",
        "        classes = []\r\n",
        "        for obj in anno.iter(\"object\"):\r\n",
        "            difficult = int(obj.find(\"difficult\").text) == 1\r\n",
        "            if not self.use_difficult and difficult:\r\n",
        "                continue\r\n",
        "            _box = obj.find(\"bndbox\")\r\n",
        "            # Make pixel indexes 0-based\r\n",
        "            # Refer to \"https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/pascal_voc.py#L208-L211\"\r\n",
        "            box=[\r\n",
        "                _box.find(\"xmin\").text,\r\n",
        "                _box.find(\"ymin\").text,\r\n",
        "                _box.find(\"xmax\").text,\r\n",
        "                _box.find(\"ymax\").text,\r\n",
        "            ]\r\n",
        "            TO_REMOVE=1\r\n",
        "            box = tuple(\r\n",
        "                map(lambda x: x - TO_REMOVE, list(map(float, box)))\r\n",
        "            )\r\n",
        "            boxes.append(box)\r\n",
        "\r\n",
        "            name = obj.find(\"name\").text.lower().strip()\r\n",
        "            classes.append(self.name2id[name])\r\n",
        "\r\n",
        "        boxes = np.array(boxes, dtype=np.float32)\r\n",
        "        if self.train:\r\n",
        "            if random.random() < 0.5:\r\n",
        "                img, boxes = flip(img, boxes)\r\n",
        "            if self.augment is not None:\r\n",
        "                img, boxes = self.augment(img, boxes)\r\n",
        "        img = np.array(img)\r\n",
        "        img,boxes = self.preprocess_img_boxes(img,boxes, self.resize_size)\r\n",
        "\r\n",
        "        img = transforms.ToTensor()(img)\r\n",
        "        boxes = torch.from_numpy(boxes)\r\n",
        "        classes = torch.LongTensor(classes)\r\n",
        "\r\n",
        "        return img, boxes, classes\r\n",
        "\r\n",
        "\r\n",
        "    def preprocess_img_boxes(self, image, boxes, input_ksize):\r\n",
        "        '''\r\n",
        "        resize image and bboxes\r\n",
        "        Returns\r\n",
        "        image_paded: input_ksize\r\n",
        "        bboxes: [None,4]\r\n",
        "        '''\r\n",
        "        min_side, max_side = input_ksize\r\n",
        "        h, w, _ = image.shape\r\n",
        "\r\n",
        "        smallest_side = min(w, h)\r\n",
        "        largest_side = max(w, h)\r\n",
        "        scale = min_side / smallest_side\r\n",
        "        if largest_side * scale > max_side:\r\n",
        "            scale = max_side / largest_side\r\n",
        "        nw, nh  = int(scale*w), int(scale*h)\r\n",
        "        image_resized = cv2.resize(image, (nw, nh))\r\n",
        "\r\n",
        "        pad_w = 32 - nw % 32\r\n",
        "        pad_h = 32 - nh % 32\r\n",
        "\r\n",
        "        image_paded = np.zeros(shape=[nh+pad_h, nw+pad_w, 3], dtype=np.uint8)\r\n",
        "        image_paded[:nh, :nw, :] = image_resized\r\n",
        "\r\n",
        "        if boxes is None:\r\n",
        "            return image_paded\r\n",
        "        else:\r\n",
        "            boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\r\n",
        "            boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\r\n",
        "            return image_paded, boxes\r\n",
        "    \r\n",
        "\r\n",
        "    def collate_fn(self,data):\r\n",
        "        imgs_list, boxes_list, classes_list = zip(*data)\r\n",
        "        assert len(imgs_list) == len(boxes_list) == len(classes_list)\r\n",
        "        batch_size = len(boxes_list)\r\n",
        "        pad_imgs_list = []\r\n",
        "        pad_boxes_list = []\r\n",
        "        pad_classes_list = []\r\n",
        "\r\n",
        "        h_list = [int(s.shape[1]) for s in imgs_list]\r\n",
        "        w_list = [int(s.shape[2]) for s in imgs_list]\r\n",
        "        max_h = np.array(h_list).max()\r\n",
        "        max_w = np.array(w_list).max()\r\n",
        "        for i in range(batch_size):\r\n",
        "            img = imgs_list[i]\r\n",
        "            pad_imgs_list.append(transforms.Normalize(self.mean, self.std,inplace=True)(torch.nn.functional.pad(img, (0, int(max_w-img.shape[2]), 0, int(max_h-img.shape[1])), value=0.)))\r\n",
        "\r\n",
        "\r\n",
        "        max_num = 0\r\n",
        "        for i in range(batch_size):\r\n",
        "            n = boxes_list[i].shape[0]\r\n",
        "            if n > max_num : max_num = n\r\n",
        "        for i in range(batch_size):\r\n",
        "            pad_boxes_list.append(torch.nn.functional.pad(boxes_list[i], (0, 0, 0, max_num-boxes_list[i].shape[0]), value=-1))\r\n",
        "            pad_classes_list.append(torch.nn.functional.pad(classes_list[i], (0, max_num-classes_list[i].shape[0]), value=-1))\r\n",
        "\r\n",
        "\r\n",
        "        batch_boxes = torch.stack(pad_boxes_list)\r\n",
        "        batch_classes = torch.stack(pad_classes_list)\r\n",
        "        batch_imgs = torch.stack(pad_imgs_list)\r\n",
        "\r\n",
        "        return batch_imgs, batch_boxes, batch_classes\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm-pbrU4iPat"
      },
      "source": [
        "#### 数据增强"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzZjZpcbhuDB"
      },
      "source": [
        "class Transforms(object):\r\n",
        "    def __init__(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def __call__(self, img, boxes):\r\n",
        "        if random.random() < 0.3:\r\n",
        "            img, boxes = colorJitter(img, boxes)\r\n",
        "        if random.random() < 0.5:\r\n",
        "            img, boxes = random_rotation(img, boxes)\r\n",
        "        if random.random() < 0.5:\r\n",
        "            img, boxes = random_crop_resize(img, boxes)\r\n",
        "        return img, boxes\r\n",
        "\r\n",
        "\r\n",
        "def colorJitter(img, boxes, brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1):\r\n",
        "    img = transforms.ColorJitter(brightness=brightness,\r\n",
        "                contrast=contrast, saturation=saturation, hue=hue)(img)\r\n",
        "    return img, boxes\r\n",
        "\r\n",
        "\r\n",
        "def random_rotation(img, boxes, degree=10):\r\n",
        "    d = random.uniform(-degree, degree)\r\n",
        "    w, h = img.size\r\n",
        "    rx0, ry0 = w / 2.0, h / 2.0\r\n",
        "    img = img.rotate(d)\r\n",
        "    a = -d / 180.0 * math.pi\r\n",
        "    boxes = torch.from_numpy(boxes)\r\n",
        "    new_boxes = torch.zeros_like(boxes)\r\n",
        "    new_boxes[:, 0] = boxes[:, 1]\r\n",
        "    new_boxes[:, 1] = boxes[:, 0]\r\n",
        "    new_boxes[:, 2] = boxes[:, 3]\r\n",
        "    new_boxes[:, 3] = boxes[:, 2]\r\n",
        "    for i in range(boxes.shape[0]):\r\n",
        "        ymin, xmin, ymax, xmax = new_boxes[i, :]\r\n",
        "        xmin, ymin, xmax, ymax = float(xmin), float(ymin), float(xmax), float(ymax)\r\n",
        "        x0, y0 = xmin, ymin\r\n",
        "        x1, y1 = xmin, ymax\r\n",
        "        x2, y2 = xmax, ymin\r\n",
        "        x3, y3 = xmax, ymax\r\n",
        "        z = torch.FloatTensor([[y0, x0], [y1, x1], [y2, x2], [y3, x3]])\r\n",
        "        tp = torch.zeros_like(z)\r\n",
        "        tp[:, 1] = (z[:, 1] - rx0) * math.cos(a) - (z[:, 0] - ry0) * math.sin(a) + rx0\r\n",
        "        tp[:, 0] = (z[:, 1] - rx0) * math.sin(a) + (z[:, 0] - ry0) * math.cos(a) + ry0\r\n",
        "        ymax, xmax = torch.max(tp, dim=0)[0]\r\n",
        "        ymin, xmin = torch.min(tp, dim=0)[0]\r\n",
        "        new_boxes[i] = torch.stack([ymin, xmin, ymax, xmax])\r\n",
        "    new_boxes[:, 1::2].clamp_(min=0, max=w - 1)\r\n",
        "    new_boxes[:, 0::2].clamp_(min=0, max=h - 1)\r\n",
        "    boxes[:, 0] = new_boxes[:, 1]\r\n",
        "    boxes[:, 1] = new_boxes[:, 0]\r\n",
        "    boxes[:, 2] = new_boxes[:, 3]\r\n",
        "    boxes[:, 3] = new_boxes[:, 2]\r\n",
        "    boxes = boxes.numpy()\r\n",
        "    return img, boxes\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def _box_inter(box1, box2):\r\n",
        "    tl = torch.max(box1[:, None, :2], box2[:, :2])  # [n,m,2]\r\n",
        "    br = torch.min(box1[:, None, 2:], box2[:, 2:])  # [n,m,2]\r\n",
        "    hw = (br-tl).clamp(min=0)  # [n,m,2]\r\n",
        "    inter = hw[:, :, 0] * hw[:, :, 1]  # [n,m]\r\n",
        "    return inter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def random_crop_resize(img, boxes, crop_scale_min=0.2, aspect_ratio=[3./4, 4./3], remain_min=0.7, attempt_max=10):\r\n",
        "    success = False\r\n",
        "    boxes = torch.from_numpy(boxes)\r\n",
        "    for attempt in range(attempt_max):\r\n",
        "        # choose crop size\r\n",
        "        area = img.size[0] * img.size[1]\r\n",
        "        target_area = random.uniform(crop_scale_min, 1.0) * area\r\n",
        "        aspect_ratio_ = random.uniform(aspect_ratio[0], aspect_ratio[1])\r\n",
        "        w = int(round(math.sqrt(target_area * aspect_ratio_)))\r\n",
        "        h = int(round(math.sqrt(target_area / aspect_ratio_)))\r\n",
        "        if random.random() < 0.5:\r\n",
        "            w, h = h, w\r\n",
        "        # if size is right then random crop\r\n",
        "        if w <= img.size[0] and h <= img.size[1]:\r\n",
        "            x = random.randint(0, img.size[0] - w)\r\n",
        "            y = random.randint(0, img.size[1] - h)\r\n",
        "            # check\r\n",
        "            crop_box = torch.FloatTensor([[x, y, x+w, y+h]])\r\n",
        "            inter = _box_inter(crop_box, boxes)  # [1,N] N can be zero\r\n",
        "            box_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])  # [N]\r\n",
        "            mask = inter > 0.0001  # [1,N] N can be zero\r\n",
        "            inter = inter[mask]  # [1,S] S can be zero\r\n",
        "            box_area = box_area[mask.view(-1)] # [S]\r\n",
        "            box_remain = inter.view(-1) / box_area # [S]\r\n",
        "            if box_remain.shape[0] != 0:\r\n",
        "                if bool(torch.min(box_remain > remain_min)):\r\n",
        "                    success = True\r\n",
        "                    break\r\n",
        "            else:\r\n",
        "                success = True\r\n",
        "                break\r\n",
        "    if success:\r\n",
        "        img = img.crop((x, y, x+w, y+h))\r\n",
        "        boxes -= torch.Tensor([x, y, x, y])\r\n",
        "        boxes[:,1::2].clamp_(min=0, max=h-1)\r\n",
        "        boxes[:,0::2].clamp_(min=0, max=w-1)\r\n",
        "        # ow, oh = (size, size)\r\n",
        "        # sw = float(ow) / img.size[0]\r\n",
        "        # sh = float(oh) / img.size[1]\r\n",
        "        # img = img.resize((ow,oh), Image.BILINEAR)\r\n",
        "        # boxes *= torch.FloatTensor([sw,sh,sw,sh])\r\n",
        "    boxes = boxes.numpy()\r\n",
        "    return img, boxes\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPh9zr12-qaK"
      },
      "source": [
        "## 配置"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNVb2KeXht6_"
      },
      "source": [
        "class DefaultConfig():\r\n",
        "    #backbone\r\n",
        "    pretrained=True\r\n",
        "    freeze_stage_1=True\r\n",
        "    freeze_bn=True\r\n",
        "\r\n",
        "    #fpn\r\n",
        "    fpn_out_channels=256\r\n",
        "    use_p5=True\r\n",
        "    \r\n",
        "    #head\r\n",
        "    class_num=80\r\n",
        "    use_GN_head=True\r\n",
        "    prior=0.01\r\n",
        "    add_centerness=True\r\n",
        "    cnt_on_reg=True\r\n",
        "\r\n",
        "    #training\r\n",
        "    strides = [8, 16, 32, 64, 128]\r\n",
        "    limit_range = [[-1, 64], [64, 128], [128, 256], [256, 512], [512, 999999]]\r\n",
        "\r\n",
        "    #inference\r\n",
        "    score_threshold = 0.05\r\n",
        "    nms_iou_threshold = 0.6\r\n",
        "    max_detection_boxes_num = 1000\r\n",
        "\r\n",
        "class optConfig():\r\n",
        "    epochs = 4\r\n",
        "    batch_size = 4\r\n",
        "    n_cpu = 4\r\n",
        "    n_gpu = \"0\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC5bV2z_-ts_"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWFzyY9Tht4E",
        "outputId": "afbd1476-b000-4dd1-ae02-e2c0d504582c"
      },
      "source": [
        "opt = optConfig()\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.n_gpu\r\n",
        "torch.manual_seed(0)\r\n",
        "torch.cuda.manual_seed(0)\r\n",
        "torch.cuda.manual_seed_all(0)\r\n",
        "np.random.seed(0)\r\n",
        "cudnn.benchmark = False\r\n",
        "cudnn.deterministic = True\r\n",
        "random.seed(0)\r\n",
        "transform = Transforms()\r\n",
        "train_dataset = VOCDataset(root_dir='/content/drive/MyDrive/My/Datasets/VOCdevkit/VOC2012', resize_size=[800, 1333],\r\n",
        "                           split='trainval', use_difficult=False, is_train=True, augment=transform)\r\n",
        "if not os.path.exists(\"./checkpoint\"):\r\n",
        "    os.makedirs(\"./checkpoint\")\r\n",
        "\r\n",
        "model = FCOSDetector(mode=\"training\").cuda()\r\n",
        "model = torch.nn.DataParallel(model)\r\n",
        "# model.load_state_dict(torch.load('./checkpoint/model_6.pth'))\r\n",
        "\r\n",
        "BATCH_SIZE = opt.batch_size\r\n",
        "EPOCHS = opt.epochs\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\r\n",
        "                                           collate_fn=train_dataset.collate_fn,\r\n",
        "                                           num_workers=opt.n_cpu, worker_init_fn=np.random.seed(0))\r\n",
        "print(\"total_images : {}\".format(len(train_dataset)))\r\n",
        "steps_per_epoch = len(train_dataset) // BATCH_SIZE\r\n",
        "TOTAL_STEPS = steps_per_epoch * EPOCHS\r\n",
        "WARMPUP_STEPS = 501\r\n",
        "\r\n",
        "GLOBAL_STEPS = 1\r\n",
        "LR_INIT = 2e-3\r\n",
        "LR_END = 2e-5\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR_INIT, momentum=0.9, weight_decay=0.0001)\r\n",
        "\r\n",
        "\r\n",
        "model.train()\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "    for epoch_step, data in enumerate(train_loader):\r\n",
        "\r\n",
        "        batch_imgs, batch_boxes, batch_classes = data\r\n",
        "        batch_imgs = batch_imgs.cuda()\r\n",
        "        batch_boxes = batch_boxes.cuda()\r\n",
        "        batch_classes = batch_classes.cuda()\r\n",
        "\r\n",
        "        if GLOBAL_STEPS < WARMPUP_STEPS:\r\n",
        "           lr = float(GLOBAL_STEPS / WARMPUP_STEPS * LR_INIT)\r\n",
        "           for param in optimizer.param_groups:\r\n",
        "               param['lr'] = lr\r\n",
        "        if GLOBAL_STEPS == 20001:\r\n",
        "           lr = LR_INIT * 0.1\r\n",
        "           for param in optimizer.param_groups:\r\n",
        "               param['lr'] = lr\r\n",
        "        if GLOBAL_STEPS == 27001:\r\n",
        "           lr = LR_INIT * 0.01\r\n",
        "           for param in optimizer.param_groups:\r\n",
        "              param['lr'] = lr\r\n",
        "        start_time = time.time()\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        losses = model([batch_imgs, batch_boxes, batch_classes])\r\n",
        "        loss = losses[-1]\r\n",
        "        loss.mean().backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        end_time = time.time()\r\n",
        "        cost_time = int((end_time - start_time) * 1000)\r\n",
        "        print(\r\n",
        "            \"global_steps:%d epoch:%d steps:%d/%d cls_loss:%.4f cnt_loss:%.4f reg_loss:%.4f cost_time:%dms lr=%.4e total_loss:%.4f\" % \\\r\n",
        "            (GLOBAL_STEPS, epoch + 1, epoch_step + 1, steps_per_epoch, losses[0].mean(), losses[1].mean(),\r\n",
        "             losses[2].mean(), cost_time, lr, loss.mean()))\r\n",
        "\r\n",
        "        GLOBAL_STEPS += 1\r\n",
        "    torch.save(model.state_dict(),\r\n",
        "               \"./checkpoint/model_{}.pth\".format(epoch + 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO=====>voc dataset init finished  ! !\n",
            "total_images : 11540\n",
            "INFO===>success frozen BN\n",
            "INFO===>success frozen backbone stage1\n",
            "global_steps:1 epoch:1 steps:1/2885 cls_loss:1.3676 cnt_loss:0.7074 reg_loss:0.9999 cost_time:890ms lr=3.9920e-06 total_loss:3.0749\n",
            "global_steps:2 epoch:1 steps:2/2885 cls_loss:1.3298 cnt_loss:0.6810 reg_loss:0.9998 cost_time:930ms lr=7.9840e-06 total_loss:3.0106\n",
            "global_steps:3 epoch:1 steps:3/2885 cls_loss:1.2676 cnt_loss:0.7038 reg_loss:0.9999 cost_time:911ms lr=1.1976e-05 total_loss:2.9713\n",
            "global_steps:4 epoch:1 steps:4/2885 cls_loss:1.3246 cnt_loss:0.6869 reg_loss:0.9997 cost_time:950ms lr=1.5968e-05 total_loss:3.0112\n",
            "global_steps:5 epoch:1 steps:5/2885 cls_loss:1.2908 cnt_loss:0.6778 reg_loss:0.9999 cost_time:820ms lr=1.9960e-05 total_loss:2.9685\n",
            "global_steps:6 epoch:1 steps:6/2885 cls_loss:1.3248 cnt_loss:0.6936 reg_loss:1.0000 cost_time:978ms lr=2.3952e-05 total_loss:3.0184\n",
            "global_steps:7 epoch:1 steps:7/2885 cls_loss:1.2570 cnt_loss:0.6787 reg_loss:1.0000 cost_time:820ms lr=2.7944e-05 total_loss:2.9356\n",
            "global_steps:8 epoch:1 steps:8/2885 cls_loss:1.4241 cnt_loss:0.6859 reg_loss:0.9989 cost_time:851ms lr=3.1936e-05 total_loss:3.1089\n",
            "global_steps:9 epoch:1 steps:9/2885 cls_loss:1.3340 cnt_loss:0.6735 reg_loss:1.0000 cost_time:730ms lr=3.5928e-05 total_loss:3.0075\n",
            "global_steps:10 epoch:1 steps:10/2885 cls_loss:1.4283 cnt_loss:0.6856 reg_loss:0.9999 cost_time:985ms lr=3.9920e-05 total_loss:3.1139\n",
            "global_steps:11 epoch:1 steps:11/2885 cls_loss:1.3595 cnt_loss:0.6494 reg_loss:1.0000 cost_time:872ms lr=4.3912e-05 total_loss:3.0090\n",
            "global_steps:12 epoch:1 steps:12/2885 cls_loss:1.2472 cnt_loss:0.6848 reg_loss:0.9999 cost_time:877ms lr=4.7904e-05 total_loss:2.9318\n",
            "global_steps:13 epoch:1 steps:13/2885 cls_loss:1.2892 cnt_loss:0.6615 reg_loss:0.9999 cost_time:814ms lr=5.1896e-05 total_loss:2.9505\n",
            "global_steps:14 epoch:1 steps:14/2885 cls_loss:1.2429 cnt_loss:0.6822 reg_loss:0.9999 cost_time:817ms lr=5.5888e-05 total_loss:2.9249\n",
            "global_steps:15 epoch:1 steps:15/2885 cls_loss:1.2674 cnt_loss:0.6466 reg_loss:0.9999 cost_time:936ms lr=5.9880e-05 total_loss:2.9139\n",
            "global_steps:16 epoch:1 steps:16/2885 cls_loss:1.2102 cnt_loss:0.7227 reg_loss:0.9989 cost_time:1104ms lr=6.3872e-05 total_loss:2.9318\n",
            "global_steps:17 epoch:1 steps:17/2885 cls_loss:1.2533 cnt_loss:0.6450 reg_loss:1.0000 cost_time:780ms lr=6.7864e-05 total_loss:2.8983\n",
            "global_steps:18 epoch:1 steps:18/2885 cls_loss:1.2851 cnt_loss:0.6981 reg_loss:0.9999 cost_time:839ms lr=7.1856e-05 total_loss:2.9831\n",
            "global_steps:19 epoch:1 steps:19/2885 cls_loss:1.2027 cnt_loss:0.6624 reg_loss:1.0000 cost_time:988ms lr=7.5848e-05 total_loss:2.8651\n",
            "global_steps:20 epoch:1 steps:20/2885 cls_loss:1.1537 cnt_loss:0.6806 reg_loss:0.9999 cost_time:881ms lr=7.9840e-05 total_loss:2.8341\n",
            "global_steps:21 epoch:1 steps:21/2885 cls_loss:1.1859 cnt_loss:0.6343 reg_loss:1.0000 cost_time:1002ms lr=8.3832e-05 total_loss:2.8202\n",
            "global_steps:22 epoch:1 steps:22/2885 cls_loss:1.3064 cnt_loss:0.6612 reg_loss:1.0000 cost_time:1386ms lr=8.7824e-05 total_loss:2.9676\n",
            "global_steps:23 epoch:1 steps:23/2885 cls_loss:1.1826 cnt_loss:0.6879 reg_loss:0.9994 cost_time:915ms lr=9.1816e-05 total_loss:2.8699\n",
            "global_steps:24 epoch:1 steps:24/2885 cls_loss:1.1789 cnt_loss:0.6251 reg_loss:1.0000 cost_time:729ms lr=9.5808e-05 total_loss:2.8040\n",
            "global_steps:25 epoch:1 steps:25/2885 cls_loss:1.1598 cnt_loss:0.6589 reg_loss:0.9998 cost_time:909ms lr=9.9800e-05 total_loss:2.8185\n",
            "global_steps:26 epoch:1 steps:26/2885 cls_loss:1.1558 cnt_loss:0.6371 reg_loss:1.0000 cost_time:854ms lr=1.0379e-04 total_loss:2.7929\n",
            "global_steps:27 epoch:1 steps:27/2885 cls_loss:1.1856 cnt_loss:0.6532 reg_loss:0.9999 cost_time:781ms lr=1.0778e-04 total_loss:2.8387\n",
            "global_steps:28 epoch:1 steps:28/2885 cls_loss:1.2194 cnt_loss:0.6566 reg_loss:1.0000 cost_time:710ms lr=1.1178e-04 total_loss:2.8760\n",
            "global_steps:29 epoch:1 steps:29/2885 cls_loss:1.2910 cnt_loss:0.7009 reg_loss:0.9994 cost_time:973ms lr=1.1577e-04 total_loss:2.9913\n",
            "global_steps:30 epoch:1 steps:30/2885 cls_loss:1.0186 cnt_loss:0.6468 reg_loss:1.0000 cost_time:1002ms lr=1.1976e-04 total_loss:2.6654\n",
            "global_steps:31 epoch:1 steps:31/2885 cls_loss:1.2053 cnt_loss:0.6493 reg_loss:1.0000 cost_time:1114ms lr=1.2375e-04 total_loss:2.8545\n",
            "global_steps:32 epoch:1 steps:32/2885 cls_loss:1.8065 cnt_loss:0.5036 reg_loss:0.7500 cost_time:1309ms lr=1.2774e-04 total_loss:3.0601\n",
            "global_steps:33 epoch:1 steps:33/2885 cls_loss:1.0857 cnt_loss:0.6624 reg_loss:1.0000 cost_time:818ms lr=1.3174e-04 total_loss:2.7481\n",
            "global_steps:34 epoch:1 steps:34/2885 cls_loss:1.2034 cnt_loss:0.6681 reg_loss:1.0003 cost_time:887ms lr=1.3573e-04 total_loss:2.8717\n",
            "global_steps:35 epoch:1 steps:35/2885 cls_loss:1.0888 cnt_loss:0.6579 reg_loss:0.9999 cost_time:873ms lr=1.3972e-04 total_loss:2.7465\n",
            "global_steps:36 epoch:1 steps:36/2885 cls_loss:0.9949 cnt_loss:0.6870 reg_loss:1.0000 cost_time:876ms lr=1.4371e-04 total_loss:2.6819\n",
            "global_steps:37 epoch:1 steps:37/2885 cls_loss:1.0630 cnt_loss:0.6490 reg_loss:0.9999 cost_time:796ms lr=1.4770e-04 total_loss:2.7119\n",
            "global_steps:38 epoch:1 steps:38/2885 cls_loss:0.9998 cnt_loss:0.6501 reg_loss:0.9998 cost_time:731ms lr=1.5170e-04 total_loss:2.6497\n",
            "global_steps:39 epoch:1 steps:39/2885 cls_loss:1.0267 cnt_loss:0.6539 reg_loss:0.9999 cost_time:874ms lr=1.5569e-04 total_loss:2.6805\n",
            "global_steps:40 epoch:1 steps:40/2885 cls_loss:1.1341 cnt_loss:0.6293 reg_loss:0.9999 cost_time:853ms lr=1.5968e-04 total_loss:2.7633\n",
            "global_steps:41 epoch:1 steps:41/2885 cls_loss:1.1107 cnt_loss:0.6548 reg_loss:1.0000 cost_time:867ms lr=1.6367e-04 total_loss:2.7654\n",
            "global_steps:42 epoch:1 steps:42/2885 cls_loss:1.1610 cnt_loss:0.6548 reg_loss:0.9998 cost_time:885ms lr=1.6766e-04 total_loss:2.8156\n",
            "global_steps:43 epoch:1 steps:43/2885 cls_loss:1.1980 cnt_loss:0.6663 reg_loss:1.0000 cost_time:1215ms lr=1.7166e-04 total_loss:2.8642\n",
            "global_steps:44 epoch:1 steps:44/2885 cls_loss:1.3409 cnt_loss:0.5008 reg_loss:0.7500 cost_time:1012ms lr=1.7565e-04 total_loss:2.5916\n",
            "global_steps:45 epoch:1 steps:45/2885 cls_loss:0.9182 cnt_loss:0.6570 reg_loss:0.9999 cost_time:816ms lr=1.7964e-04 total_loss:2.5751\n",
            "global_steps:46 epoch:1 steps:46/2885 cls_loss:0.9287 cnt_loss:0.6330 reg_loss:0.9999 cost_time:722ms lr=1.8363e-04 total_loss:2.5616\n",
            "global_steps:47 epoch:1 steps:47/2885 cls_loss:0.9686 cnt_loss:0.6401 reg_loss:0.9999 cost_time:728ms lr=1.8762e-04 total_loss:2.6086\n",
            "global_steps:48 epoch:1 steps:48/2885 cls_loss:1.0282 cnt_loss:0.6407 reg_loss:0.9999 cost_time:812ms lr=1.9162e-04 total_loss:2.6688\n",
            "global_steps:49 epoch:1 steps:49/2885 cls_loss:1.0340 cnt_loss:0.6447 reg_loss:1.0000 cost_time:1071ms lr=1.9561e-04 total_loss:2.6787\n",
            "global_steps:50 epoch:1 steps:50/2885 cls_loss:1.0538 cnt_loss:0.6641 reg_loss:1.0000 cost_time:925ms lr=1.9960e-04 total_loss:2.7178\n",
            "global_steps:51 epoch:1 steps:51/2885 cls_loss:1.0373 cnt_loss:0.6629 reg_loss:0.9999 cost_time:879ms lr=2.0359e-04 total_loss:2.7002\n",
            "global_steps:52 epoch:1 steps:52/2885 cls_loss:1.0682 cnt_loss:0.5056 reg_loss:0.7499 cost_time:926ms lr=2.0758e-04 total_loss:2.3237\n",
            "global_steps:53 epoch:1 steps:53/2885 cls_loss:0.9493 cnt_loss:0.6527 reg_loss:0.9999 cost_time:750ms lr=2.1158e-04 total_loss:2.6019\n",
            "global_steps:54 epoch:1 steps:54/2885 cls_loss:0.9839 cnt_loss:0.6439 reg_loss:0.9996 cost_time:813ms lr=2.1557e-04 total_loss:2.6274\n",
            "global_steps:55 epoch:1 steps:55/2885 cls_loss:1.0809 cnt_loss:0.6654 reg_loss:0.9999 cost_time:1136ms lr=2.1956e-04 total_loss:2.7462\n",
            "global_steps:56 epoch:1 steps:56/2885 cls_loss:0.8724 cnt_loss:0.6731 reg_loss:0.9997 cost_time:825ms lr=2.2355e-04 total_loss:2.5452\n",
            "global_steps:57 epoch:1 steps:57/2885 cls_loss:0.9789 cnt_loss:0.6572 reg_loss:0.9998 cost_time:1130ms lr=2.2754e-04 total_loss:2.6358\n",
            "global_steps:58 epoch:1 steps:58/2885 cls_loss:0.9526 cnt_loss:0.6558 reg_loss:0.9998 cost_time:928ms lr=2.3154e-04 total_loss:2.6082\n",
            "global_steps:59 epoch:1 steps:59/2885 cls_loss:1.1306 cnt_loss:0.6432 reg_loss:1.0000 cost_time:788ms lr=2.3553e-04 total_loss:2.7737\n",
            "global_steps:60 epoch:1 steps:60/2885 cls_loss:0.8611 cnt_loss:0.6875 reg_loss:0.9999 cost_time:698ms lr=2.3952e-04 total_loss:2.5485\n",
            "global_steps:61 epoch:1 steps:61/2885 cls_loss:0.9869 cnt_loss:0.6412 reg_loss:1.0000 cost_time:994ms lr=2.4351e-04 total_loss:2.6281\n",
            "global_steps:62 epoch:1 steps:62/2885 cls_loss:1.0465 cnt_loss:0.6500 reg_loss:0.9985 cost_time:813ms lr=2.4750e-04 total_loss:2.6949\n",
            "global_steps:63 epoch:1 steps:63/2885 cls_loss:1.0181 cnt_loss:0.6559 reg_loss:1.0000 cost_time:886ms lr=2.5150e-04 total_loss:2.6740\n",
            "global_steps:64 epoch:1 steps:64/2885 cls_loss:0.8406 cnt_loss:0.6041 reg_loss:1.0000 cost_time:726ms lr=2.5549e-04 total_loss:2.4447\n",
            "global_steps:65 epoch:1 steps:65/2885 cls_loss:1.0168 cnt_loss:0.6361 reg_loss:0.9999 cost_time:926ms lr=2.5948e-04 total_loss:2.6529\n",
            "global_steps:66 epoch:1 steps:66/2885 cls_loss:0.8085 cnt_loss:0.6572 reg_loss:0.9999 cost_time:807ms lr=2.6347e-04 total_loss:2.4656\n",
            "global_steps:67 epoch:1 steps:67/2885 cls_loss:1.0322 cnt_loss:0.6463 reg_loss:0.9995 cost_time:941ms lr=2.6747e-04 total_loss:2.6780\n",
            "global_steps:68 epoch:1 steps:68/2885 cls_loss:0.9003 cnt_loss:0.6857 reg_loss:0.9999 cost_time:973ms lr=2.7146e-04 total_loss:2.5860\n",
            "global_steps:69 epoch:1 steps:69/2885 cls_loss:0.9775 cnt_loss:0.5999 reg_loss:1.0000 cost_time:873ms lr=2.7545e-04 total_loss:2.5775\n",
            "global_steps:70 epoch:1 steps:70/2885 cls_loss:0.9046 cnt_loss:0.6391 reg_loss:1.0000 cost_time:921ms lr=2.7944e-04 total_loss:2.5436\n",
            "global_steps:71 epoch:1 steps:71/2885 cls_loss:0.8977 cnt_loss:0.6571 reg_loss:1.0000 cost_time:849ms lr=2.8343e-04 total_loss:2.5548\n",
            "global_steps:72 epoch:1 steps:72/2885 cls_loss:0.8327 cnt_loss:0.6061 reg_loss:0.9999 cost_time:770ms lr=2.8743e-04 total_loss:2.4388\n",
            "global_steps:73 epoch:1 steps:73/2885 cls_loss:1.1128 cnt_loss:0.6543 reg_loss:1.0000 cost_time:962ms lr=2.9142e-04 total_loss:2.7671\n",
            "global_steps:74 epoch:1 steps:74/2885 cls_loss:0.8776 cnt_loss:0.6175 reg_loss:0.9998 cost_time:761ms lr=2.9541e-04 total_loss:2.4949\n",
            "global_steps:75 epoch:1 steps:75/2885 cls_loss:0.7326 cnt_loss:0.6500 reg_loss:0.9999 cost_time:816ms lr=2.9940e-04 total_loss:2.3825\n",
            "global_steps:76 epoch:1 steps:76/2885 cls_loss:0.9395 cnt_loss:0.6266 reg_loss:0.9999 cost_time:937ms lr=3.0339e-04 total_loss:2.5659\n",
            "global_steps:77 epoch:1 steps:77/2885 cls_loss:0.8001 cnt_loss:0.6822 reg_loss:0.9998 cost_time:1050ms lr=3.0739e-04 total_loss:2.4821\n",
            "global_steps:78 epoch:1 steps:78/2885 cls_loss:1.0558 cnt_loss:0.6346 reg_loss:0.9996 cost_time:810ms lr=3.1138e-04 total_loss:2.6900\n",
            "global_steps:79 epoch:1 steps:79/2885 cls_loss:0.7598 cnt_loss:0.6422 reg_loss:1.0000 cost_time:926ms lr=3.1537e-04 total_loss:2.4020\n",
            "global_steps:80 epoch:1 steps:80/2885 cls_loss:0.6086 cnt_loss:0.6469 reg_loss:1.0001 cost_time:766ms lr=3.1936e-04 total_loss:2.2556\n",
            "global_steps:81 epoch:1 steps:81/2885 cls_loss:0.9275 cnt_loss:0.6460 reg_loss:0.9999 cost_time:786ms lr=3.2335e-04 total_loss:2.5734\n",
            "global_steps:82 epoch:1 steps:82/2885 cls_loss:0.9183 cnt_loss:0.6430 reg_loss:1.0000 cost_time:923ms lr=3.2735e-04 total_loss:2.5613\n",
            "global_steps:83 epoch:1 steps:83/2885 cls_loss:0.8586 cnt_loss:0.6405 reg_loss:0.9999 cost_time:737ms lr=3.3134e-04 total_loss:2.4990\n",
            "global_steps:84 epoch:1 steps:84/2885 cls_loss:0.9595 cnt_loss:0.6649 reg_loss:0.9999 cost_time:888ms lr=3.3533e-04 total_loss:2.6244\n",
            "global_steps:85 epoch:1 steps:85/2885 cls_loss:0.9870 cnt_loss:0.6552 reg_loss:0.9995 cost_time:824ms lr=3.3932e-04 total_loss:2.6418\n",
            "global_steps:86 epoch:1 steps:86/2885 cls_loss:1.0285 cnt_loss:0.6667 reg_loss:0.9993 cost_time:946ms lr=3.4331e-04 total_loss:2.6945\n",
            "global_steps:87 epoch:1 steps:87/2885 cls_loss:0.6791 cnt_loss:0.6486 reg_loss:0.9998 cost_time:820ms lr=3.4731e-04 total_loss:2.3275\n",
            "global_steps:88 epoch:1 steps:88/2885 cls_loss:0.8565 cnt_loss:0.6308 reg_loss:0.9999 cost_time:831ms lr=3.5130e-04 total_loss:2.4872\n",
            "global_steps:89 epoch:1 steps:89/2885 cls_loss:0.8683 cnt_loss:0.6823 reg_loss:0.9994 cost_time:798ms lr=3.5529e-04 total_loss:2.5501\n",
            "global_steps:90 epoch:1 steps:90/2885 cls_loss:0.8970 cnt_loss:0.6645 reg_loss:0.9992 cost_time:1060ms lr=3.5928e-04 total_loss:2.5608\n",
            "global_steps:91 epoch:1 steps:91/2885 cls_loss:0.8474 cnt_loss:0.6324 reg_loss:0.9998 cost_time:932ms lr=3.6327e-04 total_loss:2.4796\n",
            "global_steps:92 epoch:1 steps:92/2885 cls_loss:0.6929 cnt_loss:0.6305 reg_loss:1.0000 cost_time:810ms lr=3.6727e-04 total_loss:2.3234\n",
            "global_steps:93 epoch:1 steps:93/2885 cls_loss:1.0721 cnt_loss:0.6113 reg_loss:0.9998 cost_time:970ms lr=3.7126e-04 total_loss:2.6833\n",
            "global_steps:94 epoch:1 steps:94/2885 cls_loss:0.7526 cnt_loss:0.6578 reg_loss:0.9999 cost_time:935ms lr=3.7525e-04 total_loss:2.4103\n",
            "global_steps:95 epoch:1 steps:95/2885 cls_loss:0.6165 cnt_loss:0.6424 reg_loss:1.0000 cost_time:882ms lr=3.7924e-04 total_loss:2.2589\n",
            "global_steps:96 epoch:1 steps:96/2885 cls_loss:0.7590 cnt_loss:0.6419 reg_loss:1.0000 cost_time:1040ms lr=3.8323e-04 total_loss:2.4009\n",
            "global_steps:97 epoch:1 steps:97/2885 cls_loss:0.8551 cnt_loss:0.6170 reg_loss:0.9999 cost_time:826ms lr=3.8723e-04 total_loss:2.4720\n",
            "global_steps:98 epoch:1 steps:98/2885 cls_loss:0.9719 cnt_loss:0.5101 reg_loss:0.7500 cost_time:990ms lr=3.9122e-04 total_loss:2.2319\n",
            "global_steps:99 epoch:1 steps:99/2885 cls_loss:0.7383 cnt_loss:0.6293 reg_loss:1.0000 cost_time:777ms lr=3.9521e-04 total_loss:2.3677\n",
            "global_steps:100 epoch:1 steps:100/2885 cls_loss:1.0345 cnt_loss:0.6543 reg_loss:0.9986 cost_time:931ms lr=3.9920e-04 total_loss:2.6874\n",
            "global_steps:101 epoch:1 steps:101/2885 cls_loss:0.7404 cnt_loss:0.6237 reg_loss:1.0000 cost_time:824ms lr=4.0319e-04 total_loss:2.3641\n",
            "global_steps:102 epoch:1 steps:102/2885 cls_loss:0.7240 cnt_loss:0.6398 reg_loss:1.0000 cost_time:860ms lr=4.0719e-04 total_loss:2.3638\n",
            "global_steps:103 epoch:1 steps:103/2885 cls_loss:0.7365 cnt_loss:0.4814 reg_loss:0.7500 cost_time:1047ms lr=4.1118e-04 total_loss:1.9679\n",
            "global_steps:104 epoch:1 steps:104/2885 cls_loss:0.8367 cnt_loss:0.6442 reg_loss:0.9999 cost_time:1069ms lr=4.1517e-04 total_loss:2.4807\n",
            "global_steps:105 epoch:1 steps:105/2885 cls_loss:0.7687 cnt_loss:0.5076 reg_loss:0.7498 cost_time:891ms lr=4.1916e-04 total_loss:2.0261\n",
            "global_steps:106 epoch:1 steps:106/2885 cls_loss:0.7278 cnt_loss:0.6472 reg_loss:0.9996 cost_time:932ms lr=4.2315e-04 total_loss:2.3747\n",
            "global_steps:107 epoch:1 steps:107/2885 cls_loss:0.7614 cnt_loss:0.6429 reg_loss:1.0000 cost_time:1054ms lr=4.2715e-04 total_loss:2.4042\n",
            "global_steps:108 epoch:1 steps:108/2885 cls_loss:0.7347 cnt_loss:0.6316 reg_loss:1.0000 cost_time:1015ms lr=4.3114e-04 total_loss:2.3663\n",
            "global_steps:109 epoch:1 steps:109/2885 cls_loss:0.6337 cnt_loss:0.6245 reg_loss:0.9999 cost_time:822ms lr=4.3513e-04 total_loss:2.2581\n",
            "global_steps:110 epoch:1 steps:110/2885 cls_loss:0.7752 cnt_loss:0.4761 reg_loss:0.7497 cost_time:765ms lr=4.3912e-04 total_loss:2.0011\n",
            "global_steps:111 epoch:1 steps:111/2885 cls_loss:0.8762 cnt_loss:0.6225 reg_loss:0.9997 cost_time:853ms lr=4.4311e-04 total_loss:2.4984\n",
            "global_steps:112 epoch:1 steps:112/2885 cls_loss:0.8190 cnt_loss:0.6531 reg_loss:0.9999 cost_time:968ms lr=4.4711e-04 total_loss:2.4720\n",
            "global_steps:113 epoch:1 steps:113/2885 cls_loss:0.7230 cnt_loss:0.6288 reg_loss:1.0000 cost_time:995ms lr=4.5110e-04 total_loss:2.3518\n",
            "global_steps:114 epoch:1 steps:114/2885 cls_loss:0.9605 cnt_loss:0.4942 reg_loss:0.7498 cost_time:1071ms lr=4.5509e-04 total_loss:2.2046\n",
            "global_steps:115 epoch:1 steps:115/2885 cls_loss:0.6380 cnt_loss:0.6588 reg_loss:0.9999 cost_time:1053ms lr=4.5908e-04 total_loss:2.2968\n",
            "global_steps:116 epoch:1 steps:116/2885 cls_loss:0.8313 cnt_loss:0.6114 reg_loss:0.9999 cost_time:760ms lr=4.6307e-04 total_loss:2.4426\n",
            "global_steps:117 epoch:1 steps:117/2885 cls_loss:0.7106 cnt_loss:0.6579 reg_loss:0.9998 cost_time:842ms lr=4.6707e-04 total_loss:2.3683\n",
            "global_steps:118 epoch:1 steps:118/2885 cls_loss:0.7929 cnt_loss:0.6554 reg_loss:0.9999 cost_time:960ms lr=4.7106e-04 total_loss:2.4482\n",
            "global_steps:119 epoch:1 steps:119/2885 cls_loss:0.7925 cnt_loss:0.6372 reg_loss:0.9999 cost_time:910ms lr=4.7505e-04 total_loss:2.4296\n",
            "global_steps:120 epoch:1 steps:120/2885 cls_loss:0.9534 cnt_loss:0.6324 reg_loss:0.9996 cost_time:845ms lr=4.7904e-04 total_loss:2.5854\n",
            "global_steps:121 epoch:1 steps:121/2885 cls_loss:0.7961 cnt_loss:0.6041 reg_loss:1.0000 cost_time:1056ms lr=4.8303e-04 total_loss:2.4002\n",
            "global_steps:122 epoch:1 steps:122/2885 cls_loss:0.7277 cnt_loss:0.6512 reg_loss:0.9999 cost_time:966ms lr=4.8703e-04 total_loss:2.3788\n",
            "global_steps:123 epoch:1 steps:123/2885 cls_loss:0.8583 cnt_loss:0.6342 reg_loss:0.9999 cost_time:830ms lr=4.9102e-04 total_loss:2.4924\n",
            "global_steps:124 epoch:1 steps:124/2885 cls_loss:0.6662 cnt_loss:0.6358 reg_loss:1.0000 cost_time:966ms lr=4.9501e-04 total_loss:2.3020\n",
            "global_steps:125 epoch:1 steps:125/2885 cls_loss:0.8693 cnt_loss:0.6327 reg_loss:0.9999 cost_time:993ms lr=4.9900e-04 total_loss:2.5019\n",
            "global_steps:126 epoch:1 steps:126/2885 cls_loss:0.7785 cnt_loss:0.6315 reg_loss:1.0000 cost_time:773ms lr=5.0299e-04 total_loss:2.4100\n",
            "global_steps:127 epoch:1 steps:127/2885 cls_loss:0.8126 cnt_loss:0.6512 reg_loss:1.0000 cost_time:833ms lr=5.0699e-04 total_loss:2.4639\n",
            "global_steps:128 epoch:1 steps:128/2885 cls_loss:0.6852 cnt_loss:0.6154 reg_loss:1.0000 cost_time:845ms lr=5.1098e-04 total_loss:2.3005\n",
            "global_steps:129 epoch:1 steps:129/2885 cls_loss:0.5737 cnt_loss:0.6325 reg_loss:0.9998 cost_time:980ms lr=5.1497e-04 total_loss:2.2060\n",
            "global_steps:130 epoch:1 steps:130/2885 cls_loss:0.6091 cnt_loss:0.6530 reg_loss:1.0000 cost_time:748ms lr=5.1896e-04 total_loss:2.2621\n",
            "global_steps:131 epoch:1 steps:131/2885 cls_loss:0.7784 cnt_loss:0.6639 reg_loss:0.9999 cost_time:1018ms lr=5.2295e-04 total_loss:2.4423\n",
            "global_steps:132 epoch:1 steps:132/2885 cls_loss:0.5881 cnt_loss:0.6119 reg_loss:1.0000 cost_time:865ms lr=5.2695e-04 total_loss:2.1999\n",
            "global_steps:133 epoch:1 steps:133/2885 cls_loss:0.5877 cnt_loss:0.6701 reg_loss:1.0001 cost_time:957ms lr=5.3094e-04 total_loss:2.2578\n",
            "global_steps:134 epoch:1 steps:134/2885 cls_loss:0.6216 cnt_loss:0.6556 reg_loss:0.9999 cost_time:831ms lr=5.3493e-04 total_loss:2.2771\n",
            "global_steps:135 epoch:1 steps:135/2885 cls_loss:0.6418 cnt_loss:0.6273 reg_loss:0.9998 cost_time:840ms lr=5.3892e-04 total_loss:2.2689\n",
            "global_steps:136 epoch:1 steps:136/2885 cls_loss:0.6492 cnt_loss:0.6494 reg_loss:0.9997 cost_time:930ms lr=5.4291e-04 total_loss:2.2984\n",
            "global_steps:137 epoch:1 steps:137/2885 cls_loss:0.6934 cnt_loss:0.6404 reg_loss:0.9999 cost_time:748ms lr=5.4691e-04 total_loss:2.3337\n",
            "global_steps:138 epoch:1 steps:138/2885 cls_loss:0.5895 cnt_loss:0.6398 reg_loss:0.9999 cost_time:953ms lr=5.5090e-04 total_loss:2.2293\n",
            "global_steps:139 epoch:1 steps:139/2885 cls_loss:0.6900 cnt_loss:0.6465 reg_loss:0.9999 cost_time:1121ms lr=5.5489e-04 total_loss:2.3364\n",
            "global_steps:140 epoch:1 steps:140/2885 cls_loss:0.6749 cnt_loss:0.6585 reg_loss:0.9998 cost_time:977ms lr=5.5888e-04 total_loss:2.3331\n",
            "global_steps:141 epoch:1 steps:141/2885 cls_loss:0.9967 cnt_loss:0.6768 reg_loss:0.9998 cost_time:928ms lr=5.6287e-04 total_loss:2.6734\n",
            "global_steps:142 epoch:1 steps:142/2885 cls_loss:0.6819 cnt_loss:0.6400 reg_loss:1.0000 cost_time:877ms lr=5.6687e-04 total_loss:2.3219\n",
            "global_steps:143 epoch:1 steps:143/2885 cls_loss:0.7362 cnt_loss:0.6420 reg_loss:0.9999 cost_time:832ms lr=5.7086e-04 total_loss:2.3781\n",
            "global_steps:144 epoch:1 steps:144/2885 cls_loss:0.7170 cnt_loss:0.6181 reg_loss:0.9999 cost_time:834ms lr=5.7485e-04 total_loss:2.3350\n",
            "global_steps:145 epoch:1 steps:145/2885 cls_loss:0.7496 cnt_loss:0.6366 reg_loss:1.0000 cost_time:961ms lr=5.7884e-04 total_loss:2.3862\n",
            "global_steps:146 epoch:1 steps:146/2885 cls_loss:0.8256 cnt_loss:0.6362 reg_loss:0.9999 cost_time:1067ms lr=5.8283e-04 total_loss:2.4616\n",
            "global_steps:147 epoch:1 steps:147/2885 cls_loss:0.4758 cnt_loss:0.6486 reg_loss:1.0000 cost_time:1016ms lr=5.8683e-04 total_loss:2.1244\n",
            "global_steps:148 epoch:1 steps:148/2885 cls_loss:0.8079 cnt_loss:0.6375 reg_loss:0.9999 cost_time:820ms lr=5.9082e-04 total_loss:2.4453\n",
            "global_steps:149 epoch:1 steps:149/2885 cls_loss:0.8234 cnt_loss:0.6360 reg_loss:1.0000 cost_time:836ms lr=5.9481e-04 total_loss:2.4593\n",
            "global_steps:150 epoch:1 steps:150/2885 cls_loss:0.6095 cnt_loss:0.6231 reg_loss:1.0000 cost_time:1035ms lr=5.9880e-04 total_loss:2.2326\n",
            "global_steps:151 epoch:1 steps:151/2885 cls_loss:0.7043 cnt_loss:0.6545 reg_loss:0.9998 cost_time:907ms lr=6.0279e-04 total_loss:2.3586\n",
            "global_steps:152 epoch:1 steps:152/2885 cls_loss:0.7663 cnt_loss:0.6575 reg_loss:0.9997 cost_time:857ms lr=6.0679e-04 total_loss:2.4235\n",
            "global_steps:153 epoch:1 steps:153/2885 cls_loss:0.7599 cnt_loss:0.6557 reg_loss:0.9998 cost_time:847ms lr=6.1078e-04 total_loss:2.4154\n",
            "global_steps:154 epoch:1 steps:154/2885 cls_loss:0.8717 cnt_loss:0.6552 reg_loss:0.9997 cost_time:790ms lr=6.1477e-04 total_loss:2.5266\n",
            "global_steps:155 epoch:1 steps:155/2885 cls_loss:0.5871 cnt_loss:0.6389 reg_loss:1.0000 cost_time:962ms lr=6.1876e-04 total_loss:2.2260\n",
            "global_steps:156 epoch:1 steps:156/2885 cls_loss:1.0195 cnt_loss:0.4965 reg_loss:0.7499 cost_time:1069ms lr=6.2275e-04 total_loss:2.2659\n",
            "global_steps:157 epoch:1 steps:157/2885 cls_loss:0.7035 cnt_loss:0.6374 reg_loss:1.0007 cost_time:895ms lr=6.2675e-04 total_loss:2.3417\n",
            "global_steps:158 epoch:1 steps:158/2885 cls_loss:0.7474 cnt_loss:0.6641 reg_loss:0.9998 cost_time:955ms lr=6.3074e-04 total_loss:2.4113\n",
            "global_steps:159 epoch:1 steps:159/2885 cls_loss:0.7298 cnt_loss:0.6359 reg_loss:0.9997 cost_time:1043ms lr=6.3473e-04 total_loss:2.3654\n",
            "global_steps:160 epoch:1 steps:160/2885 cls_loss:0.7426 cnt_loss:0.6655 reg_loss:0.9999 cost_time:936ms lr=6.3872e-04 total_loss:2.4080\n",
            "global_steps:161 epoch:1 steps:161/2885 cls_loss:0.9278 cnt_loss:0.6456 reg_loss:0.9998 cost_time:1340ms lr=6.4271e-04 total_loss:2.5732\n",
            "global_steps:162 epoch:1 steps:162/2885 cls_loss:0.8595 cnt_loss:0.6444 reg_loss:0.9998 cost_time:769ms lr=6.4671e-04 total_loss:2.5036\n",
            "global_steps:163 epoch:1 steps:163/2885 cls_loss:0.8618 cnt_loss:0.6128 reg_loss:1.0000 cost_time:1197ms lr=6.5070e-04 total_loss:2.4746\n",
            "global_steps:164 epoch:1 steps:164/2885 cls_loss:0.6900 cnt_loss:0.6608 reg_loss:0.9997 cost_time:856ms lr=6.5469e-04 total_loss:2.3505\n",
            "global_steps:165 epoch:1 steps:165/2885 cls_loss:0.8590 cnt_loss:0.6432 reg_loss:0.9997 cost_time:1220ms lr=6.5868e-04 total_loss:2.5019\n",
            "global_steps:166 epoch:1 steps:166/2885 cls_loss:0.5852 cnt_loss:0.6514 reg_loss:1.0000 cost_time:752ms lr=6.6267e-04 total_loss:2.2365\n",
            "global_steps:167 epoch:1 steps:167/2885 cls_loss:0.8506 cnt_loss:0.6706 reg_loss:0.9990 cost_time:956ms lr=6.6667e-04 total_loss:2.5202\n",
            "global_steps:168 epoch:1 steps:168/2885 cls_loss:0.5703 cnt_loss:0.6094 reg_loss:0.9999 cost_time:931ms lr=6.7066e-04 total_loss:2.1796\n",
            "global_steps:169 epoch:1 steps:169/2885 cls_loss:0.7901 cnt_loss:0.6045 reg_loss:0.9992 cost_time:848ms lr=6.7465e-04 total_loss:2.3938\n",
            "global_steps:170 epoch:1 steps:170/2885 cls_loss:0.6239 cnt_loss:0.6277 reg_loss:0.9999 cost_time:759ms lr=6.7864e-04 total_loss:2.2516\n",
            "global_steps:171 epoch:1 steps:171/2885 cls_loss:0.6217 cnt_loss:0.6369 reg_loss:0.9999 cost_time:1064ms lr=6.8263e-04 total_loss:2.2585\n",
            "global_steps:172 epoch:1 steps:172/2885 cls_loss:0.9076 cnt_loss:0.6335 reg_loss:1.0000 cost_time:1038ms lr=6.8663e-04 total_loss:2.5411\n",
            "global_steps:173 epoch:1 steps:173/2885 cls_loss:0.9833 cnt_loss:0.6590 reg_loss:0.9998 cost_time:773ms lr=6.9062e-04 total_loss:2.6421\n",
            "global_steps:174 epoch:1 steps:174/2885 cls_loss:0.7071 cnt_loss:0.6523 reg_loss:0.9997 cost_time:783ms lr=6.9461e-04 total_loss:2.3591\n",
            "global_steps:175 epoch:1 steps:175/2885 cls_loss:0.7324 cnt_loss:0.6474 reg_loss:1.0000 cost_time:916ms lr=6.9860e-04 total_loss:2.3798\n",
            "global_steps:176 epoch:1 steps:176/2885 cls_loss:0.8227 cnt_loss:0.6712 reg_loss:0.9999 cost_time:913ms lr=7.0259e-04 total_loss:2.4939\n",
            "global_steps:177 epoch:1 steps:177/2885 cls_loss:0.7351 cnt_loss:0.6419 reg_loss:0.9998 cost_time:1079ms lr=7.0659e-04 total_loss:2.3769\n",
            "global_steps:178 epoch:1 steps:178/2885 cls_loss:0.6332 cnt_loss:0.6371 reg_loss:0.9999 cost_time:1052ms lr=7.1058e-04 total_loss:2.2703\n",
            "global_steps:179 epoch:1 steps:179/2885 cls_loss:0.7784 cnt_loss:0.6364 reg_loss:0.9999 cost_time:974ms lr=7.1457e-04 total_loss:2.4147\n",
            "global_steps:180 epoch:1 steps:180/2885 cls_loss:0.6477 cnt_loss:0.6753 reg_loss:0.9997 cost_time:972ms lr=7.1856e-04 total_loss:2.3227\n",
            "global_steps:181 epoch:1 steps:181/2885 cls_loss:0.6693 cnt_loss:0.6724 reg_loss:0.9999 cost_time:994ms lr=7.2255e-04 total_loss:2.3416\n",
            "global_steps:182 epoch:1 steps:182/2885 cls_loss:0.7213 cnt_loss:0.4983 reg_loss:0.7499 cost_time:850ms lr=7.2655e-04 total_loss:1.9696\n",
            "global_steps:183 epoch:1 steps:183/2885 cls_loss:0.6010 cnt_loss:0.6186 reg_loss:0.9999 cost_time:846ms lr=7.3054e-04 total_loss:2.2195\n",
            "global_steps:184 epoch:1 steps:184/2885 cls_loss:0.6795 cnt_loss:0.6421 reg_loss:0.9998 cost_time:1036ms lr=7.3453e-04 total_loss:2.3214\n",
            "global_steps:185 epoch:1 steps:185/2885 cls_loss:0.6224 cnt_loss:0.6289 reg_loss:0.9999 cost_time:841ms lr=7.3852e-04 total_loss:2.2513\n",
            "global_steps:186 epoch:1 steps:186/2885 cls_loss:0.8518 cnt_loss:0.6208 reg_loss:0.9996 cost_time:952ms lr=7.4251e-04 total_loss:2.4723\n",
            "global_steps:187 epoch:1 steps:187/2885 cls_loss:0.6978 cnt_loss:0.6478 reg_loss:0.9999 cost_time:1181ms lr=7.4651e-04 total_loss:2.3455\n",
            "global_steps:188 epoch:1 steps:188/2885 cls_loss:0.7197 cnt_loss:0.6048 reg_loss:0.9999 cost_time:844ms lr=7.5050e-04 total_loss:2.3244\n",
            "global_steps:189 epoch:1 steps:189/2885 cls_loss:0.7251 cnt_loss:0.6490 reg_loss:0.9989 cost_time:783ms lr=7.5449e-04 total_loss:2.3730\n",
            "global_steps:190 epoch:1 steps:190/2885 cls_loss:0.7540 cnt_loss:0.4628 reg_loss:0.7499 cost_time:795ms lr=7.5848e-04 total_loss:1.9668\n",
            "global_steps:191 epoch:1 steps:191/2885 cls_loss:0.7157 cnt_loss:0.6226 reg_loss:0.9998 cost_time:843ms lr=7.6248e-04 total_loss:2.3381\n",
            "global_steps:192 epoch:1 steps:192/2885 cls_loss:0.6905 cnt_loss:0.6561 reg_loss:0.9998 cost_time:862ms lr=7.6647e-04 total_loss:2.3465\n",
            "global_steps:193 epoch:1 steps:193/2885 cls_loss:0.6823 cnt_loss:0.6160 reg_loss:1.0000 cost_time:762ms lr=7.7046e-04 total_loss:2.2983\n",
            "global_steps:194 epoch:1 steps:194/2885 cls_loss:0.6815 cnt_loss:0.6388 reg_loss:0.9998 cost_time:1132ms lr=7.7445e-04 total_loss:2.3200\n",
            "global_steps:195 epoch:1 steps:195/2885 cls_loss:0.7596 cnt_loss:0.6442 reg_loss:1.0005 cost_time:866ms lr=7.7844e-04 total_loss:2.4042\n",
            "global_steps:196 epoch:1 steps:196/2885 cls_loss:0.7388 cnt_loss:0.6374 reg_loss:0.9997 cost_time:1198ms lr=7.8244e-04 total_loss:2.3759\n",
            "global_steps:197 epoch:1 steps:197/2885 cls_loss:0.6226 cnt_loss:0.6034 reg_loss:1.0000 cost_time:962ms lr=7.8643e-04 total_loss:2.2259\n",
            "global_steps:198 epoch:1 steps:198/2885 cls_loss:0.6086 cnt_loss:0.6384 reg_loss:0.9995 cost_time:912ms lr=7.9042e-04 total_loss:2.2464\n",
            "global_steps:199 epoch:1 steps:199/2885 cls_loss:0.6955 cnt_loss:0.6503 reg_loss:0.9999 cost_time:871ms lr=7.9441e-04 total_loss:2.3457\n",
            "global_steps:200 epoch:1 steps:200/2885 cls_loss:0.6880 cnt_loss:0.6544 reg_loss:1.0000 cost_time:1209ms lr=7.9840e-04 total_loss:2.3424\n",
            "global_steps:201 epoch:1 steps:201/2885 cls_loss:0.4818 cnt_loss:0.6307 reg_loss:1.0000 cost_time:846ms lr=8.0240e-04 total_loss:2.1124\n",
            "global_steps:202 epoch:1 steps:202/2885 cls_loss:0.7356 cnt_loss:0.6183 reg_loss:0.9998 cost_time:1221ms lr=8.0639e-04 total_loss:2.3537\n",
            "global_steps:203 epoch:1 steps:203/2885 cls_loss:0.7356 cnt_loss:0.6449 reg_loss:0.9998 cost_time:862ms lr=8.1038e-04 total_loss:2.3803\n",
            "global_steps:204 epoch:1 steps:204/2885 cls_loss:0.7311 cnt_loss:0.6247 reg_loss:0.9998 cost_time:854ms lr=8.1437e-04 total_loss:2.3556\n",
            "global_steps:205 epoch:1 steps:205/2885 cls_loss:0.6204 cnt_loss:0.6255 reg_loss:1.0000 cost_time:843ms lr=8.1836e-04 total_loss:2.2459\n",
            "global_steps:206 epoch:1 steps:206/2885 cls_loss:0.6418 cnt_loss:0.5859 reg_loss:0.9999 cost_time:874ms lr=8.2236e-04 total_loss:2.2275\n",
            "global_steps:207 epoch:1 steps:207/2885 cls_loss:0.7706 cnt_loss:0.6603 reg_loss:0.9996 cost_time:814ms lr=8.2635e-04 total_loss:2.4305\n",
            "global_steps:208 epoch:1 steps:208/2885 cls_loss:0.6714 cnt_loss:0.6369 reg_loss:0.9999 cost_time:940ms lr=8.3034e-04 total_loss:2.3082\n",
            "global_steps:209 epoch:1 steps:209/2885 cls_loss:0.4219 cnt_loss:0.6408 reg_loss:0.9999 cost_time:972ms lr=8.3433e-04 total_loss:2.0627\n",
            "global_steps:210 epoch:1 steps:210/2885 cls_loss:0.7660 cnt_loss:0.6596 reg_loss:0.9999 cost_time:1207ms lr=8.3832e-04 total_loss:2.4255\n",
            "global_steps:211 epoch:1 steps:211/2885 cls_loss:0.6829 cnt_loss:0.6429 reg_loss:0.9999 cost_time:834ms lr=8.4232e-04 total_loss:2.3257\n",
            "global_steps:212 epoch:1 steps:212/2885 cls_loss:0.7914 cnt_loss:0.6508 reg_loss:0.9998 cost_time:1210ms lr=8.4631e-04 total_loss:2.4420\n",
            "global_steps:213 epoch:1 steps:213/2885 cls_loss:0.7063 cnt_loss:0.6718 reg_loss:0.9984 cost_time:979ms lr=8.5030e-04 total_loss:2.3765\n",
            "global_steps:214 epoch:1 steps:214/2885 cls_loss:0.6771 cnt_loss:0.6654 reg_loss:1.0000 cost_time:880ms lr=8.5429e-04 total_loss:2.3425\n",
            "global_steps:215 epoch:1 steps:215/2885 cls_loss:0.7207 cnt_loss:0.6052 reg_loss:1.0000 cost_time:939ms lr=8.5828e-04 total_loss:2.3259\n",
            "global_steps:216 epoch:1 steps:216/2885 cls_loss:0.8240 cnt_loss:0.6525 reg_loss:0.9999 cost_time:921ms lr=8.6228e-04 total_loss:2.4763\n",
            "global_steps:217 epoch:1 steps:217/2885 cls_loss:0.7466 cnt_loss:0.6718 reg_loss:1.0000 cost_time:1385ms lr=8.6627e-04 total_loss:2.4184\n",
            "global_steps:218 epoch:1 steps:218/2885 cls_loss:0.5061 cnt_loss:0.6838 reg_loss:0.9999 cost_time:1208ms lr=8.7026e-04 total_loss:2.1899\n",
            "global_steps:219 epoch:1 steps:219/2885 cls_loss:0.5242 cnt_loss:0.6205 reg_loss:1.0000 cost_time:1109ms lr=8.7425e-04 total_loss:2.1447\n",
            "global_steps:220 epoch:1 steps:220/2885 cls_loss:0.5591 cnt_loss:0.6338 reg_loss:1.0000 cost_time:979ms lr=8.7824e-04 total_loss:2.1929\n",
            "global_steps:221 epoch:1 steps:221/2885 cls_loss:0.5746 cnt_loss:0.6467 reg_loss:0.9999 cost_time:851ms lr=8.8224e-04 total_loss:2.2213\n",
            "global_steps:222 epoch:1 steps:222/2885 cls_loss:0.6568 cnt_loss:0.6419 reg_loss:0.9998 cost_time:1130ms lr=8.8623e-04 total_loss:2.2985\n",
            "global_steps:223 epoch:1 steps:223/2885 cls_loss:0.6489 cnt_loss:0.6742 reg_loss:1.0003 cost_time:850ms lr=8.9022e-04 total_loss:2.3233\n",
            "global_steps:224 epoch:1 steps:224/2885 cls_loss:0.7712 cnt_loss:0.6185 reg_loss:0.9997 cost_time:1028ms lr=8.9421e-04 total_loss:2.3895\n",
            "global_steps:225 epoch:1 steps:225/2885 cls_loss:0.6696 cnt_loss:0.4661 reg_loss:0.7499 cost_time:932ms lr=8.9820e-04 total_loss:1.8856\n",
            "global_steps:226 epoch:1 steps:226/2885 cls_loss:0.8656 cnt_loss:0.6027 reg_loss:0.9998 cost_time:812ms lr=9.0220e-04 total_loss:2.4681\n",
            "global_steps:227 epoch:1 steps:227/2885 cls_loss:0.8549 cnt_loss:0.6508 reg_loss:0.9993 cost_time:966ms lr=9.0619e-04 total_loss:2.5050\n",
            "global_steps:228 epoch:1 steps:228/2885 cls_loss:0.6635 cnt_loss:0.6466 reg_loss:0.9997 cost_time:785ms lr=9.1018e-04 total_loss:2.3098\n",
            "global_steps:229 epoch:1 steps:229/2885 cls_loss:0.6984 cnt_loss:0.6136 reg_loss:0.9999 cost_time:855ms lr=9.1417e-04 total_loss:2.3118\n",
            "global_steps:230 epoch:1 steps:230/2885 cls_loss:0.4768 cnt_loss:0.6158 reg_loss:1.0000 cost_time:790ms lr=9.1816e-04 total_loss:2.0925\n",
            "global_steps:231 epoch:1 steps:231/2885 cls_loss:0.7091 cnt_loss:0.4591 reg_loss:0.7500 cost_time:889ms lr=9.2216e-04 total_loss:1.9181\n",
            "global_steps:232 epoch:1 steps:232/2885 cls_loss:0.5768 cnt_loss:0.6624 reg_loss:0.9999 cost_time:1089ms lr=9.2615e-04 total_loss:2.2391\n",
            "global_steps:233 epoch:1 steps:233/2885 cls_loss:0.7823 cnt_loss:0.6363 reg_loss:0.9995 cost_time:1241ms lr=9.3014e-04 total_loss:2.4181\n",
            "global_steps:234 epoch:1 steps:234/2885 cls_loss:0.4941 cnt_loss:0.6366 reg_loss:1.0000 cost_time:1323ms lr=9.3413e-04 total_loss:2.1307\n",
            "global_steps:235 epoch:1 steps:235/2885 cls_loss:0.8746 cnt_loss:0.6455 reg_loss:0.9996 cost_time:1120ms lr=9.3812e-04 total_loss:2.5198\n",
            "global_steps:236 epoch:1 steps:236/2885 cls_loss:0.7459 cnt_loss:0.6741 reg_loss:0.9997 cost_time:851ms lr=9.4212e-04 total_loss:2.4197\n",
            "global_steps:237 epoch:1 steps:237/2885 cls_loss:0.7433 cnt_loss:0.6632 reg_loss:0.9999 cost_time:932ms lr=9.4611e-04 total_loss:2.4065\n",
            "global_steps:238 epoch:1 steps:238/2885 cls_loss:0.5120 cnt_loss:0.6324 reg_loss:1.0005 cost_time:887ms lr=9.5010e-04 total_loss:2.1449\n",
            "global_steps:239 epoch:1 steps:239/2885 cls_loss:0.4694 cnt_loss:0.6007 reg_loss:0.9999 cost_time:768ms lr=9.5409e-04 total_loss:2.0700\n",
            "global_steps:240 epoch:1 steps:240/2885 cls_loss:0.5909 cnt_loss:0.6470 reg_loss:0.9997 cost_time:967ms lr=9.5808e-04 total_loss:2.2376\n",
            "global_steps:241 epoch:1 steps:241/2885 cls_loss:0.5871 cnt_loss:0.6069 reg_loss:0.9999 cost_time:1045ms lr=9.6208e-04 total_loss:2.1940\n",
            "global_steps:242 epoch:1 steps:242/2885 cls_loss:0.8851 cnt_loss:0.6546 reg_loss:0.9991 cost_time:959ms lr=9.6607e-04 total_loss:2.5389\n",
            "global_steps:243 epoch:1 steps:243/2885 cls_loss:0.6550 cnt_loss:0.6398 reg_loss:0.9999 cost_time:876ms lr=9.7006e-04 total_loss:2.2947\n",
            "global_steps:244 epoch:1 steps:244/2885 cls_loss:0.7976 cnt_loss:0.6329 reg_loss:0.9998 cost_time:871ms lr=9.7405e-04 total_loss:2.4302\n",
            "global_steps:245 epoch:1 steps:245/2885 cls_loss:0.9395 cnt_loss:0.6415 reg_loss:1.0001 cost_time:769ms lr=9.7804e-04 total_loss:2.5812\n",
            "global_steps:246 epoch:1 steps:246/2885 cls_loss:0.7605 cnt_loss:0.6342 reg_loss:0.9999 cost_time:1054ms lr=9.8204e-04 total_loss:2.3946\n",
            "global_steps:247 epoch:1 steps:247/2885 cls_loss:0.6333 cnt_loss:0.6287 reg_loss:1.0000 cost_time:982ms lr=9.8603e-04 total_loss:2.2620\n",
            "global_steps:248 epoch:1 steps:248/2885 cls_loss:0.5333 cnt_loss:0.6473 reg_loss:0.9997 cost_time:1014ms lr=9.9002e-04 total_loss:2.1803\n",
            "global_steps:249 epoch:1 steps:249/2885 cls_loss:0.5382 cnt_loss:0.6636 reg_loss:1.0000 cost_time:914ms lr=9.9401e-04 total_loss:2.2017\n",
            "global_steps:250 epoch:1 steps:250/2885 cls_loss:0.5881 cnt_loss:0.6407 reg_loss:0.9999 cost_time:861ms lr=9.9800e-04 total_loss:2.2287\n",
            "global_steps:251 epoch:1 steps:251/2885 cls_loss:0.8370 cnt_loss:0.6211 reg_loss:0.9987 cost_time:863ms lr=1.0020e-03 total_loss:2.4568\n",
            "global_steps:252 epoch:1 steps:252/2885 cls_loss:0.7321 cnt_loss:0.6575 reg_loss:0.9975 cost_time:962ms lr=1.0060e-03 total_loss:2.3871\n",
            "global_steps:253 epoch:1 steps:253/2885 cls_loss:0.6264 cnt_loss:0.6512 reg_loss:0.9999 cost_time:766ms lr=1.0100e-03 total_loss:2.2774\n",
            "global_steps:254 epoch:1 steps:254/2885 cls_loss:0.6865 cnt_loss:0.6479 reg_loss:0.9999 cost_time:977ms lr=1.0140e-03 total_loss:2.3342\n",
            "global_steps:255 epoch:1 steps:255/2885 cls_loss:0.5661 cnt_loss:0.6238 reg_loss:0.9999 cost_time:854ms lr=1.0180e-03 total_loss:2.1898\n",
            "global_steps:256 epoch:1 steps:256/2885 cls_loss:0.5610 cnt_loss:0.6322 reg_loss:0.9999 cost_time:949ms lr=1.0220e-03 total_loss:2.1931\n",
            "global_steps:257 epoch:1 steps:257/2885 cls_loss:0.5454 cnt_loss:0.6209 reg_loss:0.9999 cost_time:887ms lr=1.0259e-03 total_loss:2.1663\n",
            "global_steps:258 epoch:1 steps:258/2885 cls_loss:0.3635 cnt_loss:0.6061 reg_loss:1.0000 cost_time:856ms lr=1.0299e-03 total_loss:1.9696\n",
            "global_steps:259 epoch:1 steps:259/2885 cls_loss:0.7559 cnt_loss:0.6751 reg_loss:0.9993 cost_time:957ms lr=1.0339e-03 total_loss:2.4304\n",
            "global_steps:260 epoch:1 steps:260/2885 cls_loss:0.5642 cnt_loss:0.6410 reg_loss:0.9997 cost_time:914ms lr=1.0379e-03 total_loss:2.2049\n",
            "global_steps:261 epoch:1 steps:261/2885 cls_loss:0.6712 cnt_loss:0.6358 reg_loss:0.9996 cost_time:991ms lr=1.0419e-03 total_loss:2.3066\n",
            "global_steps:262 epoch:1 steps:262/2885 cls_loss:0.7078 cnt_loss:0.6069 reg_loss:0.9999 cost_time:862ms lr=1.0459e-03 total_loss:2.3146\n",
            "global_steps:263 epoch:1 steps:263/2885 cls_loss:0.5266 cnt_loss:0.6549 reg_loss:0.9997 cost_time:955ms lr=1.0499e-03 total_loss:2.1812\n",
            "global_steps:264 epoch:1 steps:264/2885 cls_loss:0.5488 cnt_loss:0.6430 reg_loss:0.9999 cost_time:907ms lr=1.0539e-03 total_loss:2.1917\n",
            "global_steps:265 epoch:1 steps:265/2885 cls_loss:0.6240 cnt_loss:0.6111 reg_loss:0.9999 cost_time:1146ms lr=1.0579e-03 total_loss:2.2351\n",
            "global_steps:266 epoch:1 steps:266/2885 cls_loss:0.6229 cnt_loss:0.4785 reg_loss:0.7500 cost_time:966ms lr=1.0619e-03 total_loss:1.8513\n",
            "global_steps:267 epoch:1 steps:267/2885 cls_loss:0.5459 cnt_loss:0.6611 reg_loss:0.9999 cost_time:1003ms lr=1.0659e-03 total_loss:2.2069\n",
            "global_steps:268 epoch:1 steps:268/2885 cls_loss:0.7120 cnt_loss:0.6720 reg_loss:0.9996 cost_time:770ms lr=1.0699e-03 total_loss:2.3835\n",
            "global_steps:269 epoch:1 steps:269/2885 cls_loss:0.6872 cnt_loss:0.6180 reg_loss:0.9999 cost_time:1024ms lr=1.0739e-03 total_loss:2.3051\n",
            "global_steps:270 epoch:1 steps:270/2885 cls_loss:0.4779 cnt_loss:0.6295 reg_loss:0.9987 cost_time:953ms lr=1.0778e-03 total_loss:2.1061\n",
            "global_steps:271 epoch:1 steps:271/2885 cls_loss:1.0179 cnt_loss:0.6748 reg_loss:0.9995 cost_time:961ms lr=1.0818e-03 total_loss:2.6922\n",
            "global_steps:272 epoch:1 steps:272/2885 cls_loss:0.7486 cnt_loss:0.6678 reg_loss:0.9998 cost_time:1265ms lr=1.0858e-03 total_loss:2.4162\n",
            "global_steps:273 epoch:1 steps:273/2885 cls_loss:0.3985 cnt_loss:0.6071 reg_loss:1.0000 cost_time:859ms lr=1.0898e-03 total_loss:2.0056\n",
            "global_steps:274 epoch:1 steps:274/2885 cls_loss:0.7507 cnt_loss:0.6786 reg_loss:0.9992 cost_time:1277ms lr=1.0938e-03 total_loss:2.4285\n",
            "global_steps:275 epoch:1 steps:275/2885 cls_loss:0.7945 cnt_loss:0.6570 reg_loss:0.9999 cost_time:976ms lr=1.0978e-03 total_loss:2.4514\n",
            "global_steps:276 epoch:1 steps:276/2885 cls_loss:0.6412 cnt_loss:0.6143 reg_loss:0.9998 cost_time:1134ms lr=1.1018e-03 total_loss:2.2554\n",
            "global_steps:277 epoch:1 steps:277/2885 cls_loss:0.8733 cnt_loss:0.6207 reg_loss:0.9993 cost_time:859ms lr=1.1058e-03 total_loss:2.4932\n",
            "global_steps:278 epoch:1 steps:278/2885 cls_loss:0.7186 cnt_loss:0.6541 reg_loss:0.9999 cost_time:868ms lr=1.1098e-03 total_loss:2.3726\n",
            "global_steps:279 epoch:1 steps:279/2885 cls_loss:0.6480 cnt_loss:0.6329 reg_loss:0.9994 cost_time:866ms lr=1.1138e-03 total_loss:2.2802\n",
            "global_steps:280 epoch:1 steps:280/2885 cls_loss:0.5330 cnt_loss:0.6374 reg_loss:1.0000 cost_time:1083ms lr=1.1178e-03 total_loss:2.1703\n",
            "global_steps:281 epoch:1 steps:281/2885 cls_loss:0.6735 cnt_loss:0.6439 reg_loss:0.9999 cost_time:858ms lr=1.1218e-03 total_loss:2.3174\n",
            "global_steps:282 epoch:1 steps:282/2885 cls_loss:0.6523 cnt_loss:0.6642 reg_loss:0.9999 cost_time:969ms lr=1.1257e-03 total_loss:2.3165\n",
            "global_steps:283 epoch:1 steps:283/2885 cls_loss:0.5984 cnt_loss:0.6443 reg_loss:0.9997 cost_time:1092ms lr=1.1297e-03 total_loss:2.2423\n",
            "global_steps:284 epoch:1 steps:284/2885 cls_loss:0.6863 cnt_loss:0.6485 reg_loss:0.9998 cost_time:1110ms lr=1.1337e-03 total_loss:2.3346\n",
            "global_steps:285 epoch:1 steps:285/2885 cls_loss:0.5292 cnt_loss:0.6183 reg_loss:0.9999 cost_time:994ms lr=1.1377e-03 total_loss:2.1474\n",
            "global_steps:286 epoch:1 steps:286/2885 cls_loss:0.5502 cnt_loss:0.6388 reg_loss:1.0001 cost_time:1239ms lr=1.1417e-03 total_loss:2.1890\n",
            "global_steps:287 epoch:1 steps:287/2885 cls_loss:0.6281 cnt_loss:0.6282 reg_loss:1.0000 cost_time:1300ms lr=1.1457e-03 total_loss:2.2563\n",
            "global_steps:288 epoch:1 steps:288/2885 cls_loss:0.6702 cnt_loss:0.6317 reg_loss:0.9995 cost_time:1044ms lr=1.1497e-03 total_loss:2.3014\n",
            "global_steps:289 epoch:1 steps:289/2885 cls_loss:0.6669 cnt_loss:0.6259 reg_loss:0.9999 cost_time:837ms lr=1.1537e-03 total_loss:2.2927\n",
            "global_steps:290 epoch:1 steps:290/2885 cls_loss:0.5799 cnt_loss:0.6317 reg_loss:0.9998 cost_time:857ms lr=1.1577e-03 total_loss:2.2113\n",
            "global_steps:291 epoch:1 steps:291/2885 cls_loss:0.7061 cnt_loss:0.6699 reg_loss:0.9998 cost_time:1094ms lr=1.1617e-03 total_loss:2.3757\n",
            "global_steps:292 epoch:1 steps:292/2885 cls_loss:0.7794 cnt_loss:0.6494 reg_loss:0.9942 cost_time:860ms lr=1.1657e-03 total_loss:2.4230\n",
            "global_steps:293 epoch:1 steps:293/2885 cls_loss:0.5933 cnt_loss:0.6527 reg_loss:0.9986 cost_time:787ms lr=1.1697e-03 total_loss:2.2447\n",
            "global_steps:294 epoch:1 steps:294/2885 cls_loss:0.4600 cnt_loss:0.6275 reg_loss:0.9999 cost_time:904ms lr=1.1737e-03 total_loss:2.0874\n",
            "global_steps:295 epoch:1 steps:295/2885 cls_loss:0.8085 cnt_loss:0.6506 reg_loss:0.9993 cost_time:921ms lr=1.1776e-03 total_loss:2.4584\n",
            "global_steps:296 epoch:1 steps:296/2885 cls_loss:0.5718 cnt_loss:0.6509 reg_loss:0.9997 cost_time:936ms lr=1.1816e-03 total_loss:2.2223\n",
            "global_steps:297 epoch:1 steps:297/2885 cls_loss:0.4541 cnt_loss:0.6350 reg_loss:0.9999 cost_time:769ms lr=1.1856e-03 total_loss:2.0890\n",
            "global_steps:298 epoch:1 steps:298/2885 cls_loss:0.6451 cnt_loss:0.6629 reg_loss:0.9999 cost_time:958ms lr=1.1896e-03 total_loss:2.3079\n",
            "global_steps:299 epoch:1 steps:299/2885 cls_loss:0.5516 cnt_loss:0.6102 reg_loss:0.9999 cost_time:1212ms lr=1.1936e-03 total_loss:2.1618\n",
            "global_steps:300 epoch:1 steps:300/2885 cls_loss:0.6079 cnt_loss:0.6379 reg_loss:0.9997 cost_time:1236ms lr=1.1976e-03 total_loss:2.2454\n",
            "global_steps:301 epoch:1 steps:301/2885 cls_loss:0.5713 cnt_loss:0.6460 reg_loss:0.9998 cost_time:964ms lr=1.2016e-03 total_loss:2.2171\n",
            "global_steps:302 epoch:1 steps:302/2885 cls_loss:0.4566 cnt_loss:0.6343 reg_loss:0.9999 cost_time:1086ms lr=1.2056e-03 total_loss:2.0908\n",
            "global_steps:303 epoch:1 steps:303/2885 cls_loss:0.6405 cnt_loss:0.6793 reg_loss:0.9734 cost_time:965ms lr=1.2096e-03 total_loss:2.2932\n",
            "global_steps:304 epoch:1 steps:304/2885 cls_loss:0.6151 cnt_loss:0.6562 reg_loss:0.9996 cost_time:1006ms lr=1.2136e-03 total_loss:2.2709\n",
            "global_steps:305 epoch:1 steps:305/2885 cls_loss:0.7003 cnt_loss:0.6712 reg_loss:0.9999 cost_time:1219ms lr=1.2176e-03 total_loss:2.3713\n",
            "global_steps:306 epoch:1 steps:306/2885 cls_loss:0.6108 cnt_loss:0.6321 reg_loss:0.9998 cost_time:863ms lr=1.2216e-03 total_loss:2.2428\n",
            "global_steps:307 epoch:1 steps:307/2885 cls_loss:0.7211 cnt_loss:0.6332 reg_loss:0.9999 cost_time:1207ms lr=1.2255e-03 total_loss:2.3542\n",
            "global_steps:308 epoch:1 steps:308/2885 cls_loss:0.6357 cnt_loss:0.6642 reg_loss:0.9999 cost_time:1094ms lr=1.2295e-03 total_loss:2.2998\n",
            "global_steps:309 epoch:1 steps:309/2885 cls_loss:0.5722 cnt_loss:0.6609 reg_loss:0.9994 cost_time:777ms lr=1.2335e-03 total_loss:2.2325\n",
            "global_steps:310 epoch:1 steps:310/2885 cls_loss:0.4102 cnt_loss:0.6211 reg_loss:0.9999 cost_time:800ms lr=1.2375e-03 total_loss:2.0312\n",
            "global_steps:311 epoch:1 steps:311/2885 cls_loss:0.6557 cnt_loss:0.6333 reg_loss:0.9986 cost_time:881ms lr=1.2415e-03 total_loss:2.2876\n",
            "global_steps:312 epoch:1 steps:312/2885 cls_loss:0.5461 cnt_loss:0.6428 reg_loss:0.9999 cost_time:1096ms lr=1.2455e-03 total_loss:2.1888\n",
            "global_steps:313 epoch:1 steps:313/2885 cls_loss:0.5966 cnt_loss:0.6402 reg_loss:0.9996 cost_time:1124ms lr=1.2495e-03 total_loss:2.2363\n",
            "global_steps:314 epoch:1 steps:314/2885 cls_loss:0.6702 cnt_loss:0.6379 reg_loss:0.9999 cost_time:1096ms lr=1.2535e-03 total_loss:2.3079\n",
            "global_steps:315 epoch:1 steps:315/2885 cls_loss:0.7764 cnt_loss:0.6190 reg_loss:0.9985 cost_time:773ms lr=1.2575e-03 total_loss:2.3940\n",
            "global_steps:316 epoch:1 steps:316/2885 cls_loss:0.7423 cnt_loss:0.6445 reg_loss:0.9995 cost_time:861ms lr=1.2615e-03 total_loss:2.3863\n",
            "global_steps:317 epoch:1 steps:317/2885 cls_loss:0.4412 cnt_loss:0.6507 reg_loss:0.9999 cost_time:1086ms lr=1.2655e-03 total_loss:2.0919\n",
            "global_steps:318 epoch:1 steps:318/2885 cls_loss:0.5798 cnt_loss:0.6484 reg_loss:0.9995 cost_time:844ms lr=1.2695e-03 total_loss:2.2277\n",
            "global_steps:319 epoch:1 steps:319/2885 cls_loss:0.6577 cnt_loss:0.6025 reg_loss:0.9997 cost_time:1026ms lr=1.2735e-03 total_loss:2.2599\n",
            "global_steps:320 epoch:1 steps:320/2885 cls_loss:0.7650 cnt_loss:0.6319 reg_loss:0.9990 cost_time:1203ms lr=1.2774e-03 total_loss:2.3959\n",
            "global_steps:321 epoch:1 steps:321/2885 cls_loss:0.6342 cnt_loss:0.6070 reg_loss:0.9994 cost_time:857ms lr=1.2814e-03 total_loss:2.2406\n",
            "global_steps:322 epoch:1 steps:322/2885 cls_loss:0.6345 cnt_loss:0.6383 reg_loss:0.9999 cost_time:1216ms lr=1.2854e-03 total_loss:2.2727\n",
            "global_steps:323 epoch:1 steps:323/2885 cls_loss:0.5687 cnt_loss:0.6332 reg_loss:0.9971 cost_time:823ms lr=1.2894e-03 total_loss:2.1990\n",
            "global_steps:324 epoch:1 steps:324/2885 cls_loss:0.5877 cnt_loss:0.4760 reg_loss:0.7496 cost_time:811ms lr=1.2934e-03 total_loss:1.8133\n",
            "global_steps:325 epoch:1 steps:325/2885 cls_loss:0.6356 cnt_loss:0.6394 reg_loss:0.9969 cost_time:1106ms lr=1.2974e-03 total_loss:2.2720\n",
            "global_steps:326 epoch:1 steps:326/2885 cls_loss:0.6635 cnt_loss:0.4758 reg_loss:0.7484 cost_time:1002ms lr=1.3014e-03 total_loss:1.8878\n",
            "global_steps:327 epoch:1 steps:327/2885 cls_loss:0.8990 cnt_loss:0.6704 reg_loss:0.9939 cost_time:779ms lr=1.3054e-03 total_loss:2.5633\n",
            "global_steps:328 epoch:1 steps:328/2885 cls_loss:0.6573 cnt_loss:0.6485 reg_loss:0.9943 cost_time:1076ms lr=1.3094e-03 total_loss:2.3000\n",
            "global_steps:329 epoch:1 steps:329/2885 cls_loss:0.6959 cnt_loss:0.7186 reg_loss:1.0042 cost_time:971ms lr=1.3134e-03 total_loss:2.4188\n",
            "global_steps:330 epoch:1 steps:330/2885 cls_loss:0.7750 cnt_loss:0.6668 reg_loss:0.9996 cost_time:898ms lr=1.3174e-03 total_loss:2.4414\n",
            "global_steps:331 epoch:1 steps:331/2885 cls_loss:0.7664 cnt_loss:0.6225 reg_loss:0.9995 cost_time:762ms lr=1.3214e-03 total_loss:2.3884\n",
            "global_steps:332 epoch:1 steps:332/2885 cls_loss:0.7875 cnt_loss:0.6521 reg_loss:1.0182 cost_time:879ms lr=1.3253e-03 total_loss:2.4578\n",
            "global_steps:333 epoch:1 steps:333/2885 cls_loss:0.7703 cnt_loss:0.6911 reg_loss:1.0477 cost_time:774ms lr=1.3293e-03 total_loss:2.5091\n",
            "global_steps:334 epoch:1 steps:334/2885 cls_loss:0.7782 cnt_loss:0.6414 reg_loss:0.9995 cost_time:950ms lr=1.3333e-03 total_loss:2.4191\n",
            "global_steps:335 epoch:1 steps:335/2885 cls_loss:0.9009 cnt_loss:0.6491 reg_loss:1.0039 cost_time:1012ms lr=1.3373e-03 total_loss:2.5540\n",
            "global_steps:336 epoch:1 steps:336/2885 cls_loss:0.7702 cnt_loss:0.6989 reg_loss:0.9999 cost_time:915ms lr=1.3413e-03 total_loss:2.4690\n",
            "global_steps:337 epoch:1 steps:337/2885 cls_loss:0.7195 cnt_loss:0.6795 reg_loss:0.9999 cost_time:1056ms lr=1.3453e-03 total_loss:2.3988\n",
            "global_steps:338 epoch:1 steps:338/2885 cls_loss:0.5928 cnt_loss:0.6648 reg_loss:0.9970 cost_time:823ms lr=1.3493e-03 total_loss:2.2546\n",
            "global_steps:339 epoch:1 steps:339/2885 cls_loss:0.8521 cnt_loss:0.6502 reg_loss:0.9999 cost_time:838ms lr=1.3533e-03 total_loss:2.5022\n",
            "global_steps:340 epoch:1 steps:340/2885 cls_loss:0.5874 cnt_loss:0.6786 reg_loss:0.9998 cost_time:980ms lr=1.3573e-03 total_loss:2.2659\n",
            "global_steps:341 epoch:1 steps:341/2885 cls_loss:0.7859 cnt_loss:0.6917 reg_loss:1.2197 cost_time:1037ms lr=1.3613e-03 total_loss:2.6972\n",
            "global_steps:342 epoch:1 steps:342/2885 cls_loss:0.7758 cnt_loss:0.6675 reg_loss:0.9996 cost_time:977ms lr=1.3653e-03 total_loss:2.4428\n",
            "global_steps:343 epoch:1 steps:343/2885 cls_loss:0.8010 cnt_loss:0.6633 reg_loss:0.9995 cost_time:871ms lr=1.3693e-03 total_loss:2.4637\n",
            "global_steps:344 epoch:1 steps:344/2885 cls_loss:0.8354 cnt_loss:0.6856 reg_loss:0.9988 cost_time:1125ms lr=1.3733e-03 total_loss:2.5199\n",
            "global_steps:345 epoch:1 steps:345/2885 cls_loss:0.8660 cnt_loss:0.6994 reg_loss:0.9993 cost_time:866ms lr=1.3772e-03 total_loss:2.5647\n",
            "global_steps:346 epoch:1 steps:346/2885 cls_loss:0.8145 cnt_loss:0.6368 reg_loss:0.9997 cost_time:876ms lr=1.3812e-03 total_loss:2.4510\n",
            "global_steps:347 epoch:1 steps:347/2885 cls_loss:0.8234 cnt_loss:0.6486 reg_loss:0.9999 cost_time:860ms lr=1.3852e-03 total_loss:2.4718\n",
            "global_steps:348 epoch:1 steps:348/2885 cls_loss:0.7210 cnt_loss:0.6407 reg_loss:0.9998 cost_time:903ms lr=1.3892e-03 total_loss:2.3616\n",
            "global_steps:349 epoch:1 steps:349/2885 cls_loss:0.6791 cnt_loss:0.6869 reg_loss:0.9992 cost_time:827ms lr=1.3932e-03 total_loss:2.3652\n",
            "global_steps:350 epoch:1 steps:350/2885 cls_loss:0.6133 cnt_loss:0.6210 reg_loss:0.9999 cost_time:819ms lr=1.3972e-03 total_loss:2.2342\n",
            "global_steps:351 epoch:1 steps:351/2885 cls_loss:0.6772 cnt_loss:0.6558 reg_loss:0.9998 cost_time:1062ms lr=1.4012e-03 total_loss:2.3328\n",
            "global_steps:352 epoch:1 steps:352/2885 cls_loss:0.5724 cnt_loss:0.6365 reg_loss:0.9998 cost_time:774ms lr=1.4052e-03 total_loss:2.2087\n",
            "global_steps:353 epoch:1 steps:353/2885 cls_loss:0.5758 cnt_loss:0.6387 reg_loss:0.9998 cost_time:915ms lr=1.4092e-03 total_loss:2.2143\n",
            "global_steps:354 epoch:1 steps:354/2885 cls_loss:0.6876 cnt_loss:0.6569 reg_loss:0.9999 cost_time:1043ms lr=1.4132e-03 total_loss:2.3444\n",
            "global_steps:355 epoch:1 steps:355/2885 cls_loss:0.6979 cnt_loss:0.6325 reg_loss:0.9999 cost_time:1090ms lr=1.4172e-03 total_loss:2.3303\n",
            "global_steps:356 epoch:1 steps:356/2885 cls_loss:0.5331 cnt_loss:0.6500 reg_loss:1.0000 cost_time:1093ms lr=1.4212e-03 total_loss:2.1831\n",
            "global_steps:357 epoch:1 steps:357/2885 cls_loss:0.5126 cnt_loss:0.6227 reg_loss:0.9996 cost_time:1004ms lr=1.4251e-03 total_loss:2.1349\n",
            "global_steps:358 epoch:1 steps:358/2885 cls_loss:0.7067 cnt_loss:0.6297 reg_loss:0.9999 cost_time:939ms lr=1.4291e-03 total_loss:2.3363\n",
            "global_steps:359 epoch:1 steps:359/2885 cls_loss:0.5900 cnt_loss:0.6450 reg_loss:0.9991 cost_time:783ms lr=1.4331e-03 total_loss:2.2341\n",
            "global_steps:360 epoch:1 steps:360/2885 cls_loss:0.7868 cnt_loss:0.6131 reg_loss:0.9989 cost_time:1109ms lr=1.4371e-03 total_loss:2.3987\n",
            "global_steps:361 epoch:1 steps:361/2885 cls_loss:0.5442 cnt_loss:0.6544 reg_loss:0.9998 cost_time:754ms lr=1.4411e-03 total_loss:2.1984\n",
            "global_steps:362 epoch:1 steps:362/2885 cls_loss:0.5183 cnt_loss:0.5929 reg_loss:0.9999 cost_time:865ms lr=1.4451e-03 total_loss:2.1111\n",
            "global_steps:363 epoch:1 steps:363/2885 cls_loss:0.4360 cnt_loss:0.6294 reg_loss:0.9999 cost_time:948ms lr=1.4491e-03 total_loss:2.0653\n",
            "global_steps:364 epoch:1 steps:364/2885 cls_loss:0.5071 cnt_loss:0.4842 reg_loss:0.7499 cost_time:925ms lr=1.4531e-03 total_loss:1.7413\n",
            "global_steps:365 epoch:1 steps:365/2885 cls_loss:0.6411 cnt_loss:0.4879 reg_loss:0.7500 cost_time:895ms lr=1.4571e-03 total_loss:1.8790\n",
            "global_steps:366 epoch:1 steps:366/2885 cls_loss:0.7616 cnt_loss:0.6176 reg_loss:0.9995 cost_time:869ms lr=1.4611e-03 total_loss:2.3787\n",
            "global_steps:367 epoch:1 steps:367/2885 cls_loss:0.9313 cnt_loss:0.6250 reg_loss:0.9999 cost_time:790ms lr=1.4651e-03 total_loss:2.5562\n",
            "global_steps:368 epoch:1 steps:368/2885 cls_loss:1.0059 cnt_loss:0.6633 reg_loss:0.9998 cost_time:808ms lr=1.4691e-03 total_loss:2.6690\n",
            "global_steps:369 epoch:1 steps:369/2885 cls_loss:0.8823 cnt_loss:0.6509 reg_loss:0.9999 cost_time:871ms lr=1.4731e-03 total_loss:2.5331\n",
            "global_steps:370 epoch:1 steps:370/2885 cls_loss:0.8922 cnt_loss:0.6474 reg_loss:0.9998 cost_time:861ms lr=1.4770e-03 total_loss:2.5394\n",
            "global_steps:371 epoch:1 steps:371/2885 cls_loss:0.8843 cnt_loss:0.6386 reg_loss:0.9999 cost_time:861ms lr=1.4810e-03 total_loss:2.5228\n",
            "global_steps:372 epoch:1 steps:372/2885 cls_loss:0.7293 cnt_loss:0.6242 reg_loss:0.9998 cost_time:965ms lr=1.4850e-03 total_loss:2.3533\n",
            "global_steps:373 epoch:1 steps:373/2885 cls_loss:0.6708 cnt_loss:0.6309 reg_loss:0.9976 cost_time:873ms lr=1.4890e-03 total_loss:2.2992\n",
            "global_steps:374 epoch:1 steps:374/2885 cls_loss:0.8357 cnt_loss:0.6363 reg_loss:0.9996 cost_time:863ms lr=1.4930e-03 total_loss:2.4717\n",
            "global_steps:375 epoch:1 steps:375/2885 cls_loss:0.5198 cnt_loss:0.6258 reg_loss:0.9999 cost_time:856ms lr=1.4970e-03 total_loss:2.1455\n",
            "global_steps:376 epoch:1 steps:376/2885 cls_loss:0.6961 cnt_loss:0.6420 reg_loss:0.9999 cost_time:727ms lr=1.5010e-03 total_loss:2.3381\n",
            "global_steps:377 epoch:1 steps:377/2885 cls_loss:0.6260 cnt_loss:0.6233 reg_loss:1.0000 cost_time:909ms lr=1.5050e-03 total_loss:2.2492\n",
            "global_steps:378 epoch:1 steps:378/2885 cls_loss:0.6266 cnt_loss:0.6242 reg_loss:0.9997 cost_time:1120ms lr=1.5090e-03 total_loss:2.2505\n",
            "global_steps:379 epoch:1 steps:379/2885 cls_loss:0.6910 cnt_loss:0.6216 reg_loss:0.9998 cost_time:928ms lr=1.5130e-03 total_loss:2.3124\n",
            "global_steps:380 epoch:1 steps:380/2885 cls_loss:0.7904 cnt_loss:0.6516 reg_loss:1.0007 cost_time:1345ms lr=1.5170e-03 total_loss:2.4428\n",
            "global_steps:381 epoch:1 steps:381/2885 cls_loss:0.5550 cnt_loss:0.6272 reg_loss:0.9998 cost_time:795ms lr=1.5210e-03 total_loss:2.1821\n",
            "global_steps:382 epoch:1 steps:382/2885 cls_loss:0.7985 cnt_loss:0.6364 reg_loss:0.9954 cost_time:876ms lr=1.5250e-03 total_loss:2.4303\n",
            "global_steps:383 epoch:1 steps:383/2885 cls_loss:0.5889 cnt_loss:0.6460 reg_loss:0.9990 cost_time:801ms lr=1.5289e-03 total_loss:2.2339\n",
            "global_steps:384 epoch:1 steps:384/2885 cls_loss:0.6528 cnt_loss:0.6400 reg_loss:0.9993 cost_time:920ms lr=1.5329e-03 total_loss:2.2921\n",
            "global_steps:385 epoch:1 steps:385/2885 cls_loss:0.6004 cnt_loss:0.6313 reg_loss:0.9998 cost_time:858ms lr=1.5369e-03 total_loss:2.2316\n",
            "global_steps:386 epoch:1 steps:386/2885 cls_loss:0.5324 cnt_loss:0.5986 reg_loss:1.0000 cost_time:951ms lr=1.5409e-03 total_loss:2.1310\n",
            "global_steps:387 epoch:1 steps:387/2885 cls_loss:0.6090 cnt_loss:0.6585 reg_loss:0.9996 cost_time:1010ms lr=1.5449e-03 total_loss:2.2671\n",
            "global_steps:388 epoch:1 steps:388/2885 cls_loss:0.8358 cnt_loss:0.6335 reg_loss:0.9995 cost_time:1228ms lr=1.5489e-03 total_loss:2.4688\n",
            "global_steps:389 epoch:1 steps:389/2885 cls_loss:0.6226 cnt_loss:0.6412 reg_loss:0.9997 cost_time:863ms lr=1.5529e-03 total_loss:2.2634\n",
            "global_steps:390 epoch:1 steps:390/2885 cls_loss:0.9007 cnt_loss:0.6361 reg_loss:0.9838 cost_time:845ms lr=1.5569e-03 total_loss:2.5206\n",
            "global_steps:391 epoch:1 steps:391/2885 cls_loss:0.5082 cnt_loss:0.6045 reg_loss:0.9999 cost_time:852ms lr=1.5609e-03 total_loss:2.1125\n",
            "global_steps:392 epoch:1 steps:392/2885 cls_loss:0.8204 cnt_loss:0.6467 reg_loss:0.9975 cost_time:947ms lr=1.5649e-03 total_loss:2.4645\n",
            "global_steps:393 epoch:1 steps:393/2885 cls_loss:0.8874 cnt_loss:0.6058 reg_loss:0.9999 cost_time:1214ms lr=1.5689e-03 total_loss:2.4930\n",
            "global_steps:394 epoch:1 steps:394/2885 cls_loss:0.8183 cnt_loss:0.6498 reg_loss:0.9999 cost_time:800ms lr=1.5729e-03 total_loss:2.4680\n",
            "global_steps:395 epoch:1 steps:395/2885 cls_loss:0.7535 cnt_loss:0.6618 reg_loss:0.9999 cost_time:1184ms lr=1.5768e-03 total_loss:2.4152\n",
            "global_steps:396 epoch:1 steps:396/2885 cls_loss:0.9742 cnt_loss:0.4819 reg_loss:0.7484 cost_time:998ms lr=1.5808e-03 total_loss:2.2045\n",
            "global_steps:397 epoch:1 steps:397/2885 cls_loss:0.5705 cnt_loss:0.6285 reg_loss:1.0000 cost_time:971ms lr=1.5848e-03 total_loss:2.1990\n",
            "global_steps:398 epoch:1 steps:398/2885 cls_loss:0.8087 cnt_loss:0.4755 reg_loss:0.7498 cost_time:805ms lr=1.5888e-03 total_loss:2.0340\n",
            "global_steps:399 epoch:1 steps:399/2885 cls_loss:0.8245 cnt_loss:0.6165 reg_loss:1.0000 cost_time:1123ms lr=1.5928e-03 total_loss:2.4410\n",
            "global_steps:400 epoch:1 steps:400/2885 cls_loss:0.9431 cnt_loss:0.6334 reg_loss:0.9992 cost_time:1170ms lr=1.5968e-03 total_loss:2.5757\n",
            "global_steps:401 epoch:1 steps:401/2885 cls_loss:0.8216 cnt_loss:0.6406 reg_loss:0.9998 cost_time:989ms lr=1.6008e-03 total_loss:2.4620\n",
            "global_steps:402 epoch:1 steps:402/2885 cls_loss:0.7751 cnt_loss:0.6347 reg_loss:0.9999 cost_time:785ms lr=1.6048e-03 total_loss:2.4097\n",
            "global_steps:403 epoch:1 steps:403/2885 cls_loss:0.8713 cnt_loss:0.6316 reg_loss:0.9999 cost_time:1197ms lr=1.6088e-03 total_loss:2.5028\n",
            "global_steps:404 epoch:1 steps:404/2885 cls_loss:0.5934 cnt_loss:0.6281 reg_loss:0.9999 cost_time:980ms lr=1.6128e-03 total_loss:2.2215\n",
            "global_steps:405 epoch:1 steps:405/2885 cls_loss:1.0600 cnt_loss:0.6281 reg_loss:0.9995 cost_time:1078ms lr=1.6168e-03 total_loss:2.6876\n",
            "global_steps:406 epoch:1 steps:406/2885 cls_loss:0.6514 cnt_loss:0.6236 reg_loss:0.9999 cost_time:1021ms lr=1.6208e-03 total_loss:2.2749\n",
            "global_steps:407 epoch:1 steps:407/2885 cls_loss:0.6843 cnt_loss:0.6317 reg_loss:0.9999 cost_time:1241ms lr=1.6248e-03 total_loss:2.3159\n",
            "global_steps:408 epoch:1 steps:408/2885 cls_loss:0.8549 cnt_loss:0.6140 reg_loss:0.9996 cost_time:824ms lr=1.6287e-03 total_loss:2.4685\n",
            "global_steps:409 epoch:1 steps:409/2885 cls_loss:0.9671 cnt_loss:0.6779 reg_loss:0.9993 cost_time:862ms lr=1.6327e-03 total_loss:2.6444\n",
            "global_steps:410 epoch:1 steps:410/2885 cls_loss:0.7025 cnt_loss:0.6272 reg_loss:0.9995 cost_time:1117ms lr=1.6367e-03 total_loss:2.3292\n",
            "global_steps:411 epoch:1 steps:411/2885 cls_loss:0.6600 cnt_loss:0.6436 reg_loss:0.9999 cost_time:859ms lr=1.6407e-03 total_loss:2.3035\n",
            "global_steps:412 epoch:1 steps:412/2885 cls_loss:0.7969 cnt_loss:0.6524 reg_loss:0.9999 cost_time:791ms lr=1.6447e-03 total_loss:2.4492\n",
            "global_steps:413 epoch:1 steps:413/2885 cls_loss:0.8123 cnt_loss:0.6436 reg_loss:0.9998 cost_time:1119ms lr=1.6487e-03 total_loss:2.4557\n",
            "global_steps:414 epoch:1 steps:414/2885 cls_loss:0.7627 cnt_loss:0.6322 reg_loss:0.9999 cost_time:800ms lr=1.6527e-03 total_loss:2.3948\n",
            "global_steps:415 epoch:1 steps:415/2885 cls_loss:0.6973 cnt_loss:0.6209 reg_loss:1.0000 cost_time:779ms lr=1.6567e-03 total_loss:2.3182\n",
            "global_steps:416 epoch:1 steps:416/2885 cls_loss:0.7994 cnt_loss:0.6675 reg_loss:0.9999 cost_time:779ms lr=1.6607e-03 total_loss:2.4668\n",
            "global_steps:417 epoch:1 steps:417/2885 cls_loss:0.7873 cnt_loss:0.6516 reg_loss:1.0000 cost_time:986ms lr=1.6647e-03 total_loss:2.4388\n",
            "global_steps:418 epoch:1 steps:418/2885 cls_loss:0.8428 cnt_loss:0.6475 reg_loss:0.9997 cost_time:1147ms lr=1.6687e-03 total_loss:2.4899\n",
            "global_steps:419 epoch:1 steps:419/2885 cls_loss:0.7062 cnt_loss:0.6378 reg_loss:0.9999 cost_time:873ms lr=1.6727e-03 total_loss:2.3440\n",
            "global_steps:420 epoch:1 steps:420/2885 cls_loss:0.7201 cnt_loss:0.6618 reg_loss:0.9994 cost_time:1342ms lr=1.6766e-03 total_loss:2.3814\n",
            "global_steps:421 epoch:1 steps:421/2885 cls_loss:0.7260 cnt_loss:0.6668 reg_loss:0.9999 cost_time:945ms lr=1.6806e-03 total_loss:2.3927\n",
            "global_steps:422 epoch:1 steps:422/2885 cls_loss:0.7505 cnt_loss:0.6244 reg_loss:0.9998 cost_time:823ms lr=1.6846e-03 total_loss:2.3746\n",
            "global_steps:423 epoch:1 steps:423/2885 cls_loss:0.8488 cnt_loss:0.6765 reg_loss:1.0000 cost_time:763ms lr=1.6886e-03 total_loss:2.5253\n",
            "global_steps:424 epoch:1 steps:424/2885 cls_loss:0.5231 cnt_loss:0.6222 reg_loss:1.0000 cost_time:927ms lr=1.6926e-03 total_loss:2.1453\n",
            "global_steps:425 epoch:1 steps:425/2885 cls_loss:0.7508 cnt_loss:0.6387 reg_loss:1.0000 cost_time:1095ms lr=1.6966e-03 total_loss:2.3895\n",
            "global_steps:426 epoch:1 steps:426/2885 cls_loss:0.8329 cnt_loss:0.6306 reg_loss:0.9999 cost_time:849ms lr=1.7006e-03 total_loss:2.4635\n",
            "global_steps:427 epoch:1 steps:427/2885 cls_loss:0.7354 cnt_loss:0.6248 reg_loss:0.9999 cost_time:960ms lr=1.7046e-03 total_loss:2.3602\n",
            "global_steps:428 epoch:1 steps:428/2885 cls_loss:0.6543 cnt_loss:0.6331 reg_loss:0.9999 cost_time:889ms lr=1.7086e-03 total_loss:2.2872\n",
            "global_steps:429 epoch:1 steps:429/2885 cls_loss:0.7935 cnt_loss:0.6382 reg_loss:0.9997 cost_time:1242ms lr=1.7126e-03 total_loss:2.4314\n",
            "global_steps:430 epoch:1 steps:430/2885 cls_loss:0.7517 cnt_loss:0.6540 reg_loss:0.9997 cost_time:1027ms lr=1.7166e-03 total_loss:2.4053\n",
            "global_steps:431 epoch:1 steps:431/2885 cls_loss:0.5060 cnt_loss:0.6487 reg_loss:1.0000 cost_time:1056ms lr=1.7206e-03 total_loss:2.1547\n",
            "global_steps:432 epoch:1 steps:432/2885 cls_loss:0.7018 cnt_loss:0.6322 reg_loss:0.9998 cost_time:992ms lr=1.7246e-03 total_loss:2.3338\n",
            "global_steps:433 epoch:1 steps:433/2885 cls_loss:0.5794 cnt_loss:0.6148 reg_loss:1.0000 cost_time:852ms lr=1.7285e-03 total_loss:2.1942\n",
            "global_steps:434 epoch:1 steps:434/2885 cls_loss:0.4634 cnt_loss:0.6581 reg_loss:0.9999 cost_time:1100ms lr=1.7325e-03 total_loss:2.1215\n",
            "global_steps:435 epoch:1 steps:435/2885 cls_loss:0.6016 cnt_loss:0.6411 reg_loss:1.0002 cost_time:860ms lr=1.7365e-03 total_loss:2.2430\n",
            "global_steps:436 epoch:1 steps:436/2885 cls_loss:0.8154 cnt_loss:0.6470 reg_loss:0.9999 cost_time:952ms lr=1.7405e-03 total_loss:2.4623\n",
            "global_steps:437 epoch:1 steps:437/2885 cls_loss:0.6161 cnt_loss:0.6637 reg_loss:0.9999 cost_time:1150ms lr=1.7445e-03 total_loss:2.2797\n",
            "global_steps:438 epoch:1 steps:438/2885 cls_loss:0.5320 cnt_loss:0.6446 reg_loss:0.9999 cost_time:1090ms lr=1.7485e-03 total_loss:2.1765\n",
            "global_steps:439 epoch:1 steps:439/2885 cls_loss:0.6193 cnt_loss:0.6551 reg_loss:0.9999 cost_time:973ms lr=1.7525e-03 total_loss:2.2742\n",
            "global_steps:440 epoch:1 steps:440/2885 cls_loss:0.6193 cnt_loss:0.6467 reg_loss:0.9997 cost_time:862ms lr=1.7565e-03 total_loss:2.2658\n",
            "global_steps:441 epoch:1 steps:441/2885 cls_loss:0.7485 cnt_loss:0.6200 reg_loss:0.9999 cost_time:1105ms lr=1.7605e-03 total_loss:2.3683\n",
            "global_steps:442 epoch:1 steps:442/2885 cls_loss:0.8472 cnt_loss:0.6286 reg_loss:0.9999 cost_time:995ms lr=1.7645e-03 total_loss:2.4758\n",
            "global_steps:443 epoch:1 steps:443/2885 cls_loss:0.7558 cnt_loss:0.4742 reg_loss:0.7499 cost_time:1092ms lr=1.7685e-03 total_loss:1.9799\n",
            "global_steps:444 epoch:1 steps:444/2885 cls_loss:0.5429 cnt_loss:0.4807 reg_loss:0.7499 cost_time:743ms lr=1.7725e-03 total_loss:1.7735\n",
            "global_steps:445 epoch:1 steps:445/2885 cls_loss:0.5856 cnt_loss:0.5951 reg_loss:0.9999 cost_time:995ms lr=1.7764e-03 total_loss:2.1806\n",
            "global_steps:446 epoch:1 steps:446/2885 cls_loss:0.6956 cnt_loss:0.6226 reg_loss:0.9994 cost_time:992ms lr=1.7804e-03 total_loss:2.3176\n",
            "global_steps:447 epoch:1 steps:447/2885 cls_loss:0.8172 cnt_loss:0.6576 reg_loss:0.9985 cost_time:893ms lr=1.7844e-03 total_loss:2.4733\n",
            "global_steps:448 epoch:1 steps:448/2885 cls_loss:0.5514 cnt_loss:0.5990 reg_loss:0.9996 cost_time:777ms lr=1.7884e-03 total_loss:2.1500\n",
            "global_steps:449 epoch:1 steps:449/2885 cls_loss:0.5355 cnt_loss:0.6078 reg_loss:0.9999 cost_time:1094ms lr=1.7924e-03 total_loss:2.1431\n",
            "global_steps:450 epoch:1 steps:450/2885 cls_loss:0.8820 cnt_loss:0.6320 reg_loss:0.9998 cost_time:856ms lr=1.7964e-03 total_loss:2.5139\n",
            "global_steps:451 epoch:1 steps:451/2885 cls_loss:0.5766 cnt_loss:0.6227 reg_loss:0.9998 cost_time:1117ms lr=1.8004e-03 total_loss:2.1991\n",
            "global_steps:452 epoch:1 steps:452/2885 cls_loss:0.6787 cnt_loss:0.6151 reg_loss:0.9999 cost_time:918ms lr=1.8044e-03 total_loss:2.2938\n",
            "global_steps:453 epoch:1 steps:453/2885 cls_loss:0.5969 cnt_loss:0.6177 reg_loss:1.0000 cost_time:1088ms lr=1.8084e-03 total_loss:2.2145\n",
            "global_steps:454 epoch:1 steps:454/2885 cls_loss:0.5177 cnt_loss:0.6468 reg_loss:1.0000 cost_time:852ms lr=1.8124e-03 total_loss:2.1646\n",
            "global_steps:455 epoch:1 steps:455/2885 cls_loss:0.5649 cnt_loss:0.6410 reg_loss:1.0000 cost_time:958ms lr=1.8164e-03 total_loss:2.2059\n",
            "global_steps:456 epoch:1 steps:456/2885 cls_loss:0.7850 cnt_loss:0.6222 reg_loss:0.9997 cost_time:846ms lr=1.8204e-03 total_loss:2.4069\n",
            "global_steps:457 epoch:1 steps:457/2885 cls_loss:0.6082 cnt_loss:0.5954 reg_loss:1.0000 cost_time:889ms lr=1.8244e-03 total_loss:2.2036\n",
            "global_steps:458 epoch:1 steps:458/2885 cls_loss:0.6151 cnt_loss:0.6182 reg_loss:0.9999 cost_time:946ms lr=1.8283e-03 total_loss:2.2332\n",
            "global_steps:459 epoch:1 steps:459/2885 cls_loss:0.8009 cnt_loss:0.6638 reg_loss:0.9999 cost_time:777ms lr=1.8323e-03 total_loss:2.4645\n",
            "global_steps:460 epoch:1 steps:460/2885 cls_loss:0.6604 cnt_loss:0.6144 reg_loss:0.9998 cost_time:875ms lr=1.8363e-03 total_loss:2.2746\n",
            "global_steps:461 epoch:1 steps:461/2885 cls_loss:0.6192 cnt_loss:0.6496 reg_loss:0.9999 cost_time:856ms lr=1.8403e-03 total_loss:2.2687\n",
            "global_steps:462 epoch:1 steps:462/2885 cls_loss:0.6672 cnt_loss:0.6289 reg_loss:0.9999 cost_time:838ms lr=1.8443e-03 total_loss:2.2960\n",
            "global_steps:463 epoch:1 steps:463/2885 cls_loss:0.5924 cnt_loss:0.6227 reg_loss:1.0000 cost_time:1115ms lr=1.8483e-03 total_loss:2.2150\n",
            "global_steps:464 epoch:1 steps:464/2885 cls_loss:0.5175 cnt_loss:0.6288 reg_loss:0.9999 cost_time:871ms lr=1.8523e-03 total_loss:2.1462\n",
            "global_steps:465 epoch:1 steps:465/2885 cls_loss:0.5975 cnt_loss:0.6615 reg_loss:0.9998 cost_time:975ms lr=1.8563e-03 total_loss:2.2589\n",
            "global_steps:466 epoch:1 steps:466/2885 cls_loss:0.7040 cnt_loss:0.6376 reg_loss:0.9999 cost_time:1132ms lr=1.8603e-03 total_loss:2.3415\n",
            "global_steps:467 epoch:1 steps:467/2885 cls_loss:0.5692 cnt_loss:0.6421 reg_loss:0.9998 cost_time:837ms lr=1.8643e-03 total_loss:2.2112\n",
            "global_steps:468 epoch:1 steps:468/2885 cls_loss:0.8402 cnt_loss:0.6458 reg_loss:0.9999 cost_time:919ms lr=1.8683e-03 total_loss:2.4859\n",
            "global_steps:469 epoch:1 steps:469/2885 cls_loss:0.5150 cnt_loss:0.6359 reg_loss:1.0000 cost_time:870ms lr=1.8723e-03 total_loss:2.1509\n",
            "global_steps:470 epoch:1 steps:470/2885 cls_loss:0.4962 cnt_loss:0.6260 reg_loss:0.9999 cost_time:1092ms lr=1.8762e-03 total_loss:2.1221\n",
            "global_steps:471 epoch:1 steps:471/2885 cls_loss:0.6059 cnt_loss:0.6079 reg_loss:0.9998 cost_time:799ms lr=1.8802e-03 total_loss:2.2136\n",
            "global_steps:472 epoch:1 steps:472/2885 cls_loss:0.6119 cnt_loss:0.6674 reg_loss:1.0000 cost_time:963ms lr=1.8842e-03 total_loss:2.2792\n",
            "global_steps:473 epoch:1 steps:473/2885 cls_loss:0.4923 cnt_loss:0.6391 reg_loss:0.9998 cost_time:1096ms lr=1.8882e-03 total_loss:2.1312\n",
            "global_steps:474 epoch:1 steps:474/2885 cls_loss:0.5670 cnt_loss:0.6256 reg_loss:0.9999 cost_time:880ms lr=1.8922e-03 total_loss:2.1925\n",
            "global_steps:475 epoch:1 steps:475/2885 cls_loss:0.5960 cnt_loss:0.6468 reg_loss:0.9999 cost_time:1061ms lr=1.8962e-03 total_loss:2.2428\n",
            "global_steps:476 epoch:1 steps:476/2885 cls_loss:0.6173 cnt_loss:0.5007 reg_loss:0.7500 cost_time:1061ms lr=1.9002e-03 total_loss:1.8681\n",
            "global_steps:477 epoch:1 steps:477/2885 cls_loss:0.5404 cnt_loss:0.6267 reg_loss:0.9999 cost_time:992ms lr=1.9042e-03 total_loss:2.1670\n",
            "global_steps:478 epoch:1 steps:478/2885 cls_loss:0.5938 cnt_loss:0.6374 reg_loss:0.9999 cost_time:1036ms lr=1.9082e-03 total_loss:2.2311\n",
            "global_steps:479 epoch:1 steps:479/2885 cls_loss:0.7613 cnt_loss:0.6326 reg_loss:0.9999 cost_time:734ms lr=1.9122e-03 total_loss:2.3938\n",
            "global_steps:480 epoch:1 steps:480/2885 cls_loss:0.4676 cnt_loss:0.4906 reg_loss:0.7495 cost_time:798ms lr=1.9162e-03 total_loss:1.7077\n",
            "global_steps:481 epoch:1 steps:481/2885 cls_loss:0.6475 cnt_loss:0.6555 reg_loss:0.9996 cost_time:1001ms lr=1.9202e-03 total_loss:2.3026\n",
            "global_steps:482 epoch:1 steps:482/2885 cls_loss:0.7303 cnt_loss:0.6391 reg_loss:0.9998 cost_time:1281ms lr=1.9242e-03 total_loss:2.3691\n",
            "global_steps:483 epoch:1 steps:483/2885 cls_loss:0.7681 cnt_loss:0.6313 reg_loss:0.9999 cost_time:773ms lr=1.9281e-03 total_loss:2.3993\n",
            "global_steps:484 epoch:1 steps:484/2885 cls_loss:0.5332 cnt_loss:0.6682 reg_loss:1.0000 cost_time:911ms lr=1.9321e-03 total_loss:2.2013\n",
            "global_steps:485 epoch:1 steps:485/2885 cls_loss:0.7459 cnt_loss:0.6273 reg_loss:1.0000 cost_time:1203ms lr=1.9361e-03 total_loss:2.3732\n",
            "global_steps:486 epoch:1 steps:486/2885 cls_loss:0.5432 cnt_loss:0.6443 reg_loss:0.9998 cost_time:979ms lr=1.9401e-03 total_loss:2.1873\n",
            "global_steps:487 epoch:1 steps:487/2885 cls_loss:0.5380 cnt_loss:0.6386 reg_loss:0.9998 cost_time:1076ms lr=1.9441e-03 total_loss:2.1764\n",
            "global_steps:488 epoch:1 steps:488/2885 cls_loss:0.6502 cnt_loss:0.6420 reg_loss:1.0000 cost_time:893ms lr=1.9481e-03 total_loss:2.2921\n",
            "global_steps:489 epoch:1 steps:489/2885 cls_loss:0.6873 cnt_loss:0.6221 reg_loss:0.9998 cost_time:861ms lr=1.9521e-03 total_loss:2.3092\n",
            "global_steps:490 epoch:1 steps:490/2885 cls_loss:0.5909 cnt_loss:0.6228 reg_loss:0.9999 cost_time:1223ms lr=1.9561e-03 total_loss:2.2136\n",
            "global_steps:491 epoch:1 steps:491/2885 cls_loss:0.5855 cnt_loss:0.6558 reg_loss:0.9997 cost_time:874ms lr=1.9601e-03 total_loss:2.2411\n",
            "global_steps:492 epoch:1 steps:492/2885 cls_loss:0.6846 cnt_loss:0.6614 reg_loss:1.0000 cost_time:854ms lr=1.9641e-03 total_loss:2.3459\n",
            "global_steps:493 epoch:1 steps:493/2885 cls_loss:0.6108 cnt_loss:0.6346 reg_loss:0.9999 cost_time:841ms lr=1.9681e-03 total_loss:2.2453\n",
            "global_steps:494 epoch:1 steps:494/2885 cls_loss:0.6931 cnt_loss:0.6166 reg_loss:0.9999 cost_time:858ms lr=1.9721e-03 total_loss:2.3096\n",
            "global_steps:495 epoch:1 steps:495/2885 cls_loss:0.4864 cnt_loss:0.6435 reg_loss:0.9999 cost_time:1011ms lr=1.9760e-03 total_loss:2.1298\n",
            "global_steps:496 epoch:1 steps:496/2885 cls_loss:0.5656 cnt_loss:0.6170 reg_loss:1.0000 cost_time:976ms lr=1.9800e-03 total_loss:2.1826\n",
            "global_steps:497 epoch:1 steps:497/2885 cls_loss:0.7577 cnt_loss:0.6219 reg_loss:0.9999 cost_time:906ms lr=1.9840e-03 total_loss:2.3795\n",
            "global_steps:498 epoch:1 steps:498/2885 cls_loss:0.7303 cnt_loss:0.6289 reg_loss:1.0000 cost_time:865ms lr=1.9880e-03 total_loss:2.3591\n",
            "global_steps:499 epoch:1 steps:499/2885 cls_loss:0.7159 cnt_loss:0.6261 reg_loss:0.9996 cost_time:788ms lr=1.9920e-03 total_loss:2.3417\n",
            "global_steps:500 epoch:1 steps:500/2885 cls_loss:0.5625 cnt_loss:0.6376 reg_loss:0.9999 cost_time:964ms lr=1.9960e-03 total_loss:2.2000\n",
            "global_steps:501 epoch:1 steps:501/2885 cls_loss:0.4787 cnt_loss:0.6383 reg_loss:0.9998 cost_time:849ms lr=1.9960e-03 total_loss:2.1169\n",
            "global_steps:502 epoch:1 steps:502/2885 cls_loss:0.8306 cnt_loss:0.6240 reg_loss:0.9999 cost_time:856ms lr=1.9960e-03 total_loss:2.4546\n",
            "global_steps:503 epoch:1 steps:503/2885 cls_loss:0.6759 cnt_loss:0.5152 reg_loss:0.7499 cost_time:840ms lr=1.9960e-03 total_loss:1.9410\n",
            "global_steps:504 epoch:1 steps:504/2885 cls_loss:0.6919 cnt_loss:0.6272 reg_loss:1.0000 cost_time:893ms lr=1.9960e-03 total_loss:2.3190\n",
            "global_steps:505 epoch:1 steps:505/2885 cls_loss:0.6709 cnt_loss:0.6234 reg_loss:0.9997 cost_time:1104ms lr=1.9960e-03 total_loss:2.2940\n",
            "global_steps:506 epoch:1 steps:506/2885 cls_loss:0.5387 cnt_loss:0.6414 reg_loss:0.9999 cost_time:1223ms lr=1.9960e-03 total_loss:2.1800\n",
            "global_steps:507 epoch:1 steps:507/2885 cls_loss:0.6084 cnt_loss:0.6166 reg_loss:1.0000 cost_time:766ms lr=1.9960e-03 total_loss:2.2250\n",
            "global_steps:508 epoch:1 steps:508/2885 cls_loss:0.5790 cnt_loss:0.6151 reg_loss:0.9999 cost_time:982ms lr=1.9960e-03 total_loss:2.1941\n",
            "global_steps:509 epoch:1 steps:509/2885 cls_loss:0.7092 cnt_loss:0.6432 reg_loss:0.9999 cost_time:1094ms lr=1.9960e-03 total_loss:2.3523\n",
            "global_steps:510 epoch:1 steps:510/2885 cls_loss:0.7299 cnt_loss:0.6675 reg_loss:0.9997 cost_time:908ms lr=1.9960e-03 total_loss:2.3971\n",
            "global_steps:511 epoch:1 steps:511/2885 cls_loss:0.8167 cnt_loss:0.6067 reg_loss:0.9999 cost_time:870ms lr=1.9960e-03 total_loss:2.4233\n",
            "global_steps:512 epoch:1 steps:512/2885 cls_loss:0.6117 cnt_loss:0.6431 reg_loss:0.9991 cost_time:1015ms lr=1.9960e-03 total_loss:2.2539\n",
            "global_steps:513 epoch:1 steps:513/2885 cls_loss:0.4825 cnt_loss:0.6560 reg_loss:1.0003 cost_time:1091ms lr=1.9960e-03 total_loss:2.1388\n",
            "global_steps:514 epoch:1 steps:514/2885 cls_loss:0.5431 cnt_loss:0.6162 reg_loss:1.0000 cost_time:972ms lr=1.9960e-03 total_loss:2.1592\n",
            "global_steps:515 epoch:1 steps:515/2885 cls_loss:0.6442 cnt_loss:0.6446 reg_loss:1.0000 cost_time:847ms lr=1.9960e-03 total_loss:2.2888\n",
            "global_steps:516 epoch:1 steps:516/2885 cls_loss:0.5520 cnt_loss:0.6354 reg_loss:0.9999 cost_time:1177ms lr=1.9960e-03 total_loss:2.1873\n",
            "global_steps:517 epoch:1 steps:517/2885 cls_loss:0.6627 cnt_loss:0.6182 reg_loss:0.9997 cost_time:969ms lr=1.9960e-03 total_loss:2.2806\n",
            "global_steps:518 epoch:1 steps:518/2885 cls_loss:0.6675 cnt_loss:0.6295 reg_loss:0.9999 cost_time:1088ms lr=1.9960e-03 total_loss:2.2970\n",
            "global_steps:519 epoch:1 steps:519/2885 cls_loss:0.6129 cnt_loss:0.6324 reg_loss:0.9998 cost_time:875ms lr=1.9960e-03 total_loss:2.2451\n",
            "global_steps:520 epoch:1 steps:520/2885 cls_loss:0.5697 cnt_loss:0.6265 reg_loss:0.9995 cost_time:870ms lr=1.9960e-03 total_loss:2.1957\n",
            "global_steps:521 epoch:1 steps:521/2885 cls_loss:0.6735 cnt_loss:0.6377 reg_loss:0.9999 cost_time:975ms lr=1.9960e-03 total_loss:2.3110\n",
            "global_steps:522 epoch:1 steps:522/2885 cls_loss:0.5896 cnt_loss:0.6529 reg_loss:0.9999 cost_time:1234ms lr=1.9960e-03 total_loss:2.2424\n",
            "global_steps:523 epoch:1 steps:523/2885 cls_loss:0.5226 cnt_loss:0.6496 reg_loss:0.9992 cost_time:958ms lr=1.9960e-03 total_loss:2.1714\n",
            "global_steps:524 epoch:1 steps:524/2885 cls_loss:0.6271 cnt_loss:0.6426 reg_loss:0.9997 cost_time:813ms lr=1.9960e-03 total_loss:2.2694\n",
            "global_steps:525 epoch:1 steps:525/2885 cls_loss:0.5654 cnt_loss:0.6689 reg_loss:0.9999 cost_time:861ms lr=1.9960e-03 total_loss:2.2343\n",
            "global_steps:526 epoch:1 steps:526/2885 cls_loss:0.5415 cnt_loss:0.6200 reg_loss:0.9997 cost_time:940ms lr=1.9960e-03 total_loss:2.1613\n",
            "global_steps:527 epoch:1 steps:527/2885 cls_loss:0.8231 cnt_loss:0.6155 reg_loss:0.9999 cost_time:979ms lr=1.9960e-03 total_loss:2.4385\n",
            "global_steps:528 epoch:1 steps:528/2885 cls_loss:0.5141 cnt_loss:0.6686 reg_loss:0.9999 cost_time:760ms lr=1.9960e-03 total_loss:2.1826\n",
            "global_steps:529 epoch:1 steps:529/2885 cls_loss:0.4350 cnt_loss:0.6232 reg_loss:1.0000 cost_time:982ms lr=1.9960e-03 total_loss:2.0582\n",
            "global_steps:530 epoch:1 steps:530/2885 cls_loss:0.6739 cnt_loss:0.6155 reg_loss:0.9999 cost_time:1095ms lr=1.9960e-03 total_loss:2.2893\n",
            "global_steps:531 epoch:1 steps:531/2885 cls_loss:0.7052 cnt_loss:0.6100 reg_loss:1.0000 cost_time:947ms lr=1.9960e-03 total_loss:2.3152\n",
            "global_steps:532 epoch:1 steps:532/2885 cls_loss:0.5549 cnt_loss:0.6268 reg_loss:1.0000 cost_time:939ms lr=1.9960e-03 total_loss:2.1816\n",
            "global_steps:533 epoch:1 steps:533/2885 cls_loss:0.6081 cnt_loss:0.6199 reg_loss:0.9999 cost_time:1229ms lr=1.9960e-03 total_loss:2.2279\n",
            "global_steps:534 epoch:1 steps:534/2885 cls_loss:0.5317 cnt_loss:0.6642 reg_loss:0.9996 cost_time:879ms lr=1.9960e-03 total_loss:2.1955\n",
            "global_steps:535 epoch:1 steps:535/2885 cls_loss:0.5447 cnt_loss:0.6281 reg_loss:0.9999 cost_time:1037ms lr=1.9960e-03 total_loss:2.1727\n",
            "global_steps:536 epoch:1 steps:536/2885 cls_loss:0.5493 cnt_loss:0.6353 reg_loss:0.9987 cost_time:876ms lr=1.9960e-03 total_loss:2.1833\n",
            "global_steps:537 epoch:1 steps:537/2885 cls_loss:0.6930 cnt_loss:0.6315 reg_loss:0.9999 cost_time:994ms lr=1.9960e-03 total_loss:2.3244\n",
            "global_steps:538 epoch:1 steps:538/2885 cls_loss:0.5916 cnt_loss:0.6320 reg_loss:0.9999 cost_time:880ms lr=1.9960e-03 total_loss:2.2235\n",
            "global_steps:539 epoch:1 steps:539/2885 cls_loss:0.6520 cnt_loss:0.6431 reg_loss:0.9997 cost_time:1111ms lr=1.9960e-03 total_loss:2.2948\n",
            "global_steps:540 epoch:1 steps:540/2885 cls_loss:0.5795 cnt_loss:0.6554 reg_loss:0.9993 cost_time:998ms lr=1.9960e-03 total_loss:2.2342\n",
            "global_steps:541 epoch:1 steps:541/2885 cls_loss:0.6093 cnt_loss:0.6215 reg_loss:0.9999 cost_time:992ms lr=1.9960e-03 total_loss:2.2306\n",
            "global_steps:542 epoch:1 steps:542/2885 cls_loss:0.5903 cnt_loss:0.6230 reg_loss:0.9999 cost_time:927ms lr=1.9960e-03 total_loss:2.2131\n",
            "global_steps:543 epoch:1 steps:543/2885 cls_loss:0.4491 cnt_loss:0.4783 reg_loss:0.7500 cost_time:1022ms lr=1.9960e-03 total_loss:1.6774\n",
            "global_steps:544 epoch:1 steps:544/2885 cls_loss:0.6125 cnt_loss:0.6236 reg_loss:0.9999 cost_time:945ms lr=1.9960e-03 total_loss:2.2360\n",
            "global_steps:545 epoch:1 steps:545/2885 cls_loss:0.5320 cnt_loss:0.6316 reg_loss:0.9999 cost_time:1079ms lr=1.9960e-03 total_loss:2.1634\n",
            "global_steps:546 epoch:1 steps:546/2885 cls_loss:0.6391 cnt_loss:0.6380 reg_loss:0.9999 cost_time:1111ms lr=1.9960e-03 total_loss:2.2770\n",
            "global_steps:547 epoch:1 steps:547/2885 cls_loss:0.8005 cnt_loss:0.6401 reg_loss:0.9996 cost_time:875ms lr=1.9960e-03 total_loss:2.4401\n",
            "global_steps:548 epoch:1 steps:548/2885 cls_loss:0.5699 cnt_loss:0.6355 reg_loss:1.0000 cost_time:972ms lr=1.9960e-03 total_loss:2.2054\n",
            "global_steps:549 epoch:1 steps:549/2885 cls_loss:0.5900 cnt_loss:0.6507 reg_loss:0.9998 cost_time:885ms lr=1.9960e-03 total_loss:2.2406\n",
            "global_steps:550 epoch:1 steps:550/2885 cls_loss:0.4900 cnt_loss:0.6324 reg_loss:1.0000 cost_time:861ms lr=1.9960e-03 total_loss:2.1224\n",
            "global_steps:551 epoch:1 steps:551/2885 cls_loss:0.5961 cnt_loss:0.6132 reg_loss:1.0000 cost_time:779ms lr=1.9960e-03 total_loss:2.2092\n",
            "global_steps:552 epoch:1 steps:552/2885 cls_loss:0.6498 cnt_loss:0.6336 reg_loss:0.9999 cost_time:1032ms lr=1.9960e-03 total_loss:2.2833\n",
            "global_steps:553 epoch:1 steps:553/2885 cls_loss:0.5173 cnt_loss:0.5984 reg_loss:1.0000 cost_time:858ms lr=1.9960e-03 total_loss:2.1157\n",
            "global_steps:554 epoch:1 steps:554/2885 cls_loss:0.6654 cnt_loss:0.6212 reg_loss:1.0000 cost_time:858ms lr=1.9960e-03 total_loss:2.2866\n",
            "global_steps:555 epoch:1 steps:555/2885 cls_loss:0.6912 cnt_loss:0.6121 reg_loss:1.0000 cost_time:940ms lr=1.9960e-03 total_loss:2.3033\n",
            "global_steps:556 epoch:1 steps:556/2885 cls_loss:0.6441 cnt_loss:0.5927 reg_loss:0.9999 cost_time:830ms lr=1.9960e-03 total_loss:2.2368\n",
            "global_steps:557 epoch:1 steps:557/2885 cls_loss:0.6298 cnt_loss:0.4756 reg_loss:0.7499 cost_time:784ms lr=1.9960e-03 total_loss:1.8553\n",
            "global_steps:558 epoch:1 steps:558/2885 cls_loss:0.5601 cnt_loss:0.6339 reg_loss:0.9999 cost_time:885ms lr=1.9960e-03 total_loss:2.1939\n",
            "global_steps:559 epoch:1 steps:559/2885 cls_loss:0.5143 cnt_loss:0.6170 reg_loss:1.0000 cost_time:943ms lr=1.9960e-03 total_loss:2.1313\n",
            "global_steps:560 epoch:1 steps:560/2885 cls_loss:0.4548 cnt_loss:0.6224 reg_loss:0.9984 cost_time:960ms lr=1.9960e-03 total_loss:2.0756\n",
            "global_steps:561 epoch:1 steps:561/2885 cls_loss:0.6486 cnt_loss:0.6306 reg_loss:0.9995 cost_time:1253ms lr=1.9960e-03 total_loss:2.2788\n",
            "global_steps:562 epoch:1 steps:562/2885 cls_loss:0.4332 cnt_loss:0.6188 reg_loss:1.0000 cost_time:1337ms lr=1.9960e-03 total_loss:2.0520\n",
            "global_steps:563 epoch:1 steps:563/2885 cls_loss:0.6360 cnt_loss:0.6158 reg_loss:0.9999 cost_time:906ms lr=1.9960e-03 total_loss:2.2518\n",
            "global_steps:564 epoch:1 steps:564/2885 cls_loss:0.5394 cnt_loss:0.6324 reg_loss:0.9999 cost_time:975ms lr=1.9960e-03 total_loss:2.1718\n",
            "global_steps:565 epoch:1 steps:565/2885 cls_loss:0.4798 cnt_loss:0.6308 reg_loss:1.0000 cost_time:978ms lr=1.9960e-03 total_loss:2.1106\n",
            "global_steps:566 epoch:1 steps:566/2885 cls_loss:0.4919 cnt_loss:0.6337 reg_loss:0.9996 cost_time:920ms lr=1.9960e-03 total_loss:2.1253\n",
            "global_steps:567 epoch:1 steps:567/2885 cls_loss:0.5727 cnt_loss:0.6259 reg_loss:0.9999 cost_time:859ms lr=1.9960e-03 total_loss:2.1985\n",
            "global_steps:568 epoch:1 steps:568/2885 cls_loss:0.5056 cnt_loss:0.6460 reg_loss:1.0000 cost_time:899ms lr=1.9960e-03 total_loss:2.1515\n",
            "global_steps:569 epoch:1 steps:569/2885 cls_loss:0.7214 cnt_loss:0.6197 reg_loss:0.9996 cost_time:978ms lr=1.9960e-03 total_loss:2.3407\n",
            "global_steps:570 epoch:1 steps:570/2885 cls_loss:0.5611 cnt_loss:0.6155 reg_loss:0.9999 cost_time:977ms lr=1.9960e-03 total_loss:2.1765\n",
            "global_steps:571 epoch:1 steps:571/2885 cls_loss:0.5413 cnt_loss:0.4689 reg_loss:0.7495 cost_time:1072ms lr=1.9960e-03 total_loss:1.7597\n",
            "global_steps:572 epoch:1 steps:572/2885 cls_loss:0.8371 cnt_loss:0.5947 reg_loss:0.9999 cost_time:980ms lr=1.9960e-03 total_loss:2.4317\n",
            "global_steps:573 epoch:1 steps:573/2885 cls_loss:0.8371 cnt_loss:0.6510 reg_loss:0.9998 cost_time:906ms lr=1.9960e-03 total_loss:2.4879\n",
            "global_steps:574 epoch:1 steps:574/2885 cls_loss:0.6162 cnt_loss:0.6395 reg_loss:0.9999 cost_time:858ms lr=1.9960e-03 total_loss:2.2556\n",
            "global_steps:575 epoch:1 steps:575/2885 cls_loss:0.7072 cnt_loss:0.6307 reg_loss:0.9998 cost_time:1142ms lr=1.9960e-03 total_loss:2.3377\n",
            "global_steps:576 epoch:1 steps:576/2885 cls_loss:0.5546 cnt_loss:0.6386 reg_loss:0.9997 cost_time:974ms lr=1.9960e-03 total_loss:2.1929\n",
            "global_steps:577 epoch:1 steps:577/2885 cls_loss:0.7289 cnt_loss:0.6354 reg_loss:0.9990 cost_time:1211ms lr=1.9960e-03 total_loss:2.3634\n",
            "global_steps:578 epoch:1 steps:578/2885 cls_loss:0.4949 cnt_loss:0.6368 reg_loss:0.9999 cost_time:778ms lr=1.9960e-03 total_loss:2.1317\n",
            "global_steps:579 epoch:1 steps:579/2885 cls_loss:0.7134 cnt_loss:0.6229 reg_loss:0.9997 cost_time:797ms lr=1.9960e-03 total_loss:2.3359\n",
            "global_steps:580 epoch:1 steps:580/2885 cls_loss:0.5233 cnt_loss:0.6004 reg_loss:1.0000 cost_time:788ms lr=1.9960e-03 total_loss:2.1236\n",
            "global_steps:581 epoch:1 steps:581/2885 cls_loss:0.6997 cnt_loss:0.6422 reg_loss:0.9992 cost_time:1246ms lr=1.9960e-03 total_loss:2.3411\n",
            "global_steps:582 epoch:1 steps:582/2885 cls_loss:0.6533 cnt_loss:0.6203 reg_loss:0.9997 cost_time:994ms lr=1.9960e-03 total_loss:2.2733\n",
            "global_steps:583 epoch:1 steps:583/2885 cls_loss:0.5448 cnt_loss:0.6184 reg_loss:0.9999 cost_time:981ms lr=1.9960e-03 total_loss:2.1632\n",
            "global_steps:584 epoch:1 steps:584/2885 cls_loss:0.6050 cnt_loss:0.4947 reg_loss:0.7499 cost_time:1343ms lr=1.9960e-03 total_loss:1.8497\n",
            "global_steps:585 epoch:1 steps:585/2885 cls_loss:0.4724 cnt_loss:0.6240 reg_loss:1.0000 cost_time:993ms lr=1.9960e-03 total_loss:2.0964\n",
            "global_steps:586 epoch:1 steps:586/2885 cls_loss:0.6624 cnt_loss:0.6334 reg_loss:0.9998 cost_time:863ms lr=1.9960e-03 total_loss:2.2956\n",
            "global_steps:587 epoch:1 steps:587/2885 cls_loss:0.6359 cnt_loss:0.6351 reg_loss:0.9998 cost_time:872ms lr=1.9960e-03 total_loss:2.2708\n",
            "global_steps:588 epoch:1 steps:588/2885 cls_loss:0.6856 cnt_loss:0.6183 reg_loss:0.9997 cost_time:951ms lr=1.9960e-03 total_loss:2.3036\n",
            "global_steps:589 epoch:1 steps:589/2885 cls_loss:0.5662 cnt_loss:0.6202 reg_loss:1.0000 cost_time:920ms lr=1.9960e-03 total_loss:2.1864\n",
            "global_steps:590 epoch:1 steps:590/2885 cls_loss:0.6442 cnt_loss:0.6510 reg_loss:0.9997 cost_time:777ms lr=1.9960e-03 total_loss:2.2949\n",
            "global_steps:591 epoch:1 steps:591/2885 cls_loss:0.6943 cnt_loss:0.6333 reg_loss:0.9999 cost_time:1022ms lr=1.9960e-03 total_loss:2.3275\n",
            "global_steps:592 epoch:1 steps:592/2885 cls_loss:0.5379 cnt_loss:0.6131 reg_loss:0.9999 cost_time:953ms lr=1.9960e-03 total_loss:2.1509\n",
            "global_steps:593 epoch:1 steps:593/2885 cls_loss:0.5778 cnt_loss:0.6084 reg_loss:0.9998 cost_time:782ms lr=1.9960e-03 total_loss:2.1860\n",
            "global_steps:594 epoch:1 steps:594/2885 cls_loss:0.6845 cnt_loss:0.6121 reg_loss:0.9999 cost_time:1234ms lr=1.9960e-03 total_loss:2.2965\n",
            "global_steps:595 epoch:1 steps:595/2885 cls_loss:0.5127 cnt_loss:0.6337 reg_loss:0.9999 cost_time:995ms lr=1.9960e-03 total_loss:2.1463\n",
            "global_steps:596 epoch:1 steps:596/2885 cls_loss:0.5099 cnt_loss:0.6259 reg_loss:0.9995 cost_time:1115ms lr=1.9960e-03 total_loss:2.1353\n",
            "global_steps:597 epoch:1 steps:597/2885 cls_loss:0.5985 cnt_loss:0.6281 reg_loss:0.9992 cost_time:1007ms lr=1.9960e-03 total_loss:2.2258\n",
            "global_steps:598 epoch:1 steps:598/2885 cls_loss:0.7355 cnt_loss:0.6668 reg_loss:0.9996 cost_time:1255ms lr=1.9960e-03 total_loss:2.4019\n",
            "global_steps:599 epoch:1 steps:599/2885 cls_loss:0.6433 cnt_loss:0.6086 reg_loss:0.9999 cost_time:1107ms lr=1.9960e-03 total_loss:2.2518\n",
            "global_steps:600 epoch:1 steps:600/2885 cls_loss:0.4504 cnt_loss:0.6586 reg_loss:0.9999 cost_time:895ms lr=1.9960e-03 total_loss:2.1089\n",
            "global_steps:601 epoch:1 steps:601/2885 cls_loss:0.5588 cnt_loss:0.6284 reg_loss:0.9998 cost_time:1124ms lr=1.9960e-03 total_loss:2.1870\n",
            "global_steps:602 epoch:1 steps:602/2885 cls_loss:0.4663 cnt_loss:0.6435 reg_loss:0.9998 cost_time:908ms lr=1.9960e-03 total_loss:2.1096\n",
            "global_steps:603 epoch:1 steps:603/2885 cls_loss:0.6749 cnt_loss:0.6408 reg_loss:0.9997 cost_time:967ms lr=1.9960e-03 total_loss:2.3154\n",
            "global_steps:604 epoch:1 steps:604/2885 cls_loss:0.6775 cnt_loss:0.6199 reg_loss:0.9990 cost_time:787ms lr=1.9960e-03 total_loss:2.2965\n",
            "global_steps:605 epoch:1 steps:605/2885 cls_loss:0.4158 cnt_loss:0.6286 reg_loss:0.9997 cost_time:1080ms lr=1.9960e-03 total_loss:2.0441\n",
            "global_steps:606 epoch:1 steps:606/2885 cls_loss:0.6860 cnt_loss:0.6389 reg_loss:0.9994 cost_time:861ms lr=1.9960e-03 total_loss:2.3244\n",
            "global_steps:607 epoch:1 steps:607/2885 cls_loss:0.5893 cnt_loss:0.6170 reg_loss:0.9999 cost_time:786ms lr=1.9960e-03 total_loss:2.2062\n",
            "global_steps:608 epoch:1 steps:608/2885 cls_loss:0.5553 cnt_loss:0.6233 reg_loss:0.9993 cost_time:889ms lr=1.9960e-03 total_loss:2.1778\n",
            "global_steps:609 epoch:1 steps:609/2885 cls_loss:0.5432 cnt_loss:0.6277 reg_loss:0.9997 cost_time:1123ms lr=1.9960e-03 total_loss:2.1707\n",
            "global_steps:610 epoch:1 steps:610/2885 cls_loss:0.4358 cnt_loss:0.6360 reg_loss:0.9995 cost_time:862ms lr=1.9960e-03 total_loss:2.0713\n",
            "global_steps:611 epoch:1 steps:611/2885 cls_loss:0.6122 cnt_loss:0.6569 reg_loss:0.9999 cost_time:877ms lr=1.9960e-03 total_loss:2.2689\n",
            "global_steps:612 epoch:1 steps:612/2885 cls_loss:0.5614 cnt_loss:0.6109 reg_loss:0.9999 cost_time:950ms lr=1.9960e-03 total_loss:2.1721\n",
            "global_steps:613 epoch:1 steps:613/2885 cls_loss:0.5997 cnt_loss:0.6254 reg_loss:0.9998 cost_time:872ms lr=1.9960e-03 total_loss:2.2249\n",
            "global_steps:614 epoch:1 steps:614/2885 cls_loss:0.6247 cnt_loss:0.6536 reg_loss:0.9998 cost_time:1348ms lr=1.9960e-03 total_loss:2.2780\n",
            "global_steps:615 epoch:1 steps:615/2885 cls_loss:0.5476 cnt_loss:0.6319 reg_loss:0.9994 cost_time:1115ms lr=1.9960e-03 total_loss:2.1789\n",
            "global_steps:616 epoch:1 steps:616/2885 cls_loss:0.4287 cnt_loss:0.6194 reg_loss:0.9999 cost_time:893ms lr=1.9960e-03 total_loss:2.0480\n",
            "global_steps:617 epoch:1 steps:617/2885 cls_loss:0.5989 cnt_loss:0.6327 reg_loss:0.9989 cost_time:1084ms lr=1.9960e-03 total_loss:2.2305\n",
            "global_steps:618 epoch:1 steps:618/2885 cls_loss:0.6147 cnt_loss:0.6472 reg_loss:0.9989 cost_time:1114ms lr=1.9960e-03 total_loss:2.2608\n",
            "global_steps:619 epoch:1 steps:619/2885 cls_loss:0.5632 cnt_loss:0.6267 reg_loss:0.9999 cost_time:1223ms lr=1.9960e-03 total_loss:2.1898\n",
            "global_steps:620 epoch:1 steps:620/2885 cls_loss:0.5261 cnt_loss:0.6405 reg_loss:0.9997 cost_time:1197ms lr=1.9960e-03 total_loss:2.1662\n",
            "global_steps:621 epoch:1 steps:621/2885 cls_loss:0.4513 cnt_loss:0.6384 reg_loss:0.9999 cost_time:919ms lr=1.9960e-03 total_loss:2.0896\n",
            "global_steps:622 epoch:1 steps:622/2885 cls_loss:0.4501 cnt_loss:0.4574 reg_loss:0.7498 cost_time:860ms lr=1.9960e-03 total_loss:1.6573\n",
            "global_steps:623 epoch:1 steps:623/2885 cls_loss:0.5727 cnt_loss:0.6187 reg_loss:0.9998 cost_time:702ms lr=1.9960e-03 total_loss:2.1912\n",
            "global_steps:624 epoch:1 steps:624/2885 cls_loss:0.5833 cnt_loss:0.6055 reg_loss:0.9998 cost_time:810ms lr=1.9960e-03 total_loss:2.1886\n",
            "global_steps:625 epoch:1 steps:625/2885 cls_loss:1.0342 cnt_loss:0.4686 reg_loss:0.7499 cost_time:916ms lr=1.9960e-03 total_loss:2.2527\n",
            "global_steps:626 epoch:1 steps:626/2885 cls_loss:0.5218 cnt_loss:0.6534 reg_loss:0.9998 cost_time:989ms lr=1.9960e-03 total_loss:2.1750\n",
            "global_steps:627 epoch:1 steps:627/2885 cls_loss:0.6703 cnt_loss:0.6513 reg_loss:0.9995 cost_time:862ms lr=1.9960e-03 total_loss:2.3210\n",
            "global_steps:628 epoch:1 steps:628/2885 cls_loss:0.8562 cnt_loss:0.6022 reg_loss:0.9999 cost_time:864ms lr=1.9960e-03 total_loss:2.4583\n",
            "global_steps:629 epoch:1 steps:629/2885 cls_loss:0.8637 cnt_loss:0.6441 reg_loss:0.9990 cost_time:872ms lr=1.9960e-03 total_loss:2.5069\n",
            "global_steps:630 epoch:1 steps:630/2885 cls_loss:0.8857 cnt_loss:0.6164 reg_loss:0.9999 cost_time:1113ms lr=1.9960e-03 total_loss:2.5019\n",
            "global_steps:631 epoch:1 steps:631/2885 cls_loss:0.8048 cnt_loss:0.6178 reg_loss:0.9999 cost_time:769ms lr=1.9960e-03 total_loss:2.4225\n",
            "global_steps:632 epoch:1 steps:632/2885 cls_loss:0.6403 cnt_loss:0.6208 reg_loss:0.9999 cost_time:1107ms lr=1.9960e-03 total_loss:2.2610\n",
            "global_steps:633 epoch:1 steps:633/2885 cls_loss:0.9268 cnt_loss:0.6084 reg_loss:0.9995 cost_time:1002ms lr=1.9960e-03 total_loss:2.5347\n",
            "global_steps:634 epoch:1 steps:634/2885 cls_loss:0.7351 cnt_loss:0.4624 reg_loss:0.7500 cost_time:865ms lr=1.9960e-03 total_loss:1.9475\n",
            "global_steps:635 epoch:1 steps:635/2885 cls_loss:0.6310 cnt_loss:0.6068 reg_loss:0.9999 cost_time:1022ms lr=1.9960e-03 total_loss:2.2378\n",
            "global_steps:636 epoch:1 steps:636/2885 cls_loss:0.7333 cnt_loss:0.6637 reg_loss:0.9997 cost_time:983ms lr=1.9960e-03 total_loss:2.3968\n",
            "global_steps:637 epoch:1 steps:637/2885 cls_loss:0.7927 cnt_loss:0.6353 reg_loss:0.9997 cost_time:815ms lr=1.9960e-03 total_loss:2.4277\n",
            "global_steps:638 epoch:1 steps:638/2885 cls_loss:0.8901 cnt_loss:0.6068 reg_loss:0.9996 cost_time:895ms lr=1.9960e-03 total_loss:2.4965\n",
            "global_steps:639 epoch:1 steps:639/2885 cls_loss:0.5974 cnt_loss:0.6098 reg_loss:0.9998 cost_time:780ms lr=1.9960e-03 total_loss:2.2070\n",
            "global_steps:640 epoch:1 steps:640/2885 cls_loss:0.7082 cnt_loss:0.6004 reg_loss:0.9994 cost_time:992ms lr=1.9960e-03 total_loss:2.3080\n",
            "global_steps:641 epoch:1 steps:641/2885 cls_loss:0.8161 cnt_loss:0.6328 reg_loss:0.9999 cost_time:1101ms lr=1.9960e-03 total_loss:2.4488\n",
            "global_steps:642 epoch:1 steps:642/2885 cls_loss:0.7327 cnt_loss:0.6351 reg_loss:0.9999 cost_time:717ms lr=1.9960e-03 total_loss:2.3677\n",
            "global_steps:643 epoch:1 steps:643/2885 cls_loss:0.6234 cnt_loss:0.6086 reg_loss:0.9999 cost_time:1252ms lr=1.9960e-03 total_loss:2.2319\n",
            "global_steps:644 epoch:1 steps:644/2885 cls_loss:0.6930 cnt_loss:0.6102 reg_loss:1.0000 cost_time:951ms lr=1.9960e-03 total_loss:2.3032\n",
            "global_steps:645 epoch:1 steps:645/2885 cls_loss:0.5348 cnt_loss:0.4892 reg_loss:0.7499 cost_time:1299ms lr=1.9960e-03 total_loss:1.7739\n",
            "global_steps:646 epoch:1 steps:646/2885 cls_loss:0.7316 cnt_loss:0.6304 reg_loss:0.9999 cost_time:858ms lr=1.9960e-03 total_loss:2.3620\n",
            "global_steps:647 epoch:1 steps:647/2885 cls_loss:0.5603 cnt_loss:0.6197 reg_loss:0.9999 cost_time:858ms lr=1.9960e-03 total_loss:2.1799\n",
            "global_steps:648 epoch:1 steps:648/2885 cls_loss:0.6590 cnt_loss:0.6238 reg_loss:0.9995 cost_time:916ms lr=1.9960e-03 total_loss:2.2823\n",
            "global_steps:649 epoch:1 steps:649/2885 cls_loss:0.7996 cnt_loss:0.6387 reg_loss:0.9995 cost_time:900ms lr=1.9960e-03 total_loss:2.4378\n",
            "global_steps:650 epoch:1 steps:650/2885 cls_loss:0.6713 cnt_loss:0.6016 reg_loss:0.9988 cost_time:926ms lr=1.9960e-03 total_loss:2.2717\n",
            "global_steps:651 epoch:1 steps:651/2885 cls_loss:0.6874 cnt_loss:0.6034 reg_loss:0.9999 cost_time:1057ms lr=1.9960e-03 total_loss:2.2907\n",
            "global_steps:652 epoch:1 steps:652/2885 cls_loss:0.5468 cnt_loss:0.5950 reg_loss:0.9999 cost_time:1101ms lr=1.9960e-03 total_loss:2.1418\n",
            "global_steps:653 epoch:1 steps:653/2885 cls_loss:0.7667 cnt_loss:0.6366 reg_loss:0.9998 cost_time:1102ms lr=1.9960e-03 total_loss:2.4031\n",
            "global_steps:654 epoch:1 steps:654/2885 cls_loss:0.7236 cnt_loss:0.6580 reg_loss:0.9997 cost_time:781ms lr=1.9960e-03 total_loss:2.3814\n",
            "global_steps:655 epoch:1 steps:655/2885 cls_loss:0.6443 cnt_loss:0.6504 reg_loss:0.9997 cost_time:1065ms lr=1.9960e-03 total_loss:2.2944\n",
            "global_steps:656 epoch:1 steps:656/2885 cls_loss:0.5190 cnt_loss:0.4891 reg_loss:0.7499 cost_time:800ms lr=1.9960e-03 total_loss:1.7580\n",
            "global_steps:657 epoch:1 steps:657/2885 cls_loss:0.5816 cnt_loss:0.6233 reg_loss:1.0000 cost_time:922ms lr=1.9960e-03 total_loss:2.2048\n",
            "global_steps:658 epoch:1 steps:658/2885 cls_loss:0.7112 cnt_loss:0.6016 reg_loss:0.9999 cost_time:1093ms lr=1.9960e-03 total_loss:2.3128\n",
            "global_steps:659 epoch:1 steps:659/2885 cls_loss:0.5200 cnt_loss:0.6152 reg_loss:0.9999 cost_time:1095ms lr=1.9960e-03 total_loss:2.1351\n",
            "global_steps:660 epoch:1 steps:660/2885 cls_loss:0.6848 cnt_loss:0.6143 reg_loss:0.9996 cost_time:1113ms lr=1.9960e-03 total_loss:2.2986\n",
            "global_steps:661 epoch:1 steps:661/2885 cls_loss:0.6958 cnt_loss:0.6116 reg_loss:0.9999 cost_time:766ms lr=1.9960e-03 total_loss:2.3073\n",
            "global_steps:662 epoch:1 steps:662/2885 cls_loss:0.5542 cnt_loss:0.6568 reg_loss:0.9994 cost_time:803ms lr=1.9960e-03 total_loss:2.2104\n",
            "global_steps:663 epoch:1 steps:663/2885 cls_loss:0.6870 cnt_loss:0.6495 reg_loss:0.9998 cost_time:866ms lr=1.9960e-03 total_loss:2.3362\n",
            "global_steps:664 epoch:1 steps:664/2885 cls_loss:0.9114 cnt_loss:0.6402 reg_loss:0.9993 cost_time:876ms lr=1.9960e-03 total_loss:2.5510\n",
            "global_steps:665 epoch:1 steps:665/2885 cls_loss:0.6039 cnt_loss:0.4775 reg_loss:0.7499 cost_time:965ms lr=1.9960e-03 total_loss:1.8313\n",
            "global_steps:666 epoch:1 steps:666/2885 cls_loss:0.5998 cnt_loss:0.6357 reg_loss:0.9999 cost_time:1126ms lr=1.9960e-03 total_loss:2.2354\n",
            "global_steps:667 epoch:1 steps:667/2885 cls_loss:0.5251 cnt_loss:0.6321 reg_loss:0.9997 cost_time:770ms lr=1.9960e-03 total_loss:2.1569\n",
            "global_steps:668 epoch:1 steps:668/2885 cls_loss:0.6524 cnt_loss:0.6450 reg_loss:0.9999 cost_time:950ms lr=1.9960e-03 total_loss:2.2974\n",
            "global_steps:669 epoch:1 steps:669/2885 cls_loss:0.4551 cnt_loss:0.6393 reg_loss:0.9993 cost_time:983ms lr=1.9960e-03 total_loss:2.0937\n",
            "global_steps:670 epoch:1 steps:670/2885 cls_loss:0.4856 cnt_loss:0.6603 reg_loss:0.9997 cost_time:1150ms lr=1.9960e-03 total_loss:2.1455\n",
            "global_steps:671 epoch:1 steps:671/2885 cls_loss:0.5359 cnt_loss:0.6301 reg_loss:0.9999 cost_time:854ms lr=1.9960e-03 total_loss:2.1658\n",
            "global_steps:672 epoch:1 steps:672/2885 cls_loss:0.7011 cnt_loss:0.6102 reg_loss:0.9975 cost_time:987ms lr=1.9960e-03 total_loss:2.3087\n",
            "global_steps:673 epoch:1 steps:673/2885 cls_loss:0.5634 cnt_loss:0.6246 reg_loss:0.9998 cost_time:975ms lr=1.9960e-03 total_loss:2.1878\n",
            "global_steps:674 epoch:1 steps:674/2885 cls_loss:0.6996 cnt_loss:0.6505 reg_loss:0.9938 cost_time:903ms lr=1.9960e-03 total_loss:2.3439\n",
            "global_steps:675 epoch:1 steps:675/2885 cls_loss:0.6155 cnt_loss:0.6473 reg_loss:0.9991 cost_time:985ms lr=1.9960e-03 total_loss:2.2620\n",
            "global_steps:676 epoch:1 steps:676/2885 cls_loss:0.5493 cnt_loss:0.4709 reg_loss:0.7499 cost_time:1071ms lr=1.9960e-03 total_loss:1.7701\n",
            "global_steps:677 epoch:1 steps:677/2885 cls_loss:0.5816 cnt_loss:0.6363 reg_loss:0.9998 cost_time:1092ms lr=1.9960e-03 total_loss:2.2178\n",
            "global_steps:678 epoch:1 steps:678/2885 cls_loss:0.5927 cnt_loss:0.6488 reg_loss:0.9999 cost_time:859ms lr=1.9960e-03 total_loss:2.2413\n",
            "global_steps:679 epoch:1 steps:679/2885 cls_loss:0.6253 cnt_loss:0.6631 reg_loss:0.9994 cost_time:901ms lr=1.9960e-03 total_loss:2.2877\n",
            "global_steps:680 epoch:1 steps:680/2885 cls_loss:0.5871 cnt_loss:0.6306 reg_loss:0.9999 cost_time:1138ms lr=1.9960e-03 total_loss:2.2175\n",
            "global_steps:681 epoch:1 steps:681/2885 cls_loss:0.7002 cnt_loss:0.6339 reg_loss:0.9978 cost_time:1234ms lr=1.9960e-03 total_loss:2.3319\n",
            "global_steps:682 epoch:1 steps:682/2885 cls_loss:0.7417 cnt_loss:0.6447 reg_loss:0.9978 cost_time:1116ms lr=1.9960e-03 total_loss:2.3843\n",
            "global_steps:683 epoch:1 steps:683/2885 cls_loss:0.6265 cnt_loss:0.6746 reg_loss:0.9998 cost_time:920ms lr=1.9960e-03 total_loss:2.3009\n",
            "global_steps:684 epoch:1 steps:684/2885 cls_loss:0.5100 cnt_loss:0.6517 reg_loss:0.9987 cost_time:1110ms lr=1.9960e-03 total_loss:2.1604\n",
            "global_steps:685 epoch:1 steps:685/2885 cls_loss:0.5892 cnt_loss:0.6750 reg_loss:0.9999 cost_time:848ms lr=1.9960e-03 total_loss:2.2641\n",
            "global_steps:686 epoch:1 steps:686/2885 cls_loss:0.4881 cnt_loss:0.6231 reg_loss:0.9998 cost_time:914ms lr=1.9960e-03 total_loss:2.1111\n",
            "global_steps:687 epoch:1 steps:687/2885 cls_loss:0.4092 cnt_loss:0.4747 reg_loss:0.7457 cost_time:852ms lr=1.9960e-03 total_loss:1.6296\n",
            "global_steps:688 epoch:1 steps:688/2885 cls_loss:0.6335 cnt_loss:0.6324 reg_loss:0.9987 cost_time:864ms lr=1.9960e-03 total_loss:2.2645\n",
            "global_steps:689 epoch:1 steps:689/2885 cls_loss:0.5289 cnt_loss:0.6176 reg_loss:0.9997 cost_time:978ms lr=1.9960e-03 total_loss:2.1463\n",
            "global_steps:690 epoch:1 steps:690/2885 cls_loss:0.5240 cnt_loss:0.6400 reg_loss:0.9999 cost_time:1028ms lr=1.9960e-03 total_loss:2.1639\n",
            "global_steps:691 epoch:1 steps:691/2885 cls_loss:0.6001 cnt_loss:0.6092 reg_loss:0.9994 cost_time:1110ms lr=1.9960e-03 total_loss:2.2087\n",
            "global_steps:692 epoch:1 steps:692/2885 cls_loss:0.6947 cnt_loss:0.6224 reg_loss:0.9985 cost_time:1247ms lr=1.9960e-03 total_loss:2.3156\n",
            "global_steps:693 epoch:1 steps:693/2885 cls_loss:0.5674 cnt_loss:0.6159 reg_loss:0.9998 cost_time:861ms lr=1.9960e-03 total_loss:2.1831\n",
            "global_steps:694 epoch:1 steps:694/2885 cls_loss:0.6410 cnt_loss:0.6382 reg_loss:0.9927 cost_time:1002ms lr=1.9960e-03 total_loss:2.2720\n",
            "global_steps:695 epoch:1 steps:695/2885 cls_loss:0.6188 cnt_loss:0.6410 reg_loss:0.9994 cost_time:928ms lr=1.9960e-03 total_loss:2.2592\n",
            "global_steps:696 epoch:1 steps:696/2885 cls_loss:0.6632 cnt_loss:0.6400 reg_loss:0.9959 cost_time:950ms lr=1.9960e-03 total_loss:2.2991\n",
            "global_steps:697 epoch:1 steps:697/2885 cls_loss:0.4816 cnt_loss:0.6444 reg_loss:0.9964 cost_time:822ms lr=1.9960e-03 total_loss:2.1224\n",
            "global_steps:698 epoch:1 steps:698/2885 cls_loss:0.6596 cnt_loss:0.6221 reg_loss:0.9983 cost_time:820ms lr=1.9960e-03 total_loss:2.2800\n",
            "global_steps:699 epoch:1 steps:699/2885 cls_loss:0.7148 cnt_loss:0.4659 reg_loss:0.7497 cost_time:807ms lr=1.9960e-03 total_loss:1.9303\n",
            "global_steps:700 epoch:1 steps:700/2885 cls_loss:0.5644 cnt_loss:0.6600 reg_loss:0.9981 cost_time:806ms lr=1.9960e-03 total_loss:2.2224\n",
            "global_steps:701 epoch:1 steps:701/2885 cls_loss:0.5610 cnt_loss:0.6364 reg_loss:0.9917 cost_time:1229ms lr=1.9960e-03 total_loss:2.1891\n",
            "global_steps:702 epoch:1 steps:702/2885 cls_loss:0.9056 cnt_loss:0.6385 reg_loss:0.9842 cost_time:988ms lr=1.9960e-03 total_loss:2.5283\n",
            "global_steps:703 epoch:1 steps:703/2885 cls_loss:0.3951 cnt_loss:0.4589 reg_loss:0.7498 cost_time:950ms lr=1.9960e-03 total_loss:1.6039\n",
            "global_steps:704 epoch:1 steps:704/2885 cls_loss:0.7803 cnt_loss:0.6376 reg_loss:0.9854 cost_time:1121ms lr=1.9960e-03 total_loss:2.4033\n",
            "global_steps:705 epoch:1 steps:705/2885 cls_loss:0.8178 cnt_loss:0.6541 reg_loss:0.9956 cost_time:959ms lr=1.9960e-03 total_loss:2.4674\n",
            "global_steps:706 epoch:1 steps:706/2885 cls_loss:0.8746 cnt_loss:0.6202 reg_loss:0.9843 cost_time:880ms lr=1.9960e-03 total_loss:2.4791\n",
            "global_steps:707 epoch:1 steps:707/2885 cls_loss:0.6708 cnt_loss:0.6301 reg_loss:0.9980 cost_time:1242ms lr=1.9960e-03 total_loss:2.2989\n",
            "global_steps:708 epoch:1 steps:708/2885 cls_loss:0.8222 cnt_loss:0.6450 reg_loss:0.9959 cost_time:955ms lr=1.9960e-03 total_loss:2.4631\n",
            "global_steps:709 epoch:1 steps:709/2885 cls_loss:0.5856 cnt_loss:0.6292 reg_loss:0.9988 cost_time:788ms lr=1.9960e-03 total_loss:2.2136\n",
            "global_steps:710 epoch:1 steps:710/2885 cls_loss:0.7269 cnt_loss:0.6468 reg_loss:0.9975 cost_time:1102ms lr=1.9960e-03 total_loss:2.3711\n",
            "global_steps:711 epoch:1 steps:711/2885 cls_loss:0.7066 cnt_loss:0.6258 reg_loss:0.9958 cost_time:1029ms lr=1.9960e-03 total_loss:2.3283\n",
            "global_steps:712 epoch:1 steps:712/2885 cls_loss:0.5838 cnt_loss:0.6432 reg_loss:0.9994 cost_time:941ms lr=1.9960e-03 total_loss:2.2263\n",
            "global_steps:713 epoch:1 steps:713/2885 cls_loss:0.5524 cnt_loss:0.6217 reg_loss:0.9857 cost_time:865ms lr=1.9960e-03 total_loss:2.1598\n",
            "global_steps:714 epoch:1 steps:714/2885 cls_loss:0.7429 cnt_loss:0.6235 reg_loss:0.9889 cost_time:785ms lr=1.9960e-03 total_loss:2.3553\n",
            "global_steps:715 epoch:1 steps:715/2885 cls_loss:0.5541 cnt_loss:0.6208 reg_loss:0.9932 cost_time:871ms lr=1.9960e-03 total_loss:2.1681\n",
            "global_steps:716 epoch:1 steps:716/2885 cls_loss:0.4826 cnt_loss:0.6383 reg_loss:0.9978 cost_time:975ms lr=1.9960e-03 total_loss:2.1186\n",
            "global_steps:717 epoch:1 steps:717/2885 cls_loss:0.5392 cnt_loss:0.6338 reg_loss:0.9927 cost_time:854ms lr=1.9960e-03 total_loss:2.1656\n",
            "global_steps:718 epoch:1 steps:718/2885 cls_loss:0.7227 cnt_loss:0.6537 reg_loss:0.9782 cost_time:1001ms lr=1.9960e-03 total_loss:2.3546\n",
            "global_steps:719 epoch:1 steps:719/2885 cls_loss:0.8266 cnt_loss:0.4651 reg_loss:0.7453 cost_time:920ms lr=1.9960e-03 total_loss:2.0369\n",
            "global_steps:720 epoch:1 steps:720/2885 cls_loss:0.5241 cnt_loss:0.6313 reg_loss:0.9930 cost_time:917ms lr=1.9960e-03 total_loss:2.1485\n",
            "global_steps:721 epoch:1 steps:721/2885 cls_loss:0.5989 cnt_loss:0.6399 reg_loss:0.9885 cost_time:1068ms lr=1.9960e-03 total_loss:2.2274\n",
            "global_steps:722 epoch:1 steps:722/2885 cls_loss:0.5998 cnt_loss:0.6283 reg_loss:0.9825 cost_time:1093ms lr=1.9960e-03 total_loss:2.2106\n",
            "global_steps:723 epoch:1 steps:723/2885 cls_loss:0.5890 cnt_loss:0.6453 reg_loss:0.9998 cost_time:952ms lr=1.9960e-03 total_loss:2.2342\n",
            "global_steps:724 epoch:1 steps:724/2885 cls_loss:0.5508 cnt_loss:0.6622 reg_loss:0.9902 cost_time:769ms lr=1.9960e-03 total_loss:2.2031\n",
            "global_steps:725 epoch:1 steps:725/2885 cls_loss:0.5338 cnt_loss:0.6339 reg_loss:0.9748 cost_time:978ms lr=1.9960e-03 total_loss:2.1425\n",
            "global_steps:726 epoch:1 steps:726/2885 cls_loss:0.6244 cnt_loss:0.6193 reg_loss:0.9708 cost_time:874ms lr=1.9960e-03 total_loss:2.2145\n",
            "global_steps:727 epoch:1 steps:727/2885 cls_loss:0.8804 cnt_loss:0.4735 reg_loss:0.7430 cost_time:1104ms lr=1.9960e-03 total_loss:2.0968\n",
            "global_steps:728 epoch:1 steps:728/2885 cls_loss:0.6178 cnt_loss:0.6461 reg_loss:0.9600 cost_time:851ms lr=1.9960e-03 total_loss:2.2239\n",
            "global_steps:729 epoch:1 steps:729/2885 cls_loss:0.4967 cnt_loss:0.6353 reg_loss:0.9626 cost_time:870ms lr=1.9960e-03 total_loss:2.0947\n",
            "global_steps:730 epoch:1 steps:730/2885 cls_loss:0.5209 cnt_loss:0.6102 reg_loss:0.9602 cost_time:841ms lr=1.9960e-03 total_loss:2.0913\n",
            "global_steps:731 epoch:1 steps:731/2885 cls_loss:0.7467 cnt_loss:0.6318 reg_loss:1.0157 cost_time:860ms lr=1.9960e-03 total_loss:2.3942\n",
            "global_steps:732 epoch:1 steps:732/2885 cls_loss:0.6410 cnt_loss:0.4981 reg_loss:0.7998 cost_time:856ms lr=1.9960e-03 total_loss:1.9388\n",
            "global_steps:733 epoch:1 steps:733/2885 cls_loss:0.4318 cnt_loss:0.6178 reg_loss:0.9968 cost_time:765ms lr=1.9960e-03 total_loss:2.0463\n",
            "global_steps:734 epoch:1 steps:734/2885 cls_loss:0.5632 cnt_loss:0.6362 reg_loss:1.0108 cost_time:1279ms lr=1.9960e-03 total_loss:2.2103\n",
            "global_steps:735 epoch:1 steps:735/2885 cls_loss:0.5940 cnt_loss:0.6274 reg_loss:0.9639 cost_time:872ms lr=1.9960e-03 total_loss:2.1853\n",
            "global_steps:736 epoch:1 steps:736/2885 cls_loss:0.7180 cnt_loss:0.6352 reg_loss:0.9698 cost_time:1216ms lr=1.9960e-03 total_loss:2.3230\n",
            "global_steps:737 epoch:1 steps:737/2885 cls_loss:0.6145 cnt_loss:0.6452 reg_loss:0.9642 cost_time:863ms lr=1.9960e-03 total_loss:2.2239\n",
            "global_steps:738 epoch:1 steps:738/2885 cls_loss:0.5850 cnt_loss:0.6184 reg_loss:0.9710 cost_time:986ms lr=1.9960e-03 total_loss:2.1745\n",
            "global_steps:739 epoch:1 steps:739/2885 cls_loss:0.5777 cnt_loss:0.6087 reg_loss:0.9781 cost_time:1110ms lr=1.9960e-03 total_loss:2.1646\n",
            "global_steps:740 epoch:1 steps:740/2885 cls_loss:0.6453 cnt_loss:0.6544 reg_loss:0.9577 cost_time:873ms lr=1.9960e-03 total_loss:2.2574\n",
            "global_steps:741 epoch:1 steps:741/2885 cls_loss:0.7866 cnt_loss:0.6340 reg_loss:0.8883 cost_time:1105ms lr=1.9960e-03 total_loss:2.3089\n",
            "global_steps:742 epoch:1 steps:742/2885 cls_loss:0.6385 cnt_loss:0.6509 reg_loss:0.9193 cost_time:943ms lr=1.9960e-03 total_loss:2.2087\n",
            "global_steps:743 epoch:1 steps:743/2885 cls_loss:0.5765 cnt_loss:0.6284 reg_loss:0.9856 cost_time:1038ms lr=1.9960e-03 total_loss:2.1905\n",
            "global_steps:744 epoch:1 steps:744/2885 cls_loss:0.6694 cnt_loss:0.6240 reg_loss:0.9378 cost_time:1108ms lr=1.9960e-03 total_loss:2.2312\n",
            "global_steps:745 epoch:1 steps:745/2885 cls_loss:0.5936 cnt_loss:0.6455 reg_loss:0.9912 cost_time:855ms lr=1.9960e-03 total_loss:2.2303\n",
            "global_steps:746 epoch:1 steps:746/2885 cls_loss:0.5780 cnt_loss:0.6451 reg_loss:1.0067 cost_time:1223ms lr=1.9960e-03 total_loss:2.2299\n",
            "global_steps:747 epoch:1 steps:747/2885 cls_loss:0.5402 cnt_loss:0.6250 reg_loss:1.0165 cost_time:926ms lr=1.9960e-03 total_loss:2.1817\n",
            "global_steps:748 epoch:1 steps:748/2885 cls_loss:0.6548 cnt_loss:0.6079 reg_loss:0.9975 cost_time:1035ms lr=1.9960e-03 total_loss:2.2602\n",
            "global_steps:749 epoch:1 steps:749/2885 cls_loss:0.4887 cnt_loss:0.6013 reg_loss:0.9410 cost_time:973ms lr=1.9960e-03 total_loss:2.0311\n",
            "global_steps:750 epoch:1 steps:750/2885 cls_loss:0.6470 cnt_loss:0.6477 reg_loss:0.9369 cost_time:858ms lr=1.9960e-03 total_loss:2.2316\n",
            "global_steps:751 epoch:1 steps:751/2885 cls_loss:0.4658 cnt_loss:0.6538 reg_loss:0.9659 cost_time:999ms lr=1.9960e-03 total_loss:2.0855\n",
            "global_steps:752 epoch:1 steps:752/2885 cls_loss:0.4635 cnt_loss:0.6325 reg_loss:0.8392 cost_time:1009ms lr=1.9960e-03 total_loss:1.9352\n",
            "global_steps:753 epoch:1 steps:753/2885 cls_loss:0.5915 cnt_loss:0.6622 reg_loss:0.8519 cost_time:1056ms lr=1.9960e-03 total_loss:2.1056\n",
            "global_steps:754 epoch:1 steps:754/2885 cls_loss:0.5052 cnt_loss:0.6614 reg_loss:0.8436 cost_time:915ms lr=1.9960e-03 total_loss:2.0102\n",
            "global_steps:755 epoch:1 steps:755/2885 cls_loss:0.5632 cnt_loss:0.6241 reg_loss:0.8262 cost_time:1223ms lr=1.9960e-03 total_loss:2.0135\n",
            "global_steps:756 epoch:1 steps:756/2885 cls_loss:0.5579 cnt_loss:0.6254 reg_loss:0.6725 cost_time:854ms lr=1.9960e-03 total_loss:1.8558\n",
            "global_steps:757 epoch:1 steps:757/2885 cls_loss:0.4128 cnt_loss:0.5874 reg_loss:0.7141 cost_time:776ms lr=1.9960e-03 total_loss:1.7143\n",
            "global_steps:758 epoch:1 steps:758/2885 cls_loss:0.5772 cnt_loss:0.6428 reg_loss:0.6805 cost_time:1117ms lr=1.9960e-03 total_loss:1.9004\n",
            "global_steps:759 epoch:1 steps:759/2885 cls_loss:0.5963 cnt_loss:0.6208 reg_loss:0.6271 cost_time:1156ms lr=1.9960e-03 total_loss:1.8442\n",
            "global_steps:760 epoch:1 steps:760/2885 cls_loss:0.5400 cnt_loss:0.6312 reg_loss:0.8499 cost_time:1009ms lr=1.9960e-03 total_loss:2.0211\n",
            "global_steps:761 epoch:1 steps:761/2885 cls_loss:0.5596 cnt_loss:0.6040 reg_loss:0.7376 cost_time:1076ms lr=1.9960e-03 total_loss:1.9012\n",
            "global_steps:762 epoch:1 steps:762/2885 cls_loss:0.5358 cnt_loss:0.6603 reg_loss:0.9018 cost_time:881ms lr=1.9960e-03 total_loss:2.0979\n",
            "global_steps:763 epoch:1 steps:763/2885 cls_loss:0.5776 cnt_loss:0.6417 reg_loss:0.9197 cost_time:1269ms lr=1.9960e-03 total_loss:2.1390\n",
            "global_steps:764 epoch:1 steps:764/2885 cls_loss:0.5859 cnt_loss:0.6527 reg_loss:0.9086 cost_time:1108ms lr=1.9960e-03 total_loss:2.1472\n",
            "global_steps:765 epoch:1 steps:765/2885 cls_loss:0.4244 cnt_loss:0.5965 reg_loss:0.7848 cost_time:773ms lr=1.9960e-03 total_loss:1.8058\n",
            "global_steps:766 epoch:1 steps:766/2885 cls_loss:0.5497 cnt_loss:0.6594 reg_loss:0.8717 cost_time:1205ms lr=1.9960e-03 total_loss:2.0808\n",
            "global_steps:767 epoch:1 steps:767/2885 cls_loss:0.6907 cnt_loss:0.6350 reg_loss:0.7348 cost_time:937ms lr=1.9960e-03 total_loss:2.0605\n",
            "global_steps:768 epoch:1 steps:768/2885 cls_loss:0.4898 cnt_loss:0.6290 reg_loss:0.8199 cost_time:978ms lr=1.9960e-03 total_loss:1.9387\n",
            "global_steps:769 epoch:1 steps:769/2885 cls_loss:0.4518 cnt_loss:0.6301 reg_loss:0.7381 cost_time:912ms lr=1.9960e-03 total_loss:1.8200\n",
            "global_steps:770 epoch:1 steps:770/2885 cls_loss:0.5932 cnt_loss:0.6249 reg_loss:0.8625 cost_time:769ms lr=1.9960e-03 total_loss:2.0806\n",
            "global_steps:771 epoch:1 steps:771/2885 cls_loss:0.5925 cnt_loss:0.6122 reg_loss:0.8328 cost_time:810ms lr=1.9960e-03 total_loss:2.0375\n",
            "global_steps:772 epoch:1 steps:772/2885 cls_loss:0.4227 cnt_loss:0.6141 reg_loss:0.8715 cost_time:967ms lr=1.9960e-03 total_loss:1.9083\n",
            "global_steps:773 epoch:1 steps:773/2885 cls_loss:0.4961 cnt_loss:0.6469 reg_loss:0.8328 cost_time:1218ms lr=1.9960e-03 total_loss:1.9759\n",
            "global_steps:774 epoch:1 steps:774/2885 cls_loss:0.7552 cnt_loss:0.6781 reg_loss:0.9362 cost_time:1218ms lr=1.9960e-03 total_loss:2.3695\n",
            "global_steps:775 epoch:1 steps:775/2885 cls_loss:0.6248 cnt_loss:0.6293 reg_loss:0.6992 cost_time:991ms lr=1.9960e-03 total_loss:1.9533\n",
            "global_steps:776 epoch:1 steps:776/2885 cls_loss:0.5011 cnt_loss:0.4759 reg_loss:0.4507 cost_time:777ms lr=1.9960e-03 total_loss:1.4277\n",
            "global_steps:777 epoch:1 steps:777/2885 cls_loss:0.5366 cnt_loss:0.6299 reg_loss:0.7411 cost_time:950ms lr=1.9960e-03 total_loss:1.9076\n",
            "global_steps:778 epoch:1 steps:778/2885 cls_loss:0.6645 cnt_loss:0.6532 reg_loss:0.6613 cost_time:997ms lr=1.9960e-03 total_loss:1.9791\n",
            "global_steps:779 epoch:1 steps:779/2885 cls_loss:0.7121 cnt_loss:0.6322 reg_loss:0.5718 cost_time:757ms lr=1.9960e-03 total_loss:1.9161\n",
            "global_steps:780 epoch:1 steps:780/2885 cls_loss:0.5060 cnt_loss:0.6338 reg_loss:0.5999 cost_time:777ms lr=1.9960e-03 total_loss:1.7397\n",
            "global_steps:781 epoch:1 steps:781/2885 cls_loss:0.5500 cnt_loss:0.6545 reg_loss:0.6404 cost_time:908ms lr=1.9960e-03 total_loss:1.8449\n",
            "global_steps:782 epoch:1 steps:782/2885 cls_loss:0.4626 cnt_loss:0.6340 reg_loss:0.6581 cost_time:898ms lr=1.9960e-03 total_loss:1.7547\n",
            "global_steps:783 epoch:1 steps:783/2885 cls_loss:0.5555 cnt_loss:0.6499 reg_loss:0.6325 cost_time:1054ms lr=1.9960e-03 total_loss:1.8379\n",
            "global_steps:784 epoch:1 steps:784/2885 cls_loss:0.5877 cnt_loss:0.6596 reg_loss:0.7283 cost_time:813ms lr=1.9960e-03 total_loss:1.9756\n",
            "global_steps:785 epoch:1 steps:785/2885 cls_loss:0.6326 cnt_loss:0.6440 reg_loss:0.8196 cost_time:862ms lr=1.9960e-03 total_loss:2.0962\n",
            "global_steps:786 epoch:1 steps:786/2885 cls_loss:0.5823 cnt_loss:0.6367 reg_loss:0.8188 cost_time:911ms lr=1.9960e-03 total_loss:2.0378\n",
            "global_steps:787 epoch:1 steps:787/2885 cls_loss:0.5143 cnt_loss:0.6387 reg_loss:0.7801 cost_time:856ms lr=1.9960e-03 total_loss:1.9332\n",
            "global_steps:788 epoch:1 steps:788/2885 cls_loss:0.5312 cnt_loss:0.6216 reg_loss:0.7932 cost_time:985ms lr=1.9960e-03 total_loss:1.9460\n",
            "global_steps:789 epoch:1 steps:789/2885 cls_loss:0.4792 cnt_loss:0.6356 reg_loss:0.7190 cost_time:1027ms lr=1.9960e-03 total_loss:1.8338\n",
            "global_steps:790 epoch:1 steps:790/2885 cls_loss:0.4671 cnt_loss:0.6338 reg_loss:0.7777 cost_time:1049ms lr=1.9960e-03 total_loss:1.8787\n",
            "global_steps:791 epoch:1 steps:791/2885 cls_loss:0.7154 cnt_loss:0.6481 reg_loss:0.7974 cost_time:872ms lr=1.9960e-03 total_loss:2.1609\n",
            "global_steps:792 epoch:1 steps:792/2885 cls_loss:0.5565 cnt_loss:0.6385 reg_loss:0.7940 cost_time:1105ms lr=1.9960e-03 total_loss:1.9890\n",
            "global_steps:793 epoch:1 steps:793/2885 cls_loss:0.4868 cnt_loss:0.6329 reg_loss:0.7680 cost_time:772ms lr=1.9960e-03 total_loss:1.8876\n",
            "global_steps:794 epoch:1 steps:794/2885 cls_loss:0.5864 cnt_loss:0.6382 reg_loss:0.6964 cost_time:842ms lr=1.9960e-03 total_loss:1.9211\n",
            "global_steps:795 epoch:1 steps:795/2885 cls_loss:0.6046 cnt_loss:0.6423 reg_loss:0.5582 cost_time:899ms lr=1.9960e-03 total_loss:1.8051\n",
            "global_steps:796 epoch:1 steps:796/2885 cls_loss:0.6128 cnt_loss:0.6521 reg_loss:0.5983 cost_time:951ms lr=1.9960e-03 total_loss:1.8632\n",
            "global_steps:797 epoch:1 steps:797/2885 cls_loss:0.6667 cnt_loss:0.6488 reg_loss:0.6298 cost_time:1222ms lr=1.9960e-03 total_loss:1.9453\n",
            "global_steps:798 epoch:1 steps:798/2885 cls_loss:0.6259 cnt_loss:0.6042 reg_loss:0.5770 cost_time:1094ms lr=1.9960e-03 total_loss:1.8071\n",
            "global_steps:799 epoch:1 steps:799/2885 cls_loss:0.3900 cnt_loss:0.6229 reg_loss:0.6122 cost_time:968ms lr=1.9960e-03 total_loss:1.6251\n",
            "global_steps:800 epoch:1 steps:800/2885 cls_loss:0.5594 cnt_loss:0.6205 reg_loss:0.7203 cost_time:643ms lr=1.9960e-03 total_loss:1.9002\n",
            "global_steps:801 epoch:1 steps:801/2885 cls_loss:0.6490 cnt_loss:0.6324 reg_loss:0.6458 cost_time:916ms lr=1.9960e-03 total_loss:1.9273\n",
            "global_steps:802 epoch:1 steps:802/2885 cls_loss:0.5654 cnt_loss:0.6046 reg_loss:0.6049 cost_time:1101ms lr=1.9960e-03 total_loss:1.7749\n",
            "global_steps:803 epoch:1 steps:803/2885 cls_loss:0.4671 cnt_loss:0.4757 reg_loss:0.4500 cost_time:793ms lr=1.9960e-03 total_loss:1.3928\n",
            "global_steps:804 epoch:1 steps:804/2885 cls_loss:0.5663 cnt_loss:0.6185 reg_loss:0.6665 cost_time:989ms lr=1.9960e-03 total_loss:1.8513\n",
            "global_steps:805 epoch:1 steps:805/2885 cls_loss:0.6137 cnt_loss:0.6558 reg_loss:0.8387 cost_time:809ms lr=1.9960e-03 total_loss:2.1082\n",
            "global_steps:806 epoch:1 steps:806/2885 cls_loss:0.5432 cnt_loss:0.6301 reg_loss:0.6839 cost_time:992ms lr=1.9960e-03 total_loss:1.8572\n",
            "global_steps:807 epoch:1 steps:807/2885 cls_loss:0.6019 cnt_loss:0.6169 reg_loss:0.6489 cost_time:911ms lr=1.9960e-03 total_loss:1.8676\n",
            "global_steps:808 epoch:1 steps:808/2885 cls_loss:0.4971 cnt_loss:0.6308 reg_loss:0.7078 cost_time:986ms lr=1.9960e-03 total_loss:1.8357\n",
            "global_steps:809 epoch:1 steps:809/2885 cls_loss:0.4288 cnt_loss:0.6358 reg_loss:0.6808 cost_time:976ms lr=1.9960e-03 total_loss:1.7455\n",
            "global_steps:810 epoch:1 steps:810/2885 cls_loss:0.6347 cnt_loss:0.6317 reg_loss:0.6143 cost_time:777ms lr=1.9960e-03 total_loss:1.8807\n",
            "global_steps:811 epoch:1 steps:811/2885 cls_loss:0.4974 cnt_loss:0.5980 reg_loss:0.5183 cost_time:1005ms lr=1.9960e-03 total_loss:1.6137\n",
            "global_steps:812 epoch:1 steps:812/2885 cls_loss:0.5110 cnt_loss:0.6188 reg_loss:0.7185 cost_time:970ms lr=1.9960e-03 total_loss:1.8483\n",
            "global_steps:813 epoch:1 steps:813/2885 cls_loss:0.6332 cnt_loss:0.6847 reg_loss:0.8407 cost_time:862ms lr=1.9960e-03 total_loss:2.1586\n",
            "global_steps:814 epoch:1 steps:814/2885 cls_loss:0.5849 cnt_loss:0.6420 reg_loss:0.6495 cost_time:857ms lr=1.9960e-03 total_loss:1.8765\n",
            "global_steps:815 epoch:1 steps:815/2885 cls_loss:0.6209 cnt_loss:0.6160 reg_loss:0.5058 cost_time:876ms lr=1.9960e-03 total_loss:1.7427\n",
            "global_steps:816 epoch:1 steps:816/2885 cls_loss:0.5733 cnt_loss:0.6396 reg_loss:0.5594 cost_time:965ms lr=1.9960e-03 total_loss:1.7723\n",
            "global_steps:817 epoch:1 steps:817/2885 cls_loss:0.5463 cnt_loss:0.6490 reg_loss:0.7058 cost_time:795ms lr=1.9960e-03 total_loss:1.9011\n",
            "global_steps:818 epoch:1 steps:818/2885 cls_loss:0.6021 cnt_loss:0.6407 reg_loss:0.7844 cost_time:814ms lr=1.9960e-03 total_loss:2.0271\n",
            "global_steps:819 epoch:1 steps:819/2885 cls_loss:0.5651 cnt_loss:0.6363 reg_loss:0.5990 cost_time:1223ms lr=1.9960e-03 total_loss:1.8004\n",
            "global_steps:820 epoch:1 steps:820/2885 cls_loss:0.5303 cnt_loss:0.6065 reg_loss:0.4655 cost_time:770ms lr=1.9960e-03 total_loss:1.6023\n",
            "global_steps:821 epoch:1 steps:821/2885 cls_loss:0.7200 cnt_loss:0.6427 reg_loss:0.6328 cost_time:1223ms lr=1.9960e-03 total_loss:1.9954\n",
            "global_steps:822 epoch:1 steps:822/2885 cls_loss:0.3965 cnt_loss:0.6330 reg_loss:0.6422 cost_time:796ms lr=1.9960e-03 total_loss:1.6717\n",
            "global_steps:823 epoch:1 steps:823/2885 cls_loss:0.6052 cnt_loss:0.6667 reg_loss:0.5617 cost_time:1006ms lr=1.9960e-03 total_loss:1.8337\n",
            "global_steps:824 epoch:1 steps:824/2885 cls_loss:0.5165 cnt_loss:0.6318 reg_loss:0.5679 cost_time:788ms lr=1.9960e-03 total_loss:1.7162\n",
            "global_steps:825 epoch:1 steps:825/2885 cls_loss:0.5381 cnt_loss:0.6360 reg_loss:0.6436 cost_time:930ms lr=1.9960e-03 total_loss:1.8177\n",
            "global_steps:826 epoch:1 steps:826/2885 cls_loss:0.5504 cnt_loss:0.6477 reg_loss:0.6458 cost_time:822ms lr=1.9960e-03 total_loss:1.8439\n",
            "global_steps:827 epoch:1 steps:827/2885 cls_loss:0.5686 cnt_loss:0.6419 reg_loss:0.6039 cost_time:770ms lr=1.9960e-03 total_loss:1.8144\n",
            "global_steps:828 epoch:1 steps:828/2885 cls_loss:0.6547 cnt_loss:0.6773 reg_loss:0.7756 cost_time:1358ms lr=1.9960e-03 total_loss:2.1077\n",
            "global_steps:829 epoch:1 steps:829/2885 cls_loss:0.5735 cnt_loss:0.6173 reg_loss:0.6685 cost_time:837ms lr=1.9960e-03 total_loss:1.8593\n",
            "global_steps:830 epoch:1 steps:830/2885 cls_loss:0.5606 cnt_loss:0.4813 reg_loss:0.4910 cost_time:913ms lr=1.9960e-03 total_loss:1.5330\n",
            "global_steps:831 epoch:1 steps:831/2885 cls_loss:0.6324 cnt_loss:0.6441 reg_loss:0.6335 cost_time:881ms lr=1.9960e-03 total_loss:1.9101\n",
            "global_steps:832 epoch:1 steps:832/2885 cls_loss:0.4073 cnt_loss:0.6286 reg_loss:0.6527 cost_time:817ms lr=1.9960e-03 total_loss:1.6887\n",
            "global_steps:833 epoch:1 steps:833/2885 cls_loss:0.6162 cnt_loss:0.6486 reg_loss:0.7218 cost_time:816ms lr=1.9960e-03 total_loss:1.9866\n",
            "global_steps:834 epoch:1 steps:834/2885 cls_loss:0.6554 cnt_loss:0.6213 reg_loss:0.6760 cost_time:980ms lr=1.9960e-03 total_loss:1.9527\n",
            "global_steps:835 epoch:1 steps:835/2885 cls_loss:0.4423 cnt_loss:0.4767 reg_loss:0.4951 cost_time:1213ms lr=1.9960e-03 total_loss:1.4140\n",
            "global_steps:836 epoch:1 steps:836/2885 cls_loss:0.7031 cnt_loss:0.6248 reg_loss:0.4584 cost_time:913ms lr=1.9960e-03 total_loss:1.7863\n",
            "global_steps:837 epoch:1 steps:837/2885 cls_loss:0.6790 cnt_loss:0.6541 reg_loss:0.5956 cost_time:792ms lr=1.9960e-03 total_loss:1.9287\n",
            "global_steps:838 epoch:1 steps:838/2885 cls_loss:0.6335 cnt_loss:0.6369 reg_loss:0.7484 cost_time:1004ms lr=1.9960e-03 total_loss:2.0188\n",
            "global_steps:839 epoch:1 steps:839/2885 cls_loss:0.4936 cnt_loss:0.6201 reg_loss:0.6934 cost_time:778ms lr=1.9960e-03 total_loss:1.8071\n",
            "global_steps:840 epoch:1 steps:840/2885 cls_loss:0.5314 cnt_loss:0.6548 reg_loss:0.7558 cost_time:921ms lr=1.9960e-03 total_loss:1.9420\n",
            "global_steps:841 epoch:1 steps:841/2885 cls_loss:0.6973 cnt_loss:0.4724 reg_loss:0.5067 cost_time:784ms lr=1.9960e-03 total_loss:1.6764\n",
            "global_steps:842 epoch:1 steps:842/2885 cls_loss:0.5733 cnt_loss:0.6429 reg_loss:0.6849 cost_time:1096ms lr=1.9960e-03 total_loss:1.9011\n",
            "global_steps:843 epoch:1 steps:843/2885 cls_loss:0.6051 cnt_loss:0.6208 reg_loss:0.6227 cost_time:1030ms lr=1.9960e-03 total_loss:1.8486\n",
            "global_steps:844 epoch:1 steps:844/2885 cls_loss:0.6776 cnt_loss:0.6436 reg_loss:0.7660 cost_time:781ms lr=1.9960e-03 total_loss:2.0872\n",
            "global_steps:845 epoch:1 steps:845/2885 cls_loss:0.6287 cnt_loss:0.6403 reg_loss:0.6694 cost_time:819ms lr=1.9960e-03 total_loss:1.9384\n",
            "global_steps:846 epoch:1 steps:846/2885 cls_loss:0.7430 cnt_loss:0.6311 reg_loss:0.4793 cost_time:960ms lr=1.9960e-03 total_loss:1.8533\n",
            "global_steps:847 epoch:1 steps:847/2885 cls_loss:0.7034 cnt_loss:0.6675 reg_loss:0.6966 cost_time:1106ms lr=1.9960e-03 total_loss:2.0674\n",
            "global_steps:848 epoch:1 steps:848/2885 cls_loss:0.5191 cnt_loss:0.6410 reg_loss:0.8124 cost_time:944ms lr=1.9960e-03 total_loss:1.9725\n",
            "global_steps:849 epoch:1 steps:849/2885 cls_loss:0.4351 cnt_loss:0.6697 reg_loss:0.8392 cost_time:856ms lr=1.9960e-03 total_loss:1.9440\n",
            "global_steps:850 epoch:1 steps:850/2885 cls_loss:0.5868 cnt_loss:0.6345 reg_loss:0.6485 cost_time:977ms lr=1.9960e-03 total_loss:1.8698\n",
            "global_steps:851 epoch:1 steps:851/2885 cls_loss:0.5742 cnt_loss:0.6401 reg_loss:0.7230 cost_time:787ms lr=1.9960e-03 total_loss:1.9373\n",
            "global_steps:852 epoch:1 steps:852/2885 cls_loss:0.6581 cnt_loss:0.6484 reg_loss:0.6436 cost_time:818ms lr=1.9960e-03 total_loss:1.9501\n",
            "global_steps:853 epoch:1 steps:853/2885 cls_loss:0.5511 cnt_loss:0.6104 reg_loss:0.6463 cost_time:1097ms lr=1.9960e-03 total_loss:1.8079\n",
            "global_steps:854 epoch:1 steps:854/2885 cls_loss:0.4212 cnt_loss:0.6121 reg_loss:0.6198 cost_time:845ms lr=1.9960e-03 total_loss:1.6531\n",
            "global_steps:855 epoch:1 steps:855/2885 cls_loss:0.6493 cnt_loss:0.6509 reg_loss:0.7528 cost_time:961ms lr=1.9960e-03 total_loss:2.0530\n",
            "global_steps:856 epoch:1 steps:856/2885 cls_loss:0.5410 cnt_loss:0.6616 reg_loss:0.7635 cost_time:1066ms lr=1.9960e-03 total_loss:1.9660\n",
            "global_steps:857 epoch:1 steps:857/2885 cls_loss:0.4906 cnt_loss:0.5898 reg_loss:0.4036 cost_time:862ms lr=1.9960e-03 total_loss:1.4840\n",
            "global_steps:858 epoch:1 steps:858/2885 cls_loss:0.5624 cnt_loss:0.6378 reg_loss:0.4886 cost_time:1239ms lr=1.9960e-03 total_loss:1.6889\n",
            "global_steps:859 epoch:1 steps:859/2885 cls_loss:0.6878 cnt_loss:0.6270 reg_loss:0.6110 cost_time:861ms lr=1.9960e-03 total_loss:1.9258\n",
            "global_steps:860 epoch:1 steps:860/2885 cls_loss:0.4631 cnt_loss:0.6304 reg_loss:0.6809 cost_time:870ms lr=1.9960e-03 total_loss:1.7744\n",
            "global_steps:861 epoch:1 steps:861/2885 cls_loss:0.6646 cnt_loss:0.6500 reg_loss:0.7425 cost_time:982ms lr=1.9960e-03 total_loss:2.0571\n",
            "global_steps:862 epoch:1 steps:862/2885 cls_loss:0.5161 cnt_loss:0.6360 reg_loss:0.7097 cost_time:818ms lr=1.9960e-03 total_loss:1.8619\n",
            "global_steps:863 epoch:1 steps:863/2885 cls_loss:0.5959 cnt_loss:0.6362 reg_loss:0.5600 cost_time:772ms lr=1.9960e-03 total_loss:1.7921\n",
            "global_steps:864 epoch:1 steps:864/2885 cls_loss:0.5962 cnt_loss:0.6353 reg_loss:0.5609 cost_time:992ms lr=1.9960e-03 total_loss:1.7924\n",
            "global_steps:865 epoch:1 steps:865/2885 cls_loss:0.5347 cnt_loss:0.4849 reg_loss:0.4684 cost_time:797ms lr=1.9960e-03 total_loss:1.4880\n",
            "global_steps:866 epoch:1 steps:866/2885 cls_loss:0.4813 cnt_loss:0.6737 reg_loss:0.6410 cost_time:882ms lr=1.9960e-03 total_loss:1.7960\n",
            "global_steps:867 epoch:1 steps:867/2885 cls_loss:0.4640 cnt_loss:0.4930 reg_loss:0.4406 cost_time:1251ms lr=1.9960e-03 total_loss:1.3976\n",
            "global_steps:868 epoch:1 steps:868/2885 cls_loss:0.4615 cnt_loss:0.5918 reg_loss:0.5633 cost_time:765ms lr=1.9960e-03 total_loss:1.6166\n",
            "global_steps:869 epoch:1 steps:869/2885 cls_loss:0.4950 cnt_loss:0.6463 reg_loss:0.5312 cost_time:1097ms lr=1.9960e-03 total_loss:1.6725\n",
            "global_steps:870 epoch:1 steps:870/2885 cls_loss:0.5458 cnt_loss:0.6174 reg_loss:0.5848 cost_time:938ms lr=1.9960e-03 total_loss:1.7481\n",
            "global_steps:871 epoch:1 steps:871/2885 cls_loss:0.5872 cnt_loss:0.6454 reg_loss:0.6533 cost_time:866ms lr=1.9960e-03 total_loss:1.8859\n",
            "global_steps:872 epoch:1 steps:872/2885 cls_loss:0.6517 cnt_loss:0.6546 reg_loss:0.5968 cost_time:771ms lr=1.9960e-03 total_loss:1.9032\n",
            "global_steps:873 epoch:1 steps:873/2885 cls_loss:0.4718 cnt_loss:0.6172 reg_loss:0.6231 cost_time:1138ms lr=1.9960e-03 total_loss:1.7121\n",
            "global_steps:874 epoch:1 steps:874/2885 cls_loss:0.5207 cnt_loss:0.6426 reg_loss:0.7128 cost_time:948ms lr=1.9960e-03 total_loss:1.8761\n",
            "global_steps:875 epoch:1 steps:875/2885 cls_loss:0.5410 cnt_loss:0.6164 reg_loss:0.7587 cost_time:941ms lr=1.9960e-03 total_loss:1.9160\n",
            "global_steps:876 epoch:1 steps:876/2885 cls_loss:0.5522 cnt_loss:0.6315 reg_loss:0.6827 cost_time:1026ms lr=1.9960e-03 total_loss:1.8665\n",
            "global_steps:877 epoch:1 steps:877/2885 cls_loss:0.3275 cnt_loss:0.5952 reg_loss:0.6469 cost_time:836ms lr=1.9960e-03 total_loss:1.5696\n",
            "global_steps:878 epoch:1 steps:878/2885 cls_loss:0.6701 cnt_loss:0.6478 reg_loss:0.5940 cost_time:1125ms lr=1.9960e-03 total_loss:1.9119\n",
            "global_steps:879 epoch:1 steps:879/2885 cls_loss:0.4257 cnt_loss:0.6242 reg_loss:0.6354 cost_time:886ms lr=1.9960e-03 total_loss:1.6852\n",
            "global_steps:880 epoch:1 steps:880/2885 cls_loss:0.5107 cnt_loss:0.6221 reg_loss:0.5752 cost_time:1222ms lr=1.9960e-03 total_loss:1.7079\n",
            "global_steps:881 epoch:1 steps:881/2885 cls_loss:0.6204 cnt_loss:0.6664 reg_loss:0.5831 cost_time:1013ms lr=1.9960e-03 total_loss:1.8698\n",
            "global_steps:882 epoch:1 steps:882/2885 cls_loss:0.7120 cnt_loss:0.6327 reg_loss:0.6446 cost_time:1096ms lr=1.9960e-03 total_loss:1.9893\n",
            "global_steps:883 epoch:1 steps:883/2885 cls_loss:0.5128 cnt_loss:0.6096 reg_loss:0.6822 cost_time:975ms lr=1.9960e-03 total_loss:1.8046\n",
            "global_steps:884 epoch:1 steps:884/2885 cls_loss:0.4862 cnt_loss:0.6179 reg_loss:0.7144 cost_time:855ms lr=1.9960e-03 total_loss:1.8184\n",
            "global_steps:885 epoch:1 steps:885/2885 cls_loss:0.4746 cnt_loss:0.6053 reg_loss:0.6713 cost_time:1096ms lr=1.9960e-03 total_loss:1.7513\n",
            "global_steps:886 epoch:1 steps:886/2885 cls_loss:0.5186 cnt_loss:0.6091 reg_loss:0.6055 cost_time:988ms lr=1.9960e-03 total_loss:1.7333\n",
            "global_steps:887 epoch:1 steps:887/2885 cls_loss:0.5188 cnt_loss:0.6211 reg_loss:0.5971 cost_time:856ms lr=1.9960e-03 total_loss:1.7370\n",
            "global_steps:888 epoch:1 steps:888/2885 cls_loss:0.4957 cnt_loss:0.6078 reg_loss:0.5475 cost_time:797ms lr=1.9960e-03 total_loss:1.6510\n",
            "global_steps:889 epoch:1 steps:889/2885 cls_loss:0.4978 cnt_loss:0.6127 reg_loss:0.5393 cost_time:845ms lr=1.9960e-03 total_loss:1.6499\n",
            "global_steps:890 epoch:1 steps:890/2885 cls_loss:0.5898 cnt_loss:0.6109 reg_loss:0.6201 cost_time:774ms lr=1.9960e-03 total_loss:1.8208\n",
            "global_steps:891 epoch:1 steps:891/2885 cls_loss:0.6181 cnt_loss:0.6228 reg_loss:0.5477 cost_time:909ms lr=1.9960e-03 total_loss:1.7887\n",
            "global_steps:892 epoch:1 steps:892/2885 cls_loss:0.5203 cnt_loss:0.6483 reg_loss:0.6420 cost_time:789ms lr=1.9960e-03 total_loss:1.8107\n",
            "global_steps:893 epoch:1 steps:893/2885 cls_loss:0.5777 cnt_loss:0.6475 reg_loss:0.4876 cost_time:909ms lr=1.9960e-03 total_loss:1.7128\n",
            "global_steps:894 epoch:1 steps:894/2885 cls_loss:0.4489 cnt_loss:0.4809 reg_loss:0.3574 cost_time:774ms lr=1.9960e-03 total_loss:1.2872\n",
            "global_steps:895 epoch:1 steps:895/2885 cls_loss:0.6654 cnt_loss:0.6538 reg_loss:0.5739 cost_time:843ms lr=1.9960e-03 total_loss:1.8931\n",
            "global_steps:896 epoch:1 steps:896/2885 cls_loss:0.5994 cnt_loss:0.6454 reg_loss:0.6414 cost_time:754ms lr=1.9960e-03 total_loss:1.8861\n",
            "global_steps:897 epoch:1 steps:897/2885 cls_loss:0.3943 cnt_loss:0.6410 reg_loss:0.5898 cost_time:1244ms lr=1.9960e-03 total_loss:1.6251\n",
            "global_steps:898 epoch:1 steps:898/2885 cls_loss:0.4900 cnt_loss:0.6084 reg_loss:0.4623 cost_time:882ms lr=1.9960e-03 total_loss:1.5608\n",
            "global_steps:899 epoch:1 steps:899/2885 cls_loss:0.5101 cnt_loss:0.6162 reg_loss:0.6006 cost_time:785ms lr=1.9960e-03 total_loss:1.7268\n",
            "global_steps:900 epoch:1 steps:900/2885 cls_loss:0.5734 cnt_loss:0.6362 reg_loss:0.5410 cost_time:942ms lr=1.9960e-03 total_loss:1.7506\n",
            "global_steps:901 epoch:1 steps:901/2885 cls_loss:0.5728 cnt_loss:0.6356 reg_loss:0.5845 cost_time:859ms lr=1.9960e-03 total_loss:1.7929\n",
            "global_steps:902 epoch:1 steps:902/2885 cls_loss:0.3994 cnt_loss:0.6149 reg_loss:0.6027 cost_time:875ms lr=1.9960e-03 total_loss:1.6170\n",
            "global_steps:903 epoch:1 steps:903/2885 cls_loss:0.6900 cnt_loss:0.6490 reg_loss:0.5828 cost_time:1241ms lr=1.9960e-03 total_loss:1.9218\n",
            "global_steps:904 epoch:1 steps:904/2885 cls_loss:0.5835 cnt_loss:0.6335 reg_loss:0.5726 cost_time:977ms lr=1.9960e-03 total_loss:1.7897\n",
            "global_steps:905 epoch:1 steps:905/2885 cls_loss:0.5572 cnt_loss:0.6331 reg_loss:0.4080 cost_time:882ms lr=1.9960e-03 total_loss:1.5983\n",
            "global_steps:906 epoch:1 steps:906/2885 cls_loss:0.8619 cnt_loss:0.6270 reg_loss:0.6249 cost_time:1117ms lr=1.9960e-03 total_loss:2.1139\n",
            "global_steps:907 epoch:1 steps:907/2885 cls_loss:0.4887 cnt_loss:0.6351 reg_loss:0.6653 cost_time:989ms lr=1.9960e-03 total_loss:1.7891\n",
            "global_steps:908 epoch:1 steps:908/2885 cls_loss:0.4338 cnt_loss:0.6214 reg_loss:0.5345 cost_time:775ms lr=1.9960e-03 total_loss:1.5897\n",
            "global_steps:909 epoch:1 steps:909/2885 cls_loss:0.5501 cnt_loss:0.6485 reg_loss:0.5477 cost_time:903ms lr=1.9960e-03 total_loss:1.7463\n",
            "global_steps:910 epoch:1 steps:910/2885 cls_loss:0.6604 cnt_loss:0.4871 reg_loss:0.4375 cost_time:987ms lr=1.9960e-03 total_loss:1.5849\n",
            "global_steps:911 epoch:1 steps:911/2885 cls_loss:0.5323 cnt_loss:0.6367 reg_loss:0.6617 cost_time:927ms lr=1.9960e-03 total_loss:1.8306\n",
            "global_steps:912 epoch:1 steps:912/2885 cls_loss:0.6147 cnt_loss:0.6475 reg_loss:0.6524 cost_time:874ms lr=1.9960e-03 total_loss:1.9146\n",
            "global_steps:913 epoch:1 steps:913/2885 cls_loss:0.6026 cnt_loss:0.6545 reg_loss:0.5882 cost_time:995ms lr=1.9960e-03 total_loss:1.8454\n",
            "global_steps:914 epoch:1 steps:914/2885 cls_loss:0.5526 cnt_loss:0.6392 reg_loss:0.5618 cost_time:1004ms lr=1.9960e-03 total_loss:1.7536\n",
            "global_steps:915 epoch:1 steps:915/2885 cls_loss:0.5953 cnt_loss:0.6157 reg_loss:0.5014 cost_time:1399ms lr=1.9960e-03 total_loss:1.7124\n",
            "global_steps:916 epoch:1 steps:916/2885 cls_loss:0.5906 cnt_loss:0.6172 reg_loss:0.4906 cost_time:795ms lr=1.9960e-03 total_loss:1.6984\n",
            "global_steps:917 epoch:1 steps:917/2885 cls_loss:0.4767 cnt_loss:0.6480 reg_loss:0.5904 cost_time:1094ms lr=1.9960e-03 total_loss:1.7152\n",
            "global_steps:918 epoch:1 steps:918/2885 cls_loss:0.4753 cnt_loss:0.6327 reg_loss:0.5612 cost_time:869ms lr=1.9960e-03 total_loss:1.6692\n",
            "global_steps:919 epoch:1 steps:919/2885 cls_loss:0.5629 cnt_loss:0.6495 reg_loss:0.5999 cost_time:866ms lr=1.9960e-03 total_loss:1.8124\n",
            "global_steps:920 epoch:1 steps:920/2885 cls_loss:0.5266 cnt_loss:0.6122 reg_loss:0.5432 cost_time:953ms lr=1.9960e-03 total_loss:1.6819\n",
            "global_steps:921 epoch:1 steps:921/2885 cls_loss:0.5678 cnt_loss:0.6441 reg_loss:0.6178 cost_time:936ms lr=1.9960e-03 total_loss:1.8298\n",
            "global_steps:922 epoch:1 steps:922/2885 cls_loss:0.6430 cnt_loss:0.6282 reg_loss:0.5475 cost_time:766ms lr=1.9960e-03 total_loss:1.8187\n",
            "global_steps:923 epoch:1 steps:923/2885 cls_loss:0.7382 cnt_loss:0.6384 reg_loss:0.5286 cost_time:895ms lr=1.9960e-03 total_loss:1.9052\n",
            "global_steps:924 epoch:1 steps:924/2885 cls_loss:0.4200 cnt_loss:0.6005 reg_loss:0.3987 cost_time:835ms lr=1.9960e-03 total_loss:1.4192\n",
            "global_steps:925 epoch:1 steps:925/2885 cls_loss:0.6241 cnt_loss:0.6294 reg_loss:0.5295 cost_time:949ms lr=1.9960e-03 total_loss:1.7830\n",
            "global_steps:926 epoch:1 steps:926/2885 cls_loss:0.5930 cnt_loss:0.6270 reg_loss:0.5793 cost_time:978ms lr=1.9960e-03 total_loss:1.7994\n",
            "global_steps:927 epoch:1 steps:927/2885 cls_loss:0.8585 cnt_loss:0.4728 reg_loss:0.4793 cost_time:1108ms lr=1.9960e-03 total_loss:1.8107\n",
            "global_steps:928 epoch:1 steps:928/2885 cls_loss:0.5135 cnt_loss:0.6291 reg_loss:0.5054 cost_time:859ms lr=1.9960e-03 total_loss:1.6480\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}