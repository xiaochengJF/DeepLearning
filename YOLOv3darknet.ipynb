{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv3darknet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IbcjQ8tgKDF8",
        "s8vI3qukKW9H",
        "GdQM_Zz3KeUI",
        "Z90nxkl9LVQn",
        "Gbd75EJULeQQ",
        "MPKHILkMMftb",
        "nRC1BocFMs5v",
        "DnC-uBY0M45_",
        "P1vKGlSkNDHe",
        "1KkO6ozHMy3H",
        "U-Aiwnw3NHYP"
      ],
      "authorship_tag": "ABX9TyNJeeXx24yS4AaNnEQnA6lE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochengJF/DeepLearning/blob/master/YOLOv3darknet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISr5GTIjJ9gr",
        "colab_type": "text"
      },
      "source": [
        "# <font face=STCAIYUN color=purple size=8>YOLOv3</font>\n",
        "<font face=楷体 color=skyblue size=4>YOLO 是 You Only Look Once 的缩写</font>  \n",
        "<font face=楷体 color=skyblue size=4>**全卷积神经网络** </font>  \n",
        "<font face=楷体>\n",
        "YOLO 仅使用卷积层，即：全卷积神经网络（FCN对于输入图像的大小不敏感） ，它拥有 75 个卷积层，带有跳跃连接和上采样层。不使用任何它形式的，使用步幅为 2 的卷积层代替池化层对特征图进行下采样，防止池化导致的低级特征丢失 \n",
        "    \n",
        "问题是：如果希望按批次处理图像（批量图像由 GPU 并行处理，这样可以提升速度），就需要固定图像的高度和宽度，便于将多个图像整合进一个大的批次（将许多 PyTorch 张量合并成一个）</font>  \n",
        " \n",
        "\n",
        "\n",
        "<font face=楷体 color=skyblue size=4>**YOLOv3网络结构图** </font>  \n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://img-blog.csdnimg.cn/20190824145218799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxMTU1NA==,size_16,color_FFFFFF,t_70\">\n",
        "</center>\n",
        "\n",
        "<font face=楷体 color=skyblue size=4>**网络输出**</font>   \n",
        "\n",
        "<center>\n",
        "<img src=\"https://blog.paperspace.com/content/images/2018/04/yolo-5.png\">\n",
        "</center> \n",
        "\n",
        "<font face=楷体 color=skyblue size=4>锚点框（Anchor Box）</font>  \n",
        "<font face=楷体>\n",
        "直接预测边界框的宽度和高度比较直观，但训练中容易造成梯度不稳定，所以大部分目标检测器都预测对数空间（log-space）变换量，或者预测与预定义边界框（即锚点）之间的偏移量  \n",
        "将这些变换被应用到锚点框来获得最终预测，YOLO v3 有三个锚点，所以每个单元格会预测 3 个边界框  \n",
        "</font>   \n",
        "\n",
        "<font face=楷体 color=skyblue size=4>中心坐标</font>    \n",
        "<font face=楷体>\n",
        "通过一个sigmoid函数对中心坐标预测，迫使输出介于0和1之间  \n",
        "YOLO通常不会预测边界框中心的绝对坐标而是预测偏移量：  \n",
        "\n",
        "\n",
        "*   相对于预测对象格单元格的左上角\n",
        "*   由特征图中的单元格尺寸标准化，如上图：对狗狗中心的预测是（0.4，0.7），那么中心位于$13 \\times 13$特征图上的（6.4，6.7）位置  \n",
        "</font>\n",
        "\n",
        "\n",
        "<font face=楷体 color=skyblue size=5>预测</font>  \n",
        "<font face=楷体>每个bounding box预测5个值：$\\color{pink}{t_x，t_y，t_w，t_h，t_o}$ （$t_o$类似YOLOv1中的confidence）  \n",
        "\n",
        "*   $\\color{pink}{t_x，t_y}$：经过sigmoid函数处理后范围在0到1之间，模型训练更加稳定  \n",
        "*   $\\color{pink}{c_x，c_y}$：表示一个cell和图像左上角的横纵距离 \n",
        "*   $\\color{pink}{p_w，p_h}$：表示bounding box的宽高 \n",
        "</font>\n",
        "\n",
        " \n",
        "$$\n",
        "\\begin{aligned} b_{x} &=\\sigma\\left(t_{x}\\right)+c_{x} \\\\ b_{y} &=\\sigma\\left(t_{y}\\right)+c_{y} \\\\ b_{w} &=p_{w} e^{t_{w}} \\\\ b_{h} &=p_{h} e^{t_{h}} \\\\ \\operatorname{Pr}(\\text { object }) * I O U(b, \\text { object }) &=\\sigma\\left(t_{o}\\right)\\end{aligned}\n",
        "$$\n",
        "\n",
        "<font face=楷体 color=yellow>在Faster R-CNN中：</font>\n",
        "$$\\begin{align}\n",
        "t_x &= (x - x_a) /w_a \\ ,\\  \\ t_y = (y - y_a) / h_a\\\\[1ex]t_w &= \\log(w/ w_a), \\quad \\ \\  t_h = \\log(h/h_a)\\\\[1ex]t_x^* &= (x^* - x_a) / w_a,\\  t_y^* = (y^* - y_a) /h_a\\\\[1ex]t_w^* &= \\log(w^* - w),\\ \\  h_h^* = \\log(h^*/h_a)\n",
        "\\end{align}$$  \n",
        "\n",
        "<font face=楷体 color=skyblue size=4>边界框的尺寸</font>   \n",
        "<font face=楷体>\n",
        "将输出进行对数变换乘以锚来预测边界框的尺寸  \n",
        "预测结果$b_w$和$b_h$由图像的高度和宽度标准化，如果狗的预测框为$（0.3，0.8）$，那么$13 \\times 13$特征图上的实际宽度和高度是$（13 \\times 0.3,13 \\times 0.8）$\n",
        "</font>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://blog.paperspace.com/content/images/2018/04/yolo-regression-1.png\">\n",
        "</center>\n",
        "\n",
        "<font face=楷体 color=skyblue size=4>Score</font>  \n",
        "<font face=楷体>\n",
        "对象得分通过sigmoid转换到0、1之间，可以解释为概率，红色和相邻的网格应该接近1，而角落的网格应该接近0 \n",
        "</font>  \n",
        "\n",
        "<font face=楷体 color=skyblue size=4>Class Confidences</font>   \n",
        "<font face=楷体>\n",
        "v3使用sigmoid代替softmax进行分类，原因是Softmax假设所有类别是互斥的，即：如果一个对象属于一个类，那么就一定不属于另一个类（COCO数据库是可以的但如：医生和人就不成立了）  \n",
        "</font>   \n",
        "<font face=楷体 color=skyblue size=4>多尺度预测</font>   \n",
        "<font face=楷体>\n",
        "基于三种不同尺寸的特征图进行检测，<font color=skyblue>有助于检测小物体</font>。如：输入尺寸为$416 \\times 416$，在$13 \\times 13,26 \\times 26,52 \\times 52$的特诊图上进行检测  \n",
        "每个尺度上，每个单元格使用3个anchors预测3个边界框，使得使用的anchors总数为9.（<font color=skyblue>不同尺度上的anchors不同</font>）  \n",
        "</font> \n",
        "\n",
        "<center>\n",
        "<img src=\"https://blog.paperspace.com/content/images/2018/04/yolo_Scales-1.png\">\n",
        "</center>\n",
        "\n",
        "<font face=楷体 color=skyblue size=5>输出处理</font>  \n",
        "<font face=楷体>  \n",
        "对于尺寸为$416\\times 416$的图像，YOLO预测$（（52\\times 52）+（26\\times 26）+ 13\\times 13））×3 = 10647$个边界框  \n",
        "<font color=skyblue>\n",
        "通过对象置信度进行阈值处理  \n",
        "非极大抑制（NMS）\n",
        "</font>\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbcjQ8tgKDF8",
        "colab_type": "text"
      },
      "source": [
        "# 导入相关模块"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IvoeDGMaXkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from __future__ import division\n",
        "import math\n",
        "import time\n",
        "import datetime\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import tensorflow as tf\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from terminaltables import AsciiTable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w3q31j3KOdv",
        "colab_type": "text"
      },
      "source": [
        "# 搭建模型\n",
        "<font face=楷体>\n",
        "\n",
        "* 解析模型参数\n",
        "* 构件模块\n",
        "* Darknet搭建\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8vI3qukKW9H",
        "colab_type": "text"
      },
      "source": [
        "## 解析配置文件\n",
        "<font face=楷体>\n",
        "\n",
        "* parse_cfg 解析模型参数，将 <font color=skyblue size=4>**Net 、  Convolutional  、 Shortcut  、Upsample 、 Route 、 YOLO**</font> 等模型结构信息以列表的形式返回，便于后面模型搭建\n",
        "* parse_data_config 解析数据集路径\n",
        "</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_34zcAqYhob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_model_config(path):\n",
        "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
        "    file = open(path, 'r')\n",
        "    lines = file.read().split('\\n')\n",
        "    lines = [x for x in lines if x and not x.startswith('#')]\n",
        "    lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces\n",
        "    module_defs = []\n",
        "    for line in lines:\n",
        "        if line.startswith('['):  # This marks the start of a new block\n",
        "            module_defs.append({})\n",
        "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
        "            if module_defs[-1]['type'] == 'convolutional':\n",
        "                module_defs[-1]['batch_normalize'] = 0\n",
        "        else:\n",
        "            key, value = line.split(\"=\")\n",
        "            value = value.strip()\n",
        "            module_defs[-1][key.rstrip()] = value.strip()\n",
        "\n",
        "    return module_defs\n",
        "\n",
        "def parse_data_config(path):\n",
        "    \"\"\"Parses the data configuration file\"\"\"\n",
        "    options = dict()\n",
        "    options['gpus'] = '0,1,2,3'\n",
        "    options['num_workers'] = '10'\n",
        "    with open(path, 'r') as fp:\n",
        "        lines = fp.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line == '' or line.startswith('#'):\n",
        "            continue\n",
        "        key, value = line.split('=')\n",
        "        options[key.strip()] = value.strip()\n",
        "    return options"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdQM_Zz3KeUI",
        "colab_type": "text"
      },
      "source": [
        "## 构建模块\n",
        "\n",
        "<font face=楷体>\n",
        "\n",
        "* convolutional卷积层\n",
        "* maxpool最大池化（可选）\n",
        "* upsample上采样\n",
        "* route层\n",
        "* shortcut跳跃连接\n",
        "* yolo层\n",
        "</font>\n",
        "\n",
        "<font face=楷体 color=skyblue size=4>**create_modules 函数用 parse_cfg 函数返回的模型信息列表构建网络模块：**</font>\n",
        "\n",
        "<font face=楷体>\n",
        "\n",
        "*   先定义变量 hyperparams，来存储该网络的信息\n",
        "\n",
        "*  当添加 nn.ModuleList 作为 nn.Module 对象的一个成员时（即添加模块到网络），所有 nn.ModuleList 内部的 nn.Module 对象（模块）的 parameter 也被添加   作为 nn.Module 对象（即网络添加 nn.ModuleList 作为其成员）的 parameter\n",
        "\n",
        "*   卷积核的深度是由上一层的卷积核数量（或特征图深度）决定的，需要追踪上一层卷及数量。路由层（route layer）从前面层得到特征图，不仅需要追踪前一层的卷积核数量，还需要追踪之前每一层，这意味着需要持续追踪被应用卷积层的卷积核数量，用变量 output_filters 保存\n",
        "\n",
        "*  nn.Sequential 类能让nn.Module 对象有序执行，用 nn.Sequential将一个模块的多个层串起来\n",
        "    \n",
        "</font>  \n",
        "\n",
        "<font face=楷体 color=skyblue size=4>**为什么要一个空的层？**</font>  \n",
        "<font face=楷体>\n",
        "*   如果像其它层一样，创建路由层需要构建一个 nn.Module 对象并初始化，然后在 forward 函数中拼接特征图，但拼接操作的代码相当简短（ torch.cat），像其它层一样设计route层将导致不必要的抽象，增加代码。可以用一个空的虚拟层代替路由层，然后 forward 函数中直接执行拼接操作  \n",
        "*   shortcut层是一个简单的add操作，也用一个空的虚拟层代替\n",
        "</font>\n",
        "\n",
        "<font face=楷体 color=skyblue size=4>多尺度检测</font> \n",
        "<font face=楷体> \n",
        "\n",
        "*   在特征图上进行多尺度预测, 在grid每个位置都有三个不同尺度的锚点.predict_transform()利用一个scale得到的feature map预测得到的每个anchor的属性(x,y,w,h,s,s_cls1,s_cls2...),其中x,y,w,h是在网络输入图片坐标系下的值，s是方框含有目标的置信度得分，s_cls1,s_cls_2等是方框所含目标对应每类的概率 \n",
        "\n",
        "*   输入的feature map(prediction变量) 维度为<font color=pink>【batch_size, num_anchors*bbox_attrs, grid_size, grid_size】</font>(一个batch：$B\\times C\\times H\\times W$)，这种格式对于输出处理过程（例如通过目标置信度进行阈值处理、添加对中心的网格偏移、应用锚点等）不方便\n",
        "*   将维度变换成<font color=pink>【batch_size, grid_size*grid_size*num_anchors, 5+类别数量】</font>的tensor，同时得到每个方框在网络输入图片$(416\\times 416)$坐标系下的(x,y,w,h)以及方框含有目标的得分以及每个类的得分\n",
        "* 由于检测是在三个尺度上进行的，预测图的维度将是不同的。虽然三个特征图的维度不同，但对它们执行的输出处理过程是相似的\n",
        "\n",
        "</font>\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://blog.paperspace.com/content/images/2018/04/bbox_-2.png#pic_center\">\n",
        "</center>\n",
        "\n",
        "<font face=楷体 color=green size=4>**绿色链接:**</font>  \n",
        "<font face=楷体>\n",
        "【1】[nn.Conv2d 、nn.BatchNorm2d](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#torchnn)  \n",
        "【2】[torch.nn.Upsample](https://pytorch.org/docs/stable/nn.html?highlight=nn%20upsample#torch.nn.Upsample)  \n",
        "【3】[darknet 所有层功能说明 ](https://blog.csdn.net/zhuiqiuk/article/details/88187034)  \n",
        "【4】[nn.Module](https://blog.csdn.net/u012609509/article/details/81203436) \n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTB-xwOcRNKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyLKM9zDZAIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_modules(module_defs):\n",
        "    \"\"\"\n",
        "    Constructs module list of layer blocks from module configuration in module_defs\n",
        "    \"\"\"\n",
        "    hyperparams = module_defs.pop(0)\n",
        "    output_filters = [int(hyperparams[\"channels\"])]\n",
        "    module_list = nn.ModuleList()\n",
        "    for module_i, module_def in enumerate(module_defs):\n",
        "        modules = nn.Sequential()\n",
        "\n",
        "        if module_def[\"type\"] == \"convolutional\":\n",
        "            bn = int(module_def[\"batch_normalize\"])\n",
        "            filters = int(module_def[\"filters\"])\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            pad = (kernel_size - 1) // 2\n",
        "            modules.add_module(\n",
        "                f\"conv_{module_i}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=output_filters[-1],\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=int(module_def[\"stride\"]),\n",
        "                    padding=pad,\n",
        "                    bias=not bn,\n",
        "                ),\n",
        "            )\n",
        "            if bn:\n",
        "                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n",
        "            if module_def[\"activation\"] == \"leaky\":\n",
        "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
        "\n",
        "        elif module_def[\"type\"] == \"maxpool\":\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            stride = int(module_def[\"stride\"])\n",
        "            if kernel_size == 2 and stride == 1:\n",
        "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n",
        "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
        "\n",
        "        elif module_def[\"type\"] == \"upsample\":\n",
        "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
        "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
        "\n",
        "        elif module_def[\"type\"] == \"route\":\n",
        "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
        "            filters = sum([output_filters[1:][i] for i in layers])\n",
        "            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"shortcut\":\n",
        "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
        "            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"yolo\":\n",
        "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
        "            # Extract anchors\n",
        "            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n",
        "            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
        "            anchors = [anchors[i] for i in anchor_idxs]\n",
        "            num_classes = int(module_def[\"classes\"])\n",
        "            img_size = int(hyperparams[\"height\"])\n",
        "            # Define detection layer\n",
        "            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n",
        "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
        "        # Register module list and number of output filters\n",
        "        module_list.append(modules)\n",
        "        output_filters.append(filters)  # 追踪每层的filter\n",
        "\n",
        "    return hyperparams, module_list\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor, mode=\"nearest\"):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmptyLayer(nn.Module):\n",
        "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EmptyLayer, self).__init__()\n",
        "\n",
        "\n",
        "class YOLOLayer(nn.Module):\n",
        "    \"\"\"Detection layer\"\"\"\n",
        "\n",
        "    def __init__(self, anchors, num_classes, img_dim=416):\n",
        "        super(YOLOLayer, self).__init__()\n",
        "        self.anchors = anchors\n",
        "        self.num_anchors = len(anchors)\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_thres = 0.5\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        self.obj_scale = 1\n",
        "        self.noobj_scale = 100\n",
        "        self.metrics = {}\n",
        "        self.img_dim = img_dim\n",
        "        self.grid_size = 0  # grid size\n",
        "\n",
        "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
        "        self.grid_size = grid_size\n",
        "        g = self.grid_size\n",
        "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "        self.stride = self.img_dim / self.grid_size\n",
        "        # Calculate offsets for each grid\n",
        "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
        "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
        "\n",
        "    def forward(self, x, targets=None, img_dim=None):\n",
        "\n",
        "        # Tensors for cuda support\n",
        "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
        "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
        "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
        "\n",
        "        self.img_dim = img_dim\n",
        "        num_samples = x.size(0)\n",
        "        grid_size = x.size(2)\n",
        "\n",
        "        prediction = (\n",
        "            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "        # Get outputs\n",
        "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
        "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
        "        w = prediction[..., 2]  # Width\n",
        "        h = prediction[..., 3]  # Height\n",
        "        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n",
        "        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
        "\n",
        "        # If grid size does not match current we compute new offsets\n",
        "        if grid_size != self.grid_size:\n",
        "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
        "\n",
        "        # Add offset and scale with anchors\n",
        "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
        "        pred_boxes[..., 0] = x.data + self.grid_x\n",
        "        pred_boxes[..., 1] = y.data + self.grid_y\n",
        "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n",
        "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n",
        "\n",
        "        output = torch.cat(\n",
        "            (\n",
        "                pred_boxes.view(num_samples, -1, 4) * self.stride,\n",
        "                pred_conf.view(num_samples, -1, 1),\n",
        "                pred_cls.view(num_samples, -1, self.num_classes),\n",
        "            ),\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "        if targets is None:\n",
        "            return output, 0\n",
        "        else:\n",
        "            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n",
        "                pred_boxes=pred_boxes,\n",
        "                pred_cls=pred_cls,\n",
        "                target=targets,\n",
        "                anchors=self.scaled_anchors,\n",
        "                ignore_thres=self.ignore_thres,\n",
        "            )\n",
        "\n",
        "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
        "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
        "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
        "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
        "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
        "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
        "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
        "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
        "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
        "            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
        "\n",
        "            # Metrics\n",
        "            cls_acc = 100 * class_mask[obj_mask].mean()\n",
        "            conf_obj = pred_conf[obj_mask].mean()\n",
        "            conf_noobj = pred_conf[noobj_mask].mean()\n",
        "            conf50 = (pred_conf > 0.5).float()\n",
        "            iou50 = (iou_scores > 0.5).float()\n",
        "            iou75 = (iou_scores > 0.75).float()\n",
        "            detected_mask = conf50 * class_mask * tconf\n",
        "            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n",
        "            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
        "            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
        "\n",
        "            self.metrics = {\n",
        "                \"loss\": to_cpu(total_loss).item(),\n",
        "                \"x\": to_cpu(loss_x).item(),\n",
        "                \"y\": to_cpu(loss_y).item(),\n",
        "                \"w\": to_cpu(loss_w).item(),\n",
        "                \"h\": to_cpu(loss_h).item(),\n",
        "                \"conf\": to_cpu(loss_conf).item(),\n",
        "                \"cls\": to_cpu(loss_cls).item(),\n",
        "                \"cls_acc\": to_cpu(cls_acc).item(),\n",
        "                \"recall50\": to_cpu(recall50).item(),\n",
        "                \"recall75\": to_cpu(recall75).item(),\n",
        "                \"precision\": to_cpu(precision).item(),\n",
        "                \"conf_obj\": to_cpu(conf_obj).item(),\n",
        "                \"conf_noobj\": to_cpu(conf_noobj).item(),\n",
        "                \"grid_size\": grid_size,\n",
        "            }\n",
        "\n",
        "            return output, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z90nxkl9LVQn",
        "colab_type": "text"
      },
      "source": [
        "## Darknet模型搭建\n",
        "<font face=楷体 color=skyblue size=4>加载权重</font>   \n",
        "<font face=楷体 color=skyblue>权重是如何存储：</font>  \n",
        "\n",
        "<font face=楷体>\n",
        "权重只属于<font color=skyblue>批量归一化层（batch norm layer）和卷积层</font>  两种类型的层，储存顺序和配置文件中定义层级的顺序完全相同  \n",
        "    \n",
        "下图展示了权重如何储存：\n",
        "</font>\n",
        "\n",
        "![替代文字](https://blog.paperspace.com/content/images/2018/04/wts-1.png)\n",
        "\n",
        "<font face=楷体>\n",
        "<font color=skyblue>Variable是tensor的外包装，data属性存储着tensor数据，grad属性存储关于该变量的导数，creator是代表该变量的创造者</font> \n",
        "\n",
        "<font face=楷体 color=green size=4>绿色链接</font>  \n",
        "【1】[Tensor and tensor.data](https://discuss.pytorch.org/t/tensor-and-tensor-data/18427)  \n",
        "【2】[autograd 及Variable](https://zhuanlan.zhihu.com/p/34298983)  \n",
        "【3】[What about .data?](https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#what-about-data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW7sAHb8ZBEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Darknet(nn.Module):\n",
        "    \"\"\"YOLOv3 object detection model\"\"\"\n",
        "\n",
        "    def __init__(self, config_path, img_size=416):\n",
        "        super(Darknet, self).__init__()\n",
        "        self.module_defs = parse_model_config(config_path)\n",
        "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
        "        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n",
        "        self.img_size = img_size\n",
        "        self.seen = 0\n",
        "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        img_dim = x.shape[2]\n",
        "        loss = 0\n",
        "        layer_outputs, yolo_outputs = [], []  # layer_outputs储存每一层的特征用于跳跃连接\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
        "                x = module(x)\n",
        "            elif module_def[\"type\"] == \"route\":\n",
        "                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
        "            elif module_def[\"type\"] == \"shortcut\":\n",
        "                layer_i = int(module_def[\"from\"])\n",
        "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
        "            elif module_def[\"type\"] == \"yolo\":\n",
        "                x, layer_loss = module[0](x, targets, img_dim)\n",
        "                loss += layer_loss\n",
        "                yolo_outputs.append(x)\n",
        "            layer_outputs.append(x)\n",
        "        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n",
        "        return yolo_outputs if targets is None else (loss, yolo_outputs)\n",
        "\n",
        "    def load_darknet_weights(self, weights_path):\n",
        "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
        "\n",
        "        # Open the weights file\n",
        "        with open(weights_path, \"rb\") as f:\n",
        "            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n",
        "            self.header_info = header  # Needed to write header when saving weights\n",
        "            self.seen = header[3]  # number of images seen during training\n",
        "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
        "\n",
        "        # Establish cutoff for loading backbone weights\n",
        "        cutoff = None\n",
        "        if \"darknet53.conv.74\" in weights_path:\n",
        "            cutoff = 75\n",
        "\n",
        "        ptr = 0\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if i == cutoff:\n",
        "                break\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    # Load BN bias, weights, running mean and running variance\n",
        "                    bn_layer = module[1]\n",
        "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
        "                    # Bias\n",
        "                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
        "                    bn_layer.bias.data.copy_(bn_b)\n",
        "                    ptr += num_b\n",
        "                    # Weight\n",
        "                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
        "                    bn_layer.weight.data.copy_(bn_w)\n",
        "                    ptr += num_b\n",
        "                    # Running Mean\n",
        "                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
        "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
        "                    ptr += num_b\n",
        "                    # Running Var\n",
        "                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
        "                    bn_layer.running_var.data.copy_(bn_rv)\n",
        "                    ptr += num_b\n",
        "                else:\n",
        "                    # Load conv. bias\n",
        "                    num_b = conv_layer.bias.numel()\n",
        "                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
        "                    conv_layer.bias.data.copy_(conv_b)\n",
        "                    ptr += num_b\n",
        "                # Load conv. weights\n",
        "                num_w = conv_layer.weight.numel()\n",
        "                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
        "                conv_layer.weight.data.copy_(conv_w)\n",
        "                ptr += num_w\n",
        "\n",
        "    def save_darknet_weights(self, path, cutoff=-1):\n",
        "        \"\"\"\n",
        "            @:param path    - path of the new weights file\n",
        "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
        "        \"\"\"\n",
        "        fp = open(path, \"wb\")\n",
        "        self.header_info[3] = self.seen\n",
        "        self.header_info.tofile(fp)\n",
        "\n",
        "        # Iterate through layers\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                # If batch norm, load bn first\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    bn_layer = module[1]\n",
        "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv bias\n",
        "                else:\n",
        "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv weights\n",
        "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "\n",
        "        fp.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbd75EJULeQQ",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "<font face=楷体 color=skyblue size=4> \n",
        "数据加载及增强\n",
        "</font>\n",
        "<font face=楷体>\n",
        "* horisontal_flip、pad_to_square、resize、random_resize\n",
        "*ImageFolder用于detect时加载数据\n",
        "*ListDataset用于训练时加载数据</font>\n",
        "\n",
        "<font face=楷体 color=skyblue size=4>\n",
        "OpenCV读取图片的数据形式与Pytorch读取的格式不同  \n",
        "$CV:\\color{pink} {RGB}\\color{yellow}\\Longrightarrow  Torch: \\color{pink}{BGR}\\ \\ \\ \\color{pink}{\\mathrm 【height \\times width \\times channel】\\Longrightarrow 【channel \\times height \\times width】}$\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwiH1dyqZ73x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def horisontal_flip(images, targets):\n",
        "    images = torch.flip(images, [-1])\n",
        "    targets[:, 2] = 1 - targets[:, 2]\n",
        "    return images, targets\n",
        "\n",
        "def pad_to_square(img, pad_value):\n",
        "    c, h, w = img.shape\n",
        "    dim_diff = np.abs(h - w)\n",
        "    # (upper / left) padding and (lower / right) padding\n",
        "    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
        "    # Determine padding\n",
        "    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n",
        "    # Add padding\n",
        "    img = F.pad(img, pad, \"constant\", value=pad_value)\n",
        "\n",
        "    return img, pad\n",
        "\n",
        "\n",
        "def resize(image, size):\n",
        "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_resize(images, min_size=288, max_size=448):\n",
        "    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n",
        "    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n",
        "    return images\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, folder_path, img_size=416):\n",
        "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[index % len(self.files)]\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path))\n",
        "        # Pad to square resolution\n",
        "        img, _ = pad_to_square(img, 0)\n",
        "        # Resize\n",
        "        img = resize(img, self.img_size)\n",
        "\n",
        "        return img_path, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, list_path, img_size=416, augment=True, multiscale=True, normalized_labels=True):\n",
        "        with open(list_path, \"r\") as file:\n",
        "            self.img_files = file.readlines()\n",
        "\n",
        "        self.label_files = [\n",
        "            path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n",
        "            for path in self.img_files\n",
        "        ]\n",
        "        self.img_size = img_size\n",
        "        self.max_objects = 100\n",
        "        self.augment = augment\n",
        "        self.multiscale = multiscale\n",
        "        self.normalized_labels = normalized_labels\n",
        "        self.min_size = self.img_size - 3 * 32\n",
        "        self.max_size = self.img_size + 3 * 32\n",
        "        self.batch_count = 0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # ---------\n",
        "        #  Image\n",
        "        # ---------\n",
        "\n",
        "        img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
        "\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "        # Handle images with less than three channels\n",
        "        if len(img.shape) != 3:\n",
        "            img = img.unsqueeze(0)\n",
        "            img = img.expand((3, img.shape[1:]))\n",
        "\n",
        "        _, h, w = img.shape\n",
        "        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n",
        "        # Pad to square resolution\n",
        "        img, pad = pad_to_square(img, 0)\n",
        "        _, padded_h, padded_w = img.shape\n",
        "\n",
        "        # ---------\n",
        "        #  Label\n",
        "        # ---------\n",
        "\n",
        "        label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
        "\n",
        "        targets = None\n",
        "        if os.path.exists(label_path):\n",
        "            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n",
        "            # Extract coordinates for unpadded + unscaled image\n",
        "            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n",
        "            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n",
        "            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n",
        "            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n",
        "            # Adjust for added padding\n",
        "            x1 += pad[0]\n",
        "            y1 += pad[2]\n",
        "            x2 += pad[1]\n",
        "            y2 += pad[3]\n",
        "            # Returns (x, y, w, h)\n",
        "            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n",
        "            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n",
        "            boxes[:, 3] *= w_factor / padded_w\n",
        "            boxes[:, 4] *= h_factor / padded_h\n",
        "\n",
        "            targets = torch.zeros((len(boxes), 6))\n",
        "            targets[:, 1:] = boxes\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.augment:\n",
        "            if np.random.random() < 0.5:\n",
        "                img, targets = horisontal_flip(img, targets)\n",
        "\n",
        "        return img_path, img, targets\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        paths, imgs, targets = list(zip(*batch))\n",
        "        # Remove empty placeholder targets\n",
        "        targets = [boxes for boxes in targets if boxes is not None]\n",
        "        # Add sample index to targets\n",
        "        for i, boxes in enumerate(targets):\n",
        "            boxes[:, 0] = i\n",
        "        targets = torch.cat(targets, 0)\n",
        "        # Selects new image size every tenth batch\n",
        "        if self.multiscale and self.batch_count % 10 == 0:\n",
        "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
        "        # Resize images to input shape\n",
        "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
        "        self.batch_count += 1\n",
        "        return paths, imgs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPKHILkMMftb",
        "colab_type": "text"
      },
      "source": [
        "# Tools\n",
        "<font face=楷体 color=skyblue size=4> \n",
        "工具函数\n",
        "</font>\n",
        "<font face=楷体>\n",
        "* to_cpu、load_classes加载类名、weights_init_normal初始化权重\n",
        "* rescale_boxes缩放目标框、xywh2xyxy转换框格式、bbox_wh_iou、bbox_iou计算IoU\n",
        "* ap_per_class、compute_ap、get_batch_statistics\n",
        "* non_max_suppression\n",
        "* build_targets\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ0LM0WjaAkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_cpu(tensor):\n",
        "    return tensor.detach().cpu()\n",
        "\n",
        "\n",
        "def load_classes(path):\n",
        "    \"\"\"\n",
        "    Loads class labels at 'path'\n",
        "    \"\"\"\n",
        "    fp = open(path, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def rescale_boxes(boxes, current_dim, original_shape):\n",
        "    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n",
        "    orig_h, orig_w = original_shape\n",
        "    # The amount of padding that was added\n",
        "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
        "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
        "    # Image height and width after padding is removed\n",
        "    unpad_h = current_dim - pad_y\n",
        "    unpad_w = current_dim - pad_x\n",
        "    # Rescale bounding boxes to dimension of original image\n",
        "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
        "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    y = x.new(x.shape)\n",
        "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
        "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
        "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
        "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
        "    return y\n",
        "\n",
        "\n",
        "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:    True positives (list).\n",
        "        conf:  Objectness value from 0-1 (list).\n",
        "        pred_cls: Predicted object classes (list).\n",
        "        target_cls: True object classes (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort by objectness\n",
        "    i = np.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes = np.unique(target_cls)\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    ap, p, r = [], [], []\n",
        "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
        "        i = pred_cls == c\n",
        "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
        "        n_p = i.sum()  # Number of predicted objects\n",
        "\n",
        "        if n_p == 0 and n_gt == 0:\n",
        "            continue\n",
        "        elif n_p == 0 or n_gt == 0:\n",
        "            ap.append(0)\n",
        "            r.append(0)\n",
        "            p.append(0)\n",
        "        else:\n",
        "            # Accumulate FPs and TPs\n",
        "            fpc = (1 - tp[i]).cumsum()\n",
        "            tpc = (tp[i]).cumsum()\n",
        "\n",
        "            # Recall\n",
        "            recall_curve = tpc / (n_gt + 1e-16)\n",
        "            r.append(recall_curve[-1])\n",
        "\n",
        "            # Precision\n",
        "            precision_curve = tpc / (tpc + fpc)\n",
        "            p.append(precision_curve[-1])\n",
        "\n",
        "            # AP from recall-precision curve\n",
        "            ap.append(compute_ap(recall_curve, precision_curve))\n",
        "\n",
        "    # Compute F1 score (harmonic mean of precision and recall)\n",
        "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
        "    f1 = 2 * p * r / (p + r + 1e-16)\n",
        "\n",
        "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def get_batch_statistics(outputs, targets, iou_threshold):\n",
        "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
        "    batch_metrics = []\n",
        "    for sample_i in range(len(outputs)):\n",
        "\n",
        "        if outputs[sample_i] is None:\n",
        "            continue\n",
        "\n",
        "        output = outputs[sample_i]\n",
        "        pred_boxes = output[:, :4]\n",
        "        pred_scores = output[:, 4]\n",
        "        pred_labels = output[:, -1]\n",
        "\n",
        "        true_positives = np.zeros(pred_boxes.shape[0])\n",
        "\n",
        "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
        "        target_labels = annotations[:, 0] if len(annotations) else []\n",
        "        if len(annotations):\n",
        "            detected_boxes = []\n",
        "            target_boxes = annotations[:, 1:]\n",
        "\n",
        "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
        "\n",
        "                # If targets are found break\n",
        "                if len(detected_boxes) == len(annotations):\n",
        "                    break\n",
        "\n",
        "                # Ignore if label is not one of the target labels\n",
        "                if pred_label not in target_labels:\n",
        "                    continue\n",
        "\n",
        "                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n",
        "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
        "                    true_positives[pred_i] = 1\n",
        "                    detected_boxes += [box_index]\n",
        "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
        "    return batch_metrics\n",
        "\n",
        "\n",
        "def bbox_wh_iou(wh1, wh2):\n",
        "    \"\"\"\n",
        "    计算\n",
        "    \"\"\"\n",
        "    wh2 = wh2.t()\n",
        "    w1, h1 = wh1[0], wh1[1]\n",
        "    w2, h2 = wh2[0], wh2[1]\n",
        "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
        "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "    \"\"\"\n",
        "    if not x1y1x2y2:\n",
        "        # Transform from center and width to exact coordinates\n",
        "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
        "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
        "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
        "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
        "    else:\n",
        "        # Get the coordinates of bounding boxes\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
        "\n",
        "    # get the corrdinates of the intersection rectangle\n",
        "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
        "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
        "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
        "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
        "    # Intersection area\n",
        "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
        "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
        "    )\n",
        "    # Union Area\n",
        "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
        "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
        "\n",
        "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
        "    \"\"\"\n",
        "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
        "    Non-Maximum Suppression to further filter detections.\n",
        "    Returns detections with shape:\n",
        "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
        "    \"\"\"\n",
        "\n",
        "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    for image_i, image_pred in enumerate(prediction):\n",
        "        # Filter out confidence scores below threshold\n",
        "        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n",
        "        # If none are remaining => process next image\n",
        "        if not image_pred.size(0):\n",
        "            continue\n",
        "        # Object confidence times class confidence\n",
        "        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n",
        "        # Sort by it\n",
        "        image_pred = image_pred[(-score).argsort()]\n",
        "        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n",
        "        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n",
        "        # Perform non-maximum suppression\n",
        "        keep_boxes = []\n",
        "        while detections.size(0):\n",
        "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
        "            label_match = detections[0, -1] == detections[:, -1]\n",
        "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
        "            invalid = large_overlap & label_match\n",
        "            weights = detections[invalid, 4:5]\n",
        "            # Merge overlapping bboxes by order of confidence\n",
        "            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n",
        "            keep_boxes += [detections[0]]\n",
        "            detections = detections[~invalid]\n",
        "        if keep_boxes:\n",
        "            output[image_i] = torch.stack(keep_boxes)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
        "\n",
        "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
        "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
        "\n",
        "    nB = pred_boxes.size(0)\n",
        "    nA = pred_boxes.size(1)\n",
        "    nC = pred_cls.size(-1)\n",
        "    nG = pred_boxes.size(2)\n",
        "\n",
        "    # Output tensors\n",
        "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
        "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
        "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
        "\n",
        "    # Convert to position relative to box\n",
        "    target_boxes = target[:, 2:6] * nG\n",
        "    gxy = target_boxes[:, :2]\n",
        "    gwh = target_boxes[:, 2:]\n",
        "    # Get anchors with best iou\n",
        "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
        "    best_ious, best_n = ious.max(0)\n",
        "    # Separate target values\n",
        "    b, target_labels = target[:, :2].long().t()\n",
        "    gx, gy = gxy.t()\n",
        "    gw, gh = gwh.t()\n",
        "    gi, gj = gxy.long().t()\n",
        "    # Set masks\n",
        "    obj_mask[b, best_n, gj, gi] = 1\n",
        "    noobj_mask[b, best_n, gj, gi] = 0\n",
        "\n",
        "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
        "    for i, anchor_ious in enumerate(ious.t()):\n",
        "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
        "\n",
        "    # Coordinates\n",
        "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
        "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
        "    # Width and height\n",
        "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
        "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
        "    # One-hot encoding of label\n",
        "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
        "    # Compute label correctness and iou at best anchor\n",
        "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
        "    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
        "\n",
        "    tconf = obj_mask.float()\n",
        "    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRC1BocFMs5v",
        "colab_type": "text"
      },
      "source": [
        "# 训练及评价\n",
        "\n",
        "<font face=楷体>\n",
        "\n",
        "* 训练参数设定\n",
        "* 加载数据\n",
        "* 评价及测试\n",
        "* 训练\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnC-uBY0M45_",
        "colab_type": "text"
      },
      "source": [
        "## 训练参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09KXocIywwcn",
        "colab_type": "text"
      },
      "source": [
        "执行`!bash custom.sh`得到：\n",
        "- 模型的配置参数\n",
        "- 训练数据（一条，coco数据集太大）\n",
        "- 预训练权重\n",
        "\n",
        "<img src=\"https://img-blog.csdnimg.cn/20200825120641751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzcxMTU1NA==,size_16,color_FFFFFF,t_70#pic_center\">\n",
        "\n",
        "设置训练参数如下：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hAFP8aYZjC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ArgumentParser():\n",
        "    epochs=10 \n",
        "    batch_size=1\n",
        "    gradient_accumulations=1\n",
        "    model_def=\"config/yolov3-custom.cfg\"\n",
        "    data_config=\"config/custom.data\"\n",
        "    pretrained_weights=\"weights/darknet53.conv.74\"\n",
        "    n_cpu=4\n",
        "    img_size=416\n",
        "    checkpoint_interval=1\n",
        "    evaluation_interval=1\n",
        "    compute_map=False\n",
        "    multiscale_training=True\n",
        "opt = ArgumentParser()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1vKGlSkNDHe",
        "colab_type": "text"
      },
      "source": [
        "## 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mKokwvyZqJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Get data configuration\n",
        "data_config = parse_data_config(opt.data_config)\n",
        "train_path = data_config[\"train\"]\n",
        "valid_path = data_config[\"valid\"]\n",
        "class_names = load_classes(data_config[\"names\"])\n",
        "\n",
        "# Initiate model\n",
        "model = Darknet(opt.model_def).to(device)\n",
        "model.apply(weights_init_normal)\n",
        "\n",
        "# If specified we start from checkpoint\n",
        "if opt.pretrained_weights:\n",
        "    if opt.pretrained_weights.endswith(\".pth\"):\n",
        "        model.load_state_dict(torch.load(opt.pretrained_weights))\n",
        "    else:\n",
        "        model.load_darknet_weights(opt.pretrained_weights)\n",
        "\n",
        "# Get dataloader\n",
        "dataset = ListDataset(train_path, augment=True, multiscale=opt.multiscale_training)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=opt.n_cpu,\n",
        "    pin_memory=True,\n",
        "    collate_fn=dataset.collate_fn,\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "metrics = [\n",
        "        \"grid_size\",\n",
        "        \"loss\",\n",
        "        \"x\",\n",
        "        \"y\",\n",
        "        \"w\",\n",
        "        \"h\",\n",
        "        \"conf\",\n",
        "        \"cls\",\n",
        "        \"cls_acc\",\n",
        "        \"recall50\",\n",
        "        \"recall75\",\n",
        "        \"precision\",\n",
        "        \"conf_obj\",\n",
        "        \"conf_noobj\",\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KkO6ozHMy3H",
        "colab_type": "text"
      },
      "source": [
        "## 评价及测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tCyuUh8ZDcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, path, iou_thres, conf_thres, nms_thres, img_size, batch_size):\n",
        "    model.eval()\n",
        "\n",
        "    # Get dataloader\n",
        "    dataset = ListDataset(path, img_size=img_size, augment=False, multiscale=False)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "    labels = []\n",
        "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
        "    for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=\"Detecting objects\")):\n",
        "\n",
        "        # Extract labels\n",
        "        labels += targets[:, 1].tolist()\n",
        "        # Rescale target\n",
        "        targets[:, 2:] = xywh2xyxy(targets[:, 2:])\n",
        "        targets[:, 2:] *= img_size\n",
        "\n",
        "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n",
        "\n",
        "        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n",
        "\n",
        "    # Concatenate sample statistics\n",
        "    true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
        "    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
        "\n",
        "    return precision, recall, AP, f1, ap_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Aiwnw3NHYP",
        "colab_type": "text"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbuCNEU5Zq-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5581
        },
        "outputId": "7c028ce7-ba9d-4435-e5cc-849b19775b7d"
      },
      "source": [
        "for epoch in range(opt.epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n",
        "        batches_done = len(dataloader) * epoch + batch_i\n",
        "\n",
        "        imgs = Variable(imgs.to(device))\n",
        "        targets = Variable(targets.to(device), requires_grad=False)\n",
        "\n",
        "        loss, outputs = model(imgs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        if batches_done % opt.gradient_accumulations:\n",
        "            # Accumulates gradient before each step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ----------------\n",
        "            #   Log progress\n",
        "            # ----------------\n",
        "\n",
        "        log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, opt.epochs, batch_i, len(dataloader))\n",
        "\n",
        "        metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n",
        "\n",
        "        # Log metrics at each YOLO layer\n",
        "        for i, metric in enumerate(metrics):\n",
        "            formats = {m: \"%.6f\" for m in metrics}\n",
        "            formats[\"grid_size\"] = \"%2d\"\n",
        "            formats[\"cls_acc\"] = \"%.2f%%\"\n",
        "            row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n",
        "            metric_table += [[metric, *row_metrics]]\n",
        "\n",
        "            # Tensorboard logging\n",
        "            tensorboard_log = []\n",
        "            for j, yolo in enumerate(model.yolo_layers):\n",
        "                for name, metric in yolo.metrics.items():\n",
        "                    if name != \"grid_size\":\n",
        "                        tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n",
        "            tensorboard_log += [(\"loss\", loss.item())]\n",
        "            #logger.list_of_scalars_summary(tensorboard_log, batches_done)\n",
        "\n",
        "        log_str += AsciiTable(metric_table).table\n",
        "        log_str += f\"\\nTotal loss {loss.item()}\"\n",
        "\n",
        "        # Determine approximate time left for epoch\n",
        "        epoch_batches_left = len(dataloader) - (batch_i + 1)\n",
        "        time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n",
        "        log_str += f\"\\n---- ETA {time_left}\"\n",
        "\n",
        "        print(log_str)\n",
        "\n",
        "        model.seen += imgs.size(0)\n",
        "\n",
        "    if epoch % opt.evaluation_interval == 0:\n",
        "        print(\"\\n---- Evaluating Model ----\")\n",
        "        # Evaluate the model on the validation set\n",
        "        precision, recall, AP, f1, ap_class = evaluate(\n",
        "                model,\n",
        "                path=valid_path,\n",
        "                iou_thres=0.5,\n",
        "                conf_thres=0.5,\n",
        "                nms_thres=0.5,\n",
        "                img_size=opt.img_size,\n",
        "                batch_size=1,     # 8 \n",
        "            )\n",
        "        evaluation_metrics = [\n",
        "                (\"val_precision\", precision.mean()),\n",
        "                (\"val_recall\", recall.mean()),\n",
        "                (\"val_mAP\", AP.mean()),\n",
        "                (\"val_f1\", f1.mean()),\n",
        "            ]\n",
        "        #logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "        # Print class APs and mAP\n",
        "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "        for i, c in enumerate(ap_class):\n",
        "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "        print(AsciiTable(ap_table).table)\n",
        "        print(f\"---- mAP {AP.mean()}\")\n",
        "\n",
        "    if epoch % opt.checkpoint_interval == 0:\n",
        "        torch.save(model.state_dict(), f\"checkpoints/yolov3_ckpt_%d.pth\" % epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:177: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:179: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:180: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:181: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:182: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:184: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:188: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:189: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:190: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "Detecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 0/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 12           | 24           | 48           |\n",
            "| loss       | 72.647026    | 71.010567    | 72.288361    |\n",
            "| x          | 0.108219     | 0.019106     | 0.014467     |\n",
            "| y          | 0.186920     | 0.589454     | 0.185607     |\n",
            "| w          | 1.362979     | 0.004174     | 0.941699     |\n",
            "| h          | 0.041918     | 1.374814     | 0.511466     |\n",
            "| conf       | 70.142967    | 68.219780    | 69.841225    |\n",
            "| cls        | 0.804023     | 0.803242     | 0.793898     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.356896     | 0.659352     | 0.438408     |\n",
            "| conf_noobj | 0.489510     | 0.485434     | 0.496018     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 215.94595336914062\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 82.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.10000 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 1/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 14           | 28           | 56           |\n",
            "| loss       | 72.034126    | 71.073410    | 73.163864    |\n",
            "| x          | 0.091053     | 0.066212     | 0.000006     |\n",
            "| y          | 0.269291     | 0.163544     | 0.199841     |\n",
            "| w          | 0.015488     | 0.570178     | 0.884946     |\n",
            "| h          | 0.360436     | 0.366304     | 1.585229     |\n",
            "| conf       | 70.585945    | 69.028633    | 69.660080    |\n",
            "| cls        | 0.711916     | 0.878537     | 0.833764     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.302306     | 0.339319     | 0.538292     |\n",
            "| conf_noobj | 0.489656     | 0.486047     | 0.496156     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 216.2714080810547\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 94.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00000 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 2/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 16           | 32           | 64           |\n",
            "| loss       | 73.274475    | 69.631813    | 72.114555    |\n",
            "| x          | 0.231517     | 0.020465     | 0.008029     |\n",
            "| y          | 0.311226     | 0.112611     | 0.290886     |\n",
            "| w          | 0.095395     | 0.228665     | 0.872532     |\n",
            "| h          | 1.836784     | 0.249352     | 0.493495     |\n",
            "| conf       | 70.315620    | 68.746239    | 69.735420    |\n",
            "| cls        | 0.483931     | 0.274482     | 0.714196     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.382197     | 0.443208     | 0.517887     |\n",
            "| conf_noobj | 0.490120     | 0.485958     | 0.496231     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 215.02084350585938\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 94.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00042 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.0004182350480970305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 3/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 16           | 32           | 64           |\n",
            "| loss       | 73.274475    | 69.631813    | 72.114555    |\n",
            "| x          | 0.231517     | 0.020465     | 0.008029     |\n",
            "| y          | 0.311226     | 0.112611     | 0.290886     |\n",
            "| w          | 0.095395     | 0.228665     | 0.872532     |\n",
            "| h          | 1.836784     | 0.249352     | 0.493495     |\n",
            "| conf       | 70.315620    | 68.746239    | 69.735420    |\n",
            "| cls        | 0.483931     | 0.274482     | 0.714196     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.382197     | 0.443208     | 0.517887     |\n",
            "| conf_noobj | 0.490120     | 0.485958     | 0.496231     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 215.02084350585938\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 64.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00042 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.00042319085907744394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 4/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 12           | 24           | 48           |\n",
            "| loss       | 72.647026    | 71.010567    | 72.288361    |\n",
            "| x          | 0.108219     | 0.019106     | 0.014467     |\n",
            "| y          | 0.186920     | 0.589454     | 0.185607     |\n",
            "| w          | 1.362979     | 0.004174     | 0.941699     |\n",
            "| h          | 0.041918     | 1.374814     | 0.511466     |\n",
            "| conf       | 70.142967    | 68.219780    | 69.841225    |\n",
            "| cls        | 0.804023     | 0.803242     | 0.793898     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.356896     | 0.659352     | 0.438408     |\n",
            "| conf_noobj | 0.489510     | 0.485434     | 0.496018     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 215.94595336914062\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 96.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00151 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.0015060240963855422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 5/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 15           | 30           | 60           |\n",
            "| loss       | 70.758316    | 70.784439    | 72.849770    |\n",
            "| x          | 0.024519     | 0.036519     | 0.019899     |\n",
            "| y          | 0.005948     | 0.133712     | 0.199938     |\n",
            "| w          | 0.427413     | 1.003079     | 0.914330     |\n",
            "| h          | 0.000093     | 0.156783     | 0.969389     |\n",
            "| conf       | 69.845985    | 68.671471    | 69.674767    |\n",
            "| cls        | 0.454359     | 0.782877     | 1.071451     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.391038     | 0.474979     | 0.546412     |\n",
            "| conf_noobj | 0.487880     | 0.485819     | 0.496232     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 214.39251708984375\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 108.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00056 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.0005583472920156337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 6/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 13           | 26           | 52           |\n",
            "| loss       | 69.875130    | 71.475243    | 73.496826    |\n",
            "| x          | 0.011834     | 0.091738     | 0.012279     |\n",
            "| y          | 0.033717     | 0.223053     | 0.285174     |\n",
            "| w          | 0.011008     | 1.153710     | 1.182045     |\n",
            "| h          | 0.049409     | 0.162347     | 0.871814     |\n",
            "| conf       | 69.304710    | 69.312187    | 69.957603    |\n",
            "| cls        | 0.464454     | 0.532208     | 1.187914     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 1.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 1.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.004525     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.733377     | 0.243713     | 0.411922     |\n",
            "| conf_noobj | 0.488212     | 0.485878     | 0.496285     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 214.84719848632812\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 102.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.16667 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.16666666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 7/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 14           | 28           | 56           |\n",
            "| loss       | 72.034126    | 71.073410    | 73.163864    |\n",
            "| x          | 0.091053     | 0.066212     | 0.000006     |\n",
            "| y          | 0.269291     | 0.163544     | 0.199841     |\n",
            "| w          | 0.015488     | 0.570178     | 0.884946     |\n",
            "| h          | 0.360436     | 0.366304     | 1.585229     |\n",
            "| conf       | 70.585945    | 69.028633    | 69.660080    |\n",
            "| cls        | 0.711916     | 0.878537     | 0.833764     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.302306     | 0.339319     | 0.538292     |\n",
            "| conf_noobj | 0.489656     | 0.486047     | 0.496156     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 216.2714080810547\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 100.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00100 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.001001001001001001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 8/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 16           | 32           | 64           |\n",
            "| loss       | 73.274475    | 69.631813    | 72.114555    |\n",
            "| x          | 0.231517     | 0.020465     | 0.008029     |\n",
            "| y          | 0.311226     | 0.112611     | 0.290886     |\n",
            "| w          | 0.095395     | 0.228665     | 0.872532     |\n",
            "| h          | 1.836784     | 0.249352     | 0.493495     |\n",
            "| conf       | 70.315620    | 68.746239    | 69.735420    |\n",
            "| cls        | 0.483931     | 0.274482     | 0.714196     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.382197     | 0.443208     | 0.517887     |\n",
            "| conf_noobj | 0.490120     | 0.485958     | 0.496231     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 215.02084350585938\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 72.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.00038 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.00038491147036181676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rDetecting objects:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- [Epoch 9/10, Batch 0/1] ----\n",
            "+------------+--------------+--------------+--------------+\n",
            "| Metrics    | YOLO Layer 0 | YOLO Layer 1 | YOLO Layer 2 |\n",
            "+------------+--------------+--------------+--------------+\n",
            "| grid_size  | 11           | 22           | 44           |\n",
            "| loss       | 70.541100    | 71.679482    | 71.863632    |\n",
            "| x          | 0.002614     | 0.072131     | 0.034565     |\n",
            "| y          | 0.011255     | 0.387840     | 0.158717     |\n",
            "| w          | 0.003229     | 0.033954     | 0.349912     |\n",
            "| h          | 0.122493     | 1.814953     | 0.661173     |\n",
            "| conf       | 69.918770    | 68.722076    | 69.701157    |\n",
            "| cls        | 0.482742     | 0.648532     | 0.958108     |\n",
            "| cls_acc    | 100.00%      | 100.00%      | 100.00%      |\n",
            "| recall50   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| recall75   | 0.000000     | 0.000000     | 0.000000     |\n",
            "| precision  | 0.000000     | 0.000000     | 0.000000     |\n",
            "| conf_obj   | 0.425293     | 0.433686     | 0.491383     |\n",
            "| conf_noobj | 0.488665     | 0.485813     | 0.495976     |\n",
            "+------------+--------------+--------------+--------------+\n",
            "Total loss 214.08421325683594\n",
            "---- ETA 0:00:00\n",
            "\n",
            "---- Evaluating Model ----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
            "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 96.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------+\n",
            "| Index | Class name | AP      |\n",
            "+-------+------------+---------+\n",
            "| 0     | train      | 0.14286 |\n",
            "+-------+------------+---------+\n",
            "---- mAP 0.14285714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}