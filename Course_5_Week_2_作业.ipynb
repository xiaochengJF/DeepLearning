{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 5 - Week 2 作业",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochengJF/DeepLearning/blob/master/Course_5_Week_2_%E4%BD%9C%E4%B8%9A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "W5ppbBDip-zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "dc056581-3779-451b-9462-2b3059d9316e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AvE3od_UqNrb",
        "colab_type": "code",
        "outputId": "413ec80b-2be4-4fff-9120-c84c8e772e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/My Drive/DL-GPU/DeepLearning/Course 5 - Week 2 作业/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/DL-GPU/DeepLearning/Course 5 - Week 2 作业\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NUzQ6JY0ql7D",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title 默认标题文本\n",
        "import numpy as np\n",
        "import w2v_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IjYCAy7q18K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words, word_to_vec_map = w2v_utils.read_glove_vecs('data/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cDSCXny9q5cK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "2c734f68-0aea-433c-8f39-51ba55e6b5c7"
      },
      "cell_type": "code",
      "source": [
        "# python 3.x\n",
        "print(word_to_vec_map['hello'])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
            " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
            "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
            "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
            "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
            " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
            " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
            "  0.67204 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qa3SFQ2VOB0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    u与v的余弦相似度反映了u与v的相似程度\n",
        "    \n",
        "    参数：\n",
        "        u -- 维度为(n,)的词向量\n",
        "        v -- 维度为(n,)的词向量\n",
        "        \n",
        "    返回：\n",
        "        cosine_similarity -- 由上面公式定义的u和v之间的余弦相似度。\n",
        "    \"\"\"\n",
        "    distance = 0\n",
        "    \n",
        "    # 计算u与v的内积\n",
        "    dot = np.dot(u, v)\n",
        "    \n",
        "    #计算u的L2范数\n",
        "    norm_u = np.sqrt(np.sum(np.power(u, 2)))\n",
        "    \n",
        "    #计算v的L2范数\n",
        "    norm_v = np.sqrt(np.sum(np.power(v, 2)))\n",
        "    \n",
        "    # 根据公式1计算余弦相似度\n",
        "    cosine_similarity = np.divide(dot, norm_u * norm_v)\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S5QW2xaZOPj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "43a924b0-2702-45ad-c341-e163b36a6369"
      },
      "cell_type": "code",
      "source": [
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "ball = word_to_vec_map[\"ball\"]\n",
        "crocodile = word_to_vec_map[\"crocodile\"]\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n",
        "rome = word_to_vec_map[\"rome\"]\n",
        "\n",
        "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
        "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
        "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine_similarity(father, mother) =  0.8909038442893615\n",
            "cosine_similarity(ball, crocodile) =  0.2743924626137942\n",
            "cosine_similarity(france - paris, rome - italy) =  -0.6751479308174201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aDCpfIdFOSYH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    解决“A与B相比就类似于C与____相比一样”之类的问题\n",
        "    \n",
        "    参数：\n",
        "        word_a -- 一个字符串类型的词\n",
        "        word_b -- 一个字符串类型的词\n",
        "        word_c -- 一个字符串类型的词\n",
        "        word_to_vec_map -- 字典类型，单词到GloVe向量的映射\n",
        "        \n",
        "    返回：\n",
        "        best_word -- 满足(v_b - v_a) 最接近 (v_best_word - v_c) 的词\n",
        "    \"\"\"\n",
        "    \n",
        "    # 把单词转换为小写\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    \n",
        "    # 获取对应单词的词向量\n",
        "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
        "    \n",
        "    # 获取全部的单词\n",
        "    words = word_to_vec_map.keys()\n",
        "    \n",
        "    # 将max_cosine_sim初始化为一个比较大的负数\n",
        "    max_cosine_sim = -100\n",
        "    best_word = None\n",
        "    \n",
        "    # 遍历整个数据集\n",
        "    for word in words:\n",
        "        # 要避免匹配到输入的数据\n",
        "        if word in [word_a, word_b, word_c]:\n",
        "            continue\n",
        "        # 计算余弦相似度\n",
        "        cosine_sim = cosine_similarity((e_b - e_a), (word_to_vec_map[word] - e_c))\n",
        "        \n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = word\n",
        "            \n",
        "    return best_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rkEDcFmLOW-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c32d67f6-1a30-49a3-ec81-4afb5ba49b70"
      },
      "cell_type": "code",
      "source": [
        "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
        "for triad in triads_to_try:\n",
        "    print ('{} -> {} <====> {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "italy -> italian <====> spain -> spanish\n",
            "india -> delhi <====> japan -> tokyo\n",
            "man -> woman <====> boy -> girl\n",
            "small -> smaller <====> large -> larger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ca2mGnmUObWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31a841fa-a81d-4545-81df-a5698d6782e1"
      },
      "cell_type": "code",
      "source": [
        "triads_to_try = [('small', 'smaller', 'big')]\n",
        "for triad in triads_to_try:\n",
        "    print ('{} -> {} <====> {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small -> smaller <====> big -> competitors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o5A23vmROkfw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "4677b422-cb86-4fe1-d1a4-0e84fdde26ad"
      },
      "cell_type": "code",
      "source": [
        "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
        "print(g)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
            " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
            "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
            "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
            "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
            " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
            " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
            "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
            " -0.04371     0.01258   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gGpCuPqaOrvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "f9eb7120-e97b-48e4-ce71-27ebf0bb5cf2"
      },
      "cell_type": "code",
      "source": [
        "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
        "\n",
        "for w in name_list:\n",
        "    print (w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "john -0.23163356145973724\n",
            "marie 0.315597935396073\n",
            "sophie 0.31868789859418784\n",
            "ronaldo -0.31244796850329437\n",
            "priya 0.17632041839009402\n",
            "rahul -0.16915471039231716\n",
            "danielle 0.24393299216283895\n",
            "reza -0.07930429672199553\n",
            "katy 0.2831068659572615\n",
            "yasmin 0.23313857767928758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "464eS22yOxno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "b31fe26a-0229-4566-895e-e9fc9cf70ccc"
      },
      "cell_type": "code",
      "source": [
        "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
        "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
        "for w in word_list:\n",
        "    print (w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lipstick 0.2769191625638267\n",
            "guns -0.1888485567898898\n",
            "science -0.06082906540929701\n",
            "arts 0.008189312385880337\n",
            "literature 0.06472504433459932\n",
            "warrior -0.20920164641125288\n",
            "doctor 0.11895289410935041\n",
            "tree -0.07089399175478091\n",
            "receptionist 0.33077941750593737\n",
            "technology -0.13193732447554302\n",
            "fashion 0.03563894625772699\n",
            "teacher 0.17920923431825664\n",
            "engineer -0.0803928049452407\n",
            "pilot 0.0010764498991916937\n",
            "computer -0.10330358873850498\n",
            "singer 0.1850051813649629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gmtd_q1JO03f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neutralize(word, g, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    通过将“word”投影到与偏置轴正交的空间上，消除了“word”的偏差。\n",
        "    该函数确保“word”在性别的子空间中的值为0\n",
        "    \n",
        "    参数：\n",
        "        word -- 待消除偏差的字符串\n",
        "        g -- 维度为(50,)，对应于偏置轴（如性别）\n",
        "        word_to_vec_map -- 字典类型，单词到GloVe向量的映射\n",
        "        \n",
        "    返回：\n",
        "        e_debiased -- 消除了偏差的向量。\n",
        "    \"\"\"\n",
        "    \n",
        "    # 根据word选择对应的词向量\n",
        "    e = word_to_vec_map[word]\n",
        "    \n",
        "    # 根据公式2计算e_biascomponent\n",
        "    e_biascomponent = np.divide(np.dot(e, g), np.square(np.linalg.norm(g))) * g\n",
        "    \n",
        "    # 根据公式3计算e_debiased\n",
        "    e_debiased = e - e_biascomponent\n",
        "    \n",
        "    return e_debiased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8yaxMUTO4UH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8e32af9e-334a-454d-a7f4-e530460792f7"
      },
      "cell_type": "code",
      "source": [
        "e = \"receptionist\"\n",
        "print(\"去偏差前{0}与g的余弦相似度为：{1}\".format(e, cosine_similarity(word_to_vec_map[\"receptionist\"], g)))\n",
        "\n",
        "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
        "print(\"去偏差后{0}与g的余弦相似度为：{1}\".format(e, cosine_similarity(e_debiased, g)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "去偏差前receptionist与g的余弦相似度为：0.33077941750593737\n",
            "去偏差后receptionist与g的余弦相似度为：-2.099120994400013e-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6vRYLeD5O7GH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def equalize(pair, bias_axis, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    通过遵循上图中所描述的均衡方法来消除性别偏差。\n",
        "    \n",
        "    参数：\n",
        "        pair -- 要消除性别偏差的词组，比如 (\"actress\", \"actor\") \n",
        "        bias_axis -- 维度为(50,)，对应于偏置轴（如性别）\n",
        "        word_to_vec_map -- 字典类型，单词到GloVe向量的映射\n",
        "    \n",
        "    返回：\n",
        "        e_1 -- 第一个词的词向量\n",
        "        e_2 -- 第二个词的词向量\n",
        "    \"\"\"\n",
        "    # 第1步：获取词向量\n",
        "    w1, w2 = pair\n",
        "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
        "    \n",
        "    # 第2步：计算w1与w2的均值\n",
        "    mu = (e_w1 + e_w2) / 2.0\n",
        "    \n",
        "    # 第3步：计算mu在偏置轴与正交轴上的投影\n",
        "    mu_B = np.divide(np.dot(mu, bias_axis), np.square(np.linalg.norm(bias_axis))) * bias_axis\n",
        "    mu_orth = mu - mu_B\n",
        "    \n",
        "    # 第4步：使用公式7、8计算e_w1B 与 e_w2B\n",
        "    e_w1B = np.divide(np.dot(e_w1, bias_axis), np.square(np.linalg.norm(bias_axis))) * bias_axis\n",
        "    e_w2B = np.divide(np.dot(e_w2, bias_axis), np.square(np.linalg.norm(bias_axis))) * bias_axis\n",
        "    \n",
        "    # 第5步：根据公式9、10调整e_w1B 与 e_w2B的偏置部分\n",
        "    corrected_e_w1B = np.sqrt(np.abs(1-np.square(np.linalg.norm(mu_orth)))) * np.divide(e_w1B-mu_B, np.abs(e_w1 - mu_orth - mu_B))\n",
        "    corrected_e_w2B = np.sqrt(np.abs(1-np.square(np.linalg.norm(mu_orth)))) * np.divide(e_w2B-mu_B, np.abs(e_w2 - mu_orth - mu_B))\n",
        "    \n",
        "    # 第6步： 使e1和e2等于它们修正后的投影之和，从而消除偏差\n",
        "    e1 = corrected_e_w1B + mu_orth\n",
        "    e2 = corrected_e_w2B + mu_orth\n",
        "    \n",
        "    return e1, e2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CqfMRdAZO-6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "08b6edb2-1f07-4f22-aae5-8de04765f4e9"
      },
      "cell_type": "code",
      "source": [
        "print(\"==========均衡校正前==========\")\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
        "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "print(\"\\n==========均衡校正后==========\")\n",
        "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
        "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========均衡校正前==========\n",
            "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.11711095765336832\n",
            "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.35666618846270376\n",
            "\n",
            "==========均衡校正后==========\n",
            "cosine_similarity(e1, gender) =  -0.7165727525843935\n",
            "cosine_similarity(e2, gender) =  0.7396596474928909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kdQov9NfPaJf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39c0eadb-746f-4a6a-e8ac-f29581de95bc"
      },
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dFJYu3nEPBJx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import emo_utils\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1O9nnO-PGJm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, Y_train = emo_utils.read_csv('data/train_emoji.csv')\n",
        "X_test, Y_test = emo_utils.read_csv('data/test.csv')\n",
        "\n",
        "max_Len = len(max(X_train, key=len).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E6sDcNdQPqe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af001764-9fc7-4fc0-eb58-eb2458a3ceab"
      },
      "cell_type": "code",
      "source": [
        "index  = 3\n",
        "print(X_train[index], emo_utils.label_to_emoji(Y_train[index]))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Miss you so much ❤️\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQpiIgPcPtGv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_oh_train = emo_utils.convert_to_one_hot(Y_train, C=5)\n",
        "Y_oh_test = emo_utils.convert_to_one_hot(Y_test, C=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6ZaCS79PzsW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5047a842-db93-4f97-8644-dbbff2c15e1b"
      },
      "cell_type": "code",
      "source": [
        "index = 0 \n",
        "print(\"{0}对应的独热编码是{1}\".format(Y_train[index], Y_oh_train[index]))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3对应的独热编码是[0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TvooHe1hP2a7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = emo_utils.read_glove_vecs('data/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4S6SbY0P7QO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "63c3ffab-7113-443b-edcc-1f7a1321ace7"
      },
      "cell_type": "code",
      "source": [
        "word = \"cucumber\"\n",
        "index = 113317\n",
        "print(\"单词{0}对应的索引是：{1}\".format(word, word_to_index[word]))\n",
        "print(\"索引{0}对应的单词是：{1}\".format(index, index_to_word[index]))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "单词cucumber对应的索引是：113317\n",
            "索引113317对应的单词是：cucumber\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0HxCf8XP-dP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    将句子转换为单词列表，提取其GloVe向量，然后将其平均。\n",
        "    \n",
        "    参数：\n",
        "        sentence -- 字符串类型，从X中获取的样本。\n",
        "        word_to_vec_map -- 字典类型，单词映射到50维的向量的字典\n",
        "        \n",
        "    返回：\n",
        "        avg -- 对句子的均值编码，维度为(50,)\n",
        "    \"\"\"\n",
        "    \n",
        "    # 第一步：分割句子，转换为列表。\n",
        "    words = sentence.lower().split()\n",
        "    \n",
        "    # 初始化均值词向量\n",
        "    avg = np.zeros(50,)\n",
        "    \n",
        "    # 第二步：对词向量取平均。\n",
        "    for w in words:\n",
        "        avg += word_to_vec_map[w]\n",
        "    avg = np.divide(avg, len(words))\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yNHQfwaSQCx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b85f4453-fc82-4b96-de17-de58bb74d415"
      },
      "cell_type": "code",
      "source": [
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "print(\"avg = \", avg)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
            " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
            "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
            "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
            "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
            "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
            " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
            " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
            "  0.1445417   0.09808667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eHG1y8IFQFe_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X, Y, word_to_vec_map, learning_rate=0.01, num_iterations=400):\n",
        "    \"\"\"\n",
        "    在numpy中训练词向量模型。\n",
        "    \n",
        "    参数：\n",
        "        X -- 输入的字符串类型的数据，维度为(m, 1)。\n",
        "        Y -- 对应的标签，0-7的数组，维度为(m, 1)。\n",
        "        word_to_vec_map -- 字典类型的单词到50维词向量的映射。\n",
        "        learning_rate -- 学习率.\n",
        "        num_iterations -- 迭代次数。\n",
        "        \n",
        "    返回：\n",
        "        pred -- 预测的向量，维度为(m, 1)。\n",
        "        W -- 权重参数，维度为(n_y, n_h)。\n",
        "        b -- 偏置参数，维度为(n_y,)\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    \n",
        "    # 定义训练数量\n",
        "    m = Y.shape[0]\n",
        "    n_y = 5\n",
        "    n_h = 50\n",
        "    \n",
        "    # 使用Xavier初始化参数\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # 将Y转换成独热编码\n",
        "    Y_oh = emo_utils.convert_to_one_hot(Y, C=n_y)\n",
        "    \n",
        "    # 优化循环\n",
        "    for t in range(num_iterations):\n",
        "        for i in range(m):\n",
        "            # 获取第i个训练样本的均值\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "            \n",
        "            # 前向传播\n",
        "            z = np.dot(W, avg) + b\n",
        "            a = emo_utils.softmax(z)\n",
        "            \n",
        "            # 计算第i个训练的损失\n",
        "            cost = -np.sum(Y_oh[i]*np.log(a))\n",
        "            \n",
        "            # 计算梯度\n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "            \n",
        "            # 更新参数\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        if t % 100 == 0:\n",
        "            print(\"第{t}轮，损失为{cost}\".format(t=t,cost=cost))\n",
        "            pred = emo_utils.predict(X, Y, W, b, word_to_vec_map)\n",
        "            \n",
        "    return pred, W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LqfsQBGDQIwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a2dd821a-9ed8-4daa-df46-87ee184421f5"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(X_train[0])\n",
        "print(type(X_train))\n",
        "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
        "print(Y.shape)\n",
        "\n",
        "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
        " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
        " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
        " 'You totally deserve this prize', 'Let us go play football',\n",
        " 'Are you down for football this afternoon', 'Work hard play harder',\n",
        " 'It is suprising how people can be dumb sometimes',\n",
        " 'I am very disappointed','It is the best day in my life',\n",
        " 'I think I will end up alone','My life is so boring','Good job',\n",
        " 'Great so awesome'])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(132,)\n",
            "(132,)\n",
            "(132, 5)\n",
            "never talk to me again\n",
            "<class 'numpy.ndarray'>\n",
            "(20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8IfrVw-EQLvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "13655304-b38e-404f-b7d9-d9ee9ca54562"
      },
      "cell_type": "code",
      "source": [
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第0轮，损失为1.9520498812810076\n",
            "Accuracy: 0.3484848484848485\n",
            "第100轮，损失为0.07971818726014794\n",
            "Accuracy: 0.9318181818181818\n",
            "第200轮，损失为0.04456369243681402\n",
            "Accuracy: 0.9545454545454546\n",
            "第300轮，损失为0.03432267378786059\n",
            "Accuracy: 0.9696969696969697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bA4XvfUcQPmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "822cc7e0-57e1-416b-a505-fc2bf905c7ad"
      },
      "cell_type": "code",
      "source": [
        "print(\"=====训练集====\")\n",
        "pred_train = emo_utils.predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print(\"=====测试集====\")\n",
        "pred_test = emo_utils.predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====训练集====\n",
            "Accuracy: 0.9772727272727273\n",
            "=====测试集====\n",
            "Accuracy: 0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gofmmFzSQTjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "a7977e2f-1f14-4b97-b962-11ebb8265003"
      },
      "cell_type": "code",
      "source": [
        "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"you are not happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = emo_utils.predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "emo_utils.print_predictions(X_my_sentences, pred)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "\n",
            "i adore you ❤️\n",
            "i love you ❤️\n",
            "funny lol 😄\n",
            "lets play with a ball ⚾\n",
            "food is ready 🍴\n",
            "you are not happy ❤️\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6SwCXD5LQY94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "09cf8891-15c2-46d9-b2bc-02464a543d6b"
      },
      "cell_type": "code",
      "source": [
        "print(\" \\t {0} \\t {1} \\t {2} \\t {3} \\t {4}\".format(emo_utils.label_to_emoji(0), emo_utils.label_to_emoji(1), \\\n",
        "                                                 emo_utils.label_to_emoji(2), emo_utils.label_to_emoji(3), \\\n",
        "                                                 emo_utils.label_to_emoji(4)))\n",
        "import pandas as pd\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "emo_utils.plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \t ❤️ \t ⚾ \t 😄 \t 😞 \t 🍴\n",
            "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
            "Actual                                 \n",
            "0            6    0    0    1    0    7\n",
            "1            0    8    0    0    0    8\n",
            "2            2    0   16    0    0   18\n",
            "3            1    1    2   12    0   16\n",
            "4            0    0    1    0    6    7\n",
            "All          9    9   19   13    6   56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD3CAYAAADormr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIipLwoiIypIwIqKyJIyIqOz/A70x/QDkgLmJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TlfpIt3DQiWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "np.random.seed(1)\n",
        "from keras.initializers import glorot_uniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55VSCS68QqE-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    输入的是X（字符串类型的句子的数组），再转化为对应的句子列表，\n",
        "    输出的是能够让Embedding()函数接受的列表或矩阵（参见图4）。\n",
        "    \n",
        "    参数：\n",
        "        X -- 句子数组，维度为(m, 1)\n",
        "        word_to_index -- 字典类型的单词到索引的映射\n",
        "        max_len -- 最大句子的长度，数据集中所有的句子的长度都不会超过它。\n",
        "        \n",
        "    返回：\n",
        "        X_indices -- 对应于X中的单词索引数组，维度为(m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]  # 训练集数量\n",
        "    # 使用0初始化X_indices\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "    \n",
        "    for i in range(m):\n",
        "        # 将第i个居住转化为小写并按单词分开。\n",
        "        sentences_words = X[i].lower().split()\n",
        "        \n",
        "        # 初始化j为0\n",
        "        j = 0\n",
        "        \n",
        "        # 遍历这个单词列表\n",
        "        for w in sentences_words:\n",
        "            # 将X_indices的第(i, j)号元素为对应的单词索引\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            \n",
        "            j += 1\n",
        "            \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwGRAxhtQtfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a20b51b3-8837-4511-beff-67acf5af1567"
      },
      "cell_type": "code",
      "source": [
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
            "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
            " [220930. 286375.  69714.      0.      0.]\n",
            " [151204. 192973. 302254. 151349. 394475.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Xa-cOgRQv5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    创建Keras Embedding()层，加载已经训练好了的50维GloVe向量\n",
        "    \n",
        "    参数：\n",
        "        word_to_vec_map -- 字典类型的单词与词嵌入的映射\n",
        "        word_to_index -- 字典类型的单词到词汇表（400,001个单词）的索引的映射。\n",
        "        \n",
        "    返回：\n",
        "        embedding_layer() -- 训练好了的Keras的实体层。\n",
        "    \"\"\"\n",
        "    vocab_len = len(word_to_index) + 1\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
        "    \n",
        "    # 初始化嵌入矩阵\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # 将嵌入矩阵的每行的“index”设置为词汇“index”的词向量表示\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "    \n",
        "    # 定义Keras的embbeding层\n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "    \n",
        "    # 构建embedding层。\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    # 将嵌入层的权重设置为嵌入矩阵。\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zlE0-aeFQy1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9bc47b45-56a7-4300-ed1c-1a90ac65cfa7"
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights[0][1][3] = -0.3403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SePlkrIIQ1j-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    实现Emojify-V2模型的计算图\n",
        "    \n",
        "    参数：\n",
        "        input_shape -- 输入的维度，通常是(max_len,)\n",
        "        word_to_vec_map -- 字典类型的单词与词嵌入的映射。\n",
        "        word_to_index -- 字典类型的单词到词汇表（400,001个单词）的索引的映射。\n",
        "    \n",
        "    返回：\n",
        "        model -- Keras模型实体\n",
        "    \"\"\"\n",
        "    # 定义sentence_indices为计算图的输入，维度为(input_shape,)，类型为dtype 'int32' \n",
        "    sentence_indices = Input(input_shape, dtype='int32')\n",
        "    \n",
        "    # 创建embedding层\n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    # 通过嵌入层传播sentence_indices，你会得到嵌入的结果\n",
        "    embeddings = embedding_layer(sentence_indices)\n",
        "    \n",
        "    # 通过带有128维隐藏状态的LSTM层传播嵌入\n",
        "    # 需要注意的是，返回的输出应该是一批序列。\n",
        "    X = LSTM(128, return_sequences=True)(embeddings)\n",
        "    # 使用dropout，概率为0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # 通过另一个128维隐藏状态的LSTM层传播X\n",
        "    # 注意，返回的输出应该是单个隐藏状态，而不是一组序列。\n",
        "    X = LSTM(128, return_sequences=False)(X)\n",
        "    # 使用dropout，概率为0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # 通过softmax激活的Dense层传播X，得到一批5维向量。\n",
        "    X = Dense(5)(X)\n",
        "    # 添加softmax激活\n",
        "    X = Activation('softmax')(X)\n",
        "    \n",
        "    # 创建模型实体\n",
        "    model = Model(inputs=sentence_indices, outputs=X)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzJVZr0EQ44O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "92d60564-da6f-47c9-f48b-ed4a0023529e"
      },
      "cell_type": "code",
      "source": [
        "model = Emojify_V2((max_Len,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 20,223,927\n",
            "Trainable params: 223,877\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PyrkKn2DQ8Ne",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keZtaNAJRJr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = emo_utils.convert_to_one_hot(Y_train, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTO2JNmSRNdW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8oyqXl5_RQ0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ffbd446a-691b-4e9e-b406-8e76d70f5eab"
      },
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = emo_utils.convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 0s 4ms/step\n",
            "Test accuracy =  0.8928571343421936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y6uyrsImRUYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "03fc3a32-9fb5-49f2-bc8f-1b2676d19743"
      },
      "cell_type": "code",
      "source": [
        "C = 5\n",
        "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('正确表情：'+ emo_utils.label_to_emoji(Y_test[i]) + '   预测结果： '+ X_test[i] + emo_utils.label_to_emoji(num).strip())"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "正确表情：😄   预测结果： she got me a nice present\t❤️\n",
            "正确表情：😞   预测结果： work is hard\t😄\n",
            "正确表情：😞   预测结果： This girl is messing with me\t❤️\n",
            "正确表情：❤️   预测结果： I love taking breaks\t😞\n",
            "正确表情：😄   预测结果： you brighten my day\t❤️\n",
            "正确表情：😄   预测结果： will you be my valentine\t❤️\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TeVdYTGHRXWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91039d0c-3ebb-4052-f3c5-cc20ea96ff47"
      },
      "cell_type": "code",
      "source": [
        "x_test = np.array(['you are so beautiful'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you are so beautiful ❤️\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u5wXxQl2R_u2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}