{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeformConv2d.ipynb",
      "provenance": [],
      "mount_file_id": "10ff4ID6R3MSwArcsqmsLd3ahriq-tBce",
      "authorship_tag": "ABX9TyPi8aMyCWk1Hk2owAq3rBlZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochengJF/DeepLearning/blob/master/DeformConv2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uINJHksdKOcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class DeformConv2d(nn.Module):\n",
        "    def __init__(self, inc, outc, kernel_size=3, padding=1, stride=1, bias=None, modulation=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            modulation (bool, optional): If True, Modulated Defomable Convolution (Deformable ConvNets v2).\n",
        "        \"\"\"\n",
        "        super(DeformConv2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "        self.zero_padding = nn.ZeroPad2d(padding)\n",
        "        self.conv = nn.Conv2d(inc, outc, kernel_size=kernel_size, stride=kernel_size, bias=bias)\n",
        "\n",
        "        self.p_conv = nn.Conv2d(inc, 2*kernel_size*kernel_size, kernel_size=3, padding=1, stride=stride)\n",
        "        nn.init.constant_(self.p_conv.weight, 0)\n",
        "        self.p_conv.register_backward_hook(self._set_lr)\n",
        "\n",
        "        self.modulation = modulation\n",
        "        if modulation:\n",
        "            self.m_conv = nn.Conv2d(inc, kernel_size*kernel_size, kernel_size=3, padding=1, stride=stride)\n",
        "            nn.init.constant_(self.m_conv.weight, 0)\n",
        "            self.m_conv.register_backward_hook(self._set_lr)\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_lr(module, grad_input, grad_output):\n",
        "        grad_input = (grad_input[i] * 0.1 for i in range(len(grad_input)))\n",
        "        grad_output = (grad_output[i] * 0.1 for i in range(len(grad_output)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset = self.p_conv(x)\n",
        "        if self.modulation:\n",
        "            m = torch.sigmoid(self.m_conv(x))\n",
        "\n",
        "        dtype = offset.data.type()\n",
        "        ks = self.kernel_size\n",
        "        N = offset.size(1) // 2\n",
        "\n",
        "        if self.padding:\n",
        "            x = self.zero_padding(x)\n",
        "\n",
        "        # (b, 2N, h, w)\n",
        "        p = self._get_p(offset, dtype)\n",
        "\n",
        "        # (b, h, w, 2N)\n",
        "        p = p.contiguous().permute(0, 2, 3, 1)\n",
        "        q_lt = p.detach().floor()\n",
        "        q_rb = q_lt + 1\n",
        "\n",
        "        q_lt = torch.cat([torch.clamp(q_lt[..., :N], 0, x.size(2)-1), torch.clamp(q_lt[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
        "        q_rb = torch.cat([torch.clamp(q_rb[..., :N], 0, x.size(2)-1), torch.clamp(q_rb[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
        "        q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], dim=-1)\n",
        "        q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], dim=-1)\n",
        "\n",
        "        # clip p\n",
        "        p = torch.cat([torch.clamp(p[..., :N], 0, x.size(2)-1), torch.clamp(p[..., N:], 0, x.size(3)-1)], dim=-1)\n",
        "\n",
        "        # bilinear kernel (b, h, w, N)\n",
        "        g_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))\n",
        "        g_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))\n",
        "        g_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))\n",
        "        g_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))\n",
        "\n",
        "        # (b, c, h, w, N)\n",
        "        x_q_lt = self._get_x_q(x, q_lt, N)\n",
        "        x_q_rb = self._get_x_q(x, q_rb, N)\n",
        "        x_q_lb = self._get_x_q(x, q_lb, N)\n",
        "        x_q_rt = self._get_x_q(x, q_rt, N)\n",
        "\n",
        "        # (b, c, h, w, N)\n",
        "        x_offset = g_lt.unsqueeze(dim=1) * x_q_lt + \\\n",
        "                   g_rb.unsqueeze(dim=1) * x_q_rb + \\\n",
        "                   g_lb.unsqueeze(dim=1) * x_q_lb + \\\n",
        "                   g_rt.unsqueeze(dim=1) * x_q_rt\n",
        "\n",
        "        # modulation\n",
        "        if self.modulation:\n",
        "            m = m.contiguous().permute(0, 2, 3, 1)\n",
        "            m = m.unsqueeze(dim=1)\n",
        "            m = torch.cat([m for _ in range(x_offset.size(1))], dim=1)\n",
        "            x_offset *= m\n",
        "\n",
        "        x_offset = self._reshape_x_offset(x_offset, ks)\n",
        "        out = self.conv(x_offset)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _get_p_n(self, N, dtype):\n",
        "        p_n_x, p_n_y = torch.meshgrid(\n",
        "            torch.arange(-(self.kernel_size-1)//2, (self.kernel_size-1)//2+1),\n",
        "            torch.arange(-(self.kernel_size-1)//2, (self.kernel_size-1)//2+1))\n",
        "        # (2N, 1)\n",
        "        p_n = torch.cat([torch.flatten(p_n_x), torch.flatten(p_n_y)], 0)\n",
        "        p_n = p_n.view(1, 2*N, 1, 1).type(dtype)\n",
        "\n",
        "        return p_n\n",
        "\n",
        "    def _get_p_0(self, h, w, N, dtype):\n",
        "        p_0_x, p_0_y = torch.meshgrid(\n",
        "            torch.arange(1, h*self.stride+1, self.stride),\n",
        "            torch.arange(1, w*self.stride+1, self.stride))\n",
        "        p_0_x = torch.flatten(p_0_x).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
        "        p_0_y = torch.flatten(p_0_y).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
        "        p_0 = torch.cat([p_0_x, p_0_y], 1).type(dtype)\n",
        "\n",
        "        return p_0\n",
        "\n",
        "    def _get_p(self, offset, dtype):\n",
        "        N, h, w = offset.size(1)//2, offset.size(2), offset.size(3)\n",
        "\n",
        "        # (1, 2N, 1, 1)\n",
        "        p_n = self._get_p_n(N, dtype)\n",
        "        # (1, 2N, h, w)\n",
        "        p_0 = self._get_p_0(h, w, N, dtype)\n",
        "        p = p_0 + p_n + offset\n",
        "        return p\n",
        "\n",
        "    def _get_x_q(self, x, q, N):\n",
        "        b, h, w, _ = q.size()\n",
        "        padded_w = x.size(3)\n",
        "        c = x.size(1)\n",
        "        # (b, c, h*w)\n",
        "        x = x.contiguous().view(b, c, -1)\n",
        "\n",
        "        # (b, h, w, N)\n",
        "        index = q[..., :N]*padded_w + q[..., N:]  # offset_x*w + offset_y\n",
        "        # (b, c, h*w*N)\n",
        "        index = index.contiguous().unsqueeze(dim=1).expand(-1, c, -1, -1, -1).contiguous().view(b, c, -1)\n",
        "\n",
        "        x_offset = x.gather(dim=-1, index=index).contiguous().view(b, c, h, w, N)\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "    @staticmethod\n",
        "    def _reshape_x_offset(x_offset, ks):\n",
        "        b, c, h, w, N = x_offset.size()\n",
        "        x_offset = torch.cat([x_offset[..., s:s+ks].contiguous().view(b, c, h, w*ks) for s in range(0, N, ks)], dim=-1)\n",
        "        x_offset = x_offset.contiguous().view(b, c, h*ks, w*ks)\n",
        "\n",
        "        return x_offset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0HmTM_KG7Mr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dd762ae0-8ee0-42f5-eb12-e37131a3c190"
      },
      "source": [
        "x = torch.randn(32, 64, 28, 28)\n",
        "print(a.shape)\n",
        "dconv = DeformConv2d(64, 18)\n",
        "x = dconv(x)\n",
        "x.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 64, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 18, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}